paper name,abstract,keywords,link,year
Seemingly unrelated clusterwise linear regression,"Linear regression models based on finite Gaussian mixtures represent a flexible tool for the analysis of linear dependencies in multivariate data. They are suitable for dealing with correlated response variables when data come from a heterogeneous population composed of two or more sub-populations, each of which is characterised by a different linear regression model. Several types of finite mixtures of linear regression models have been specified by changing the assumptions on the parameters that differentiate the sub-populations and/or the vectors of regressors that affect the response variables. They are made more flexible in the class of models defined by mixtures of seemingly unrelated Gaussian linear regressions illustrated in this paper. With these models, the researcher is enabled to use a different vector of regressors for each dependent variable. The proposed class includes parsimonious models obtained by imposing suitable constraints on the variances and covariances of the response variables in the sub-populations. Details about the model identification and maximum likelihood estimation are given. The usefulness of these models is shown through the analysis of a real dataset. Regularity conditions for the model class are illustrated and a proof is provided that, when these conditions are met, the consistency of the maximum likelihood estimator under the examined models is ensured. In addition, the behaviour of this estimator in the presence of finite samples is numerically evaluated through the analysis of simulated datasets.","EM algorithm, Envelope function, Identifiability, Linear model, Regularity conditions",https://link.springer.com//article/10.1007/s11634-019-00369-4,1163
Semiparametric mixtures of regressions with single-index for model based clustering,"In this article, we propose two classes of semiparametric mixture regression models with single-index for model based clustering. Unlike many semiparametric/nonparametric mixture regression models that can only be applied to low dimensional predictors, the new semiparametric models can easily incorporate high dimensional predictors into the nonparametric components. The proposed models are very general, and many of the recently proposed semiparametric/nonparametric mixture regression models are indeed special cases of the new models. Backfitting estimates and the corresponding modified EM algorithms are proposed to achieve optimal convergence rates for both parametric and nonparametric parts. We establish the identifiability results of the proposed two models and investigate the asymptotic properties of the proposed estimation procedures. Simulation studies are conducted to demonstrate the finite sample performance of the proposed models. Two real data applications using the new models reveal some interesting findings.","EM algorithm, Kernel regression, Mixture regression model, Model based clustering, Single-index model",https://link.springer.com//article/10.1007/s11634-020-00392-w,1163
Gaussian parsimonious clustering models with covariates and a noise component,"We consider model-based clustering methods for continuous, correlated data that account for external information available in the presence of mixed-type fixed covariates by proposing the MoEClust suite of models. These models allow different subsets of covariates to influence the component weights and/or component densities by modelling the parameters of the mixture as functions of the covariates. A familiar range of constrained eigen-decomposition parameterisations of the component covariance matrices are also accommodated. This paper thus addresses the equivalent aims of including covariates in Gaussian parsimonious clustering models and incorporating parsimonious covariance structures into all special cases of the Gaussian mixture of experts framework. The MoEClust models demonstrate significant improvement from both perspectives in applications to both univariate and multivariate data sets. Novel extensions to include a uniform noise component for capturing outliers and to address initialisation of the EM algorithm, model selection, and the visualisation of results are also proposed.","Model-based clustering, Mixtures of experts, EM algorithm, Parsimony, Multivariate response, Covariates, Noise component",https://link.springer.com//article/10.1007/s11634-019-00373-8,1163
A robust approach to model-based classification based on trimming and constraints,"In a standard classification framework a set of trustworthy learning data are employed to build a decision rule, with the final aim of classifying unlabelled units belonging to the test set. Therefore, unreliable labelled observations, namely outliers and data with incorrect labels, can strongly undermine the classifier performance, especially if the training size is small. The present work introduces a robust modification to the Model-Based Classification framework, employing impartial trimming and constraints on the ratio between the maximum and the minimum eigenvalue of the group scatter matrices. The proposed method effectively handles noise presence in both response and exploratory variables, providing reliable classification even when dealing with contaminated datasets. A robust information criterion is proposed for model selection. Experiments on real and simulated data, artificially adulterated, are provided to underline the benefits of the proposed method.","Model-based classification, Label noise, Outliers detection, Impartial trimming, Eigenvalues restrictions, Robust estimation",https://link.springer.com//article/10.1007/s11634-019-00371-w,1163
Mixture modeling of data with multiple partial right-censoring levels,"In this paper, a new flexible approach to modeling data with multiple partial right-censoring points is proposed. This method is based on finite mixture models, flexible tool to model heterogeneity in data. A general framework to accommodate partial censoring is considered. In this setting, it is assumed that a certain portion of data points are censored and the rest are not. This situation occurs in many insurance loss data sets. A novel probability function is proposed to be used as a mixture component and the expectation-maximization algorithm is employed for estimating model parameters. The Bayesian information criterion is used for model selection. Additionally, an approach for the variability assessment of parameter estimates as well as the computation of quantiles commonly known as risk measures is considered. The proposed model is evaluated using a simulation study based on four common probability distribution functions used to model right skewed loss data and applied to a real data set with good results.","Finite mixture models, EM algorithm, Right-censoring, Partial censoring, BIC, Insurance loss modeling",https://link.springer.com//article/10.1007/s11634-020-00391-x,1163
Gaussian mixture modeling and model-based clustering under measurement inconsistency,"Finite mixtures present a powerful tool for modeling complex heterogeneous data. One of their most important applications is model-based clustering. It assumes that each data group can be reasonably described by one mixture model component. This establishes a one-to-one relationship between mixture components and clusters. In some cases, however, this relationship can be broken due to the presence of observations from the same class recorded in different ways. This effect can occur because of recording inconsistencies due to the use of different scales, operator errors, or simply various recording styles. The idea presented in this paper aims to alleviate this issue through modifications incorporated into mixture models. While the proposed methodology is applicable to a broad class of mixture models, in this paper it is illustrated on Gaussian mixtures. Several simulation studies and an application to a real-life data set are considered, yielding promising results.","Mixture modeling, K-means, Cluster analysis, Measurement inconsistency, EM algorithm, Hand-written digits",https://link.springer.com//article/10.1007/s11634-020-00393-9,1163
Mixtures of skewed matrix variate bilinear factor analyzers,"In recent years, data have become increasingly higher dimensional and, therefore, an increased need has arisen for dimension reduction techniques for clustering. Although such techniques are firmly established in the literature for multivariate data, there is a relative paucity in the area of matrix variate, or three-way, data. Furthermore, the few methods that are available all assume matrix variate normality, which is not always sensible if cluster skewness or excess kurtosis is present. Mixtures of bilinear factor analyzers using skewed matrix variate distributions are proposed. In all, four such mixture models are presented, based on matrix variate skew-t, generalized hyperbolic, variance-gamma, and normal inverse Gaussian distributions, respectively.","Clustering, Factor analysis, Kurtosis, Skewed, Matrix variate distribution, Mixture models",https://link.springer.com//article/10.1007/s11634-019-00377-4,1163
Seemingly unrelated clusterwise linear regression,"Linear regression models based on finite Gaussian mixtures represent a flexible tool for the analysis of linear dependencies in multivariate data. They are suitable for dealing with correlated response variables when data come from a heterogeneous population composed of two or more sub-populations, each of which is characterised by a different linear regression model. Several types of finite mixtures of linear regression models have been specified by changing the assumptions on the parameters that differentiate the sub-populations and/or the vectors of regressors that affect the response variables. They are made more flexible in the class of models defined by mixtures of seemingly unrelated Gaussian linear regressions illustrated in this paper. With these models, the researcher is enabled to use a different vector of regressors for each dependent variable. The proposed class includes parsimonious models obtained by imposing suitable constraints on the variances and covariances of the response variables in the sub-populations. Details about the model identification and maximum likelihood estimation are given. The usefulness of these models is shown through the analysis of a real dataset. Regularity conditions for the model class are illustrated and a proof is provided that, when these conditions are met, the consistency of the maximum likelihood estimator under the examined models is ensured. In addition, the behaviour of this estimator in the presence of finite samples is numerically evaluated through the analysis of simulated datasets.","EM algorithm, Envelope function, Identifiability, Linear model, Regularity conditions",https://link.springer.com//article/10.1007/s11634-019-00369-4,1163
Semiparametric mixtures of regressions with single-index for model based clustering,"In this article, we propose two classes of semiparametric mixture regression models with single-index for model based clustering. Unlike many semiparametric/nonparametric mixture regression models that can only be applied to low dimensional predictors, the new semiparametric models can easily incorporate high dimensional predictors into the nonparametric components. The proposed models are very general, and many of the recently proposed semiparametric/nonparametric mixture regression models are indeed special cases of the new models. Backfitting estimates and the corresponding modified EM algorithms are proposed to achieve optimal convergence rates for both parametric and nonparametric parts. We establish the identifiability results of the proposed two models and investigate the asymptotic properties of the proposed estimation procedures. Simulation studies are conducted to demonstrate the finite sample performance of the proposed models. Two real data applications using the new models reveal some interesting findings.","EM algorithm, Kernel regression, Mixture regression model, Model based clustering, Single-index model",https://link.springer.com//article/10.1007/s11634-020-00392-w,1163
Gaussian parsimonious clustering models with covariates and a noise component,"We consider model-based clustering methods for continuous, correlated data that account for external information available in the presence of mixed-type fixed covariates by proposing the MoEClust suite of models. These models allow different subsets of covariates to influence the component weights and/or component densities by modelling the parameters of the mixture as functions of the covariates. A familiar range of constrained eigen-decomposition parameterisations of the component covariance matrices are also accommodated. This paper thus addresses the equivalent aims of including covariates in Gaussian parsimonious clustering models and incorporating parsimonious covariance structures into all special cases of the Gaussian mixture of experts framework. The MoEClust models demonstrate significant improvement from both perspectives in applications to both univariate and multivariate data sets. Novel extensions to include a uniform noise component for capturing outliers and to address initialisation of the EM algorithm, model selection, and the visualisation of results are also proposed.","Model-based clustering, Mixtures of experts, EM algorithm, Parsimony, Multivariate response, Covariates, Noise component",https://link.springer.com//article/10.1007/s11634-019-00373-8,1163
A robust approach to model-based classification based on trimming and constraints,"In a standard classification framework a set of trustworthy learning data are employed to build a decision rule, with the final aim of classifying unlabelled units belonging to the test set. Therefore, unreliable labelled observations, namely outliers and data with incorrect labels, can strongly undermine the classifier performance, especially if the training size is small. The present work introduces a robust modification to the Model-Based Classification framework, employing impartial trimming and constraints on the ratio between the maximum and the minimum eigenvalue of the group scatter matrices. The proposed method effectively handles noise presence in both response and exploratory variables, providing reliable classification even when dealing with contaminated datasets. A robust information criterion is proposed for model selection. Experiments on real and simulated data, artificially adulterated, are provided to underline the benefits of the proposed method.","Model-based classification, Label noise, Outliers detection, Impartial trimming, Eigenvalues restrictions, Robust estimation",https://link.springer.com//article/10.1007/s11634-019-00371-w,1163
Mixture modeling of data with multiple partial right-censoring levels,"In this paper, a new flexible approach to modeling data with multiple partial right-censoring points is proposed. This method is based on finite mixture models, flexible tool to model heterogeneity in data. A general framework to accommodate partial censoring is considered. In this setting, it is assumed that a certain portion of data points are censored and the rest are not. This situation occurs in many insurance loss data sets. A novel probability function is proposed to be used as a mixture component and the expectation-maximization algorithm is employed for estimating model parameters. The Bayesian information criterion is used for model selection. Additionally, an approach for the variability assessment of parameter estimates as well as the computation of quantiles commonly known as risk measures is considered. The proposed model is evaluated using a simulation study based on four common probability distribution functions used to model right skewed loss data and applied to a real data set with good results.","Finite mixture models, EM algorithm, Right-censoring, Partial censoring, BIC, Insurance loss modeling",https://link.springer.com//article/10.1007/s11634-020-00391-x,1163
Gaussian mixture modeling and model-based clustering under measurement inconsistency,"Finite mixtures present a powerful tool for modeling complex heterogeneous data. One of their most important applications is model-based clustering. It assumes that each data group can be reasonably described by one mixture model component. This establishes a one-to-one relationship between mixture components and clusters. In some cases, however, this relationship can be broken due to the presence of observations from the same class recorded in different ways. This effect can occur because of recording inconsistencies due to the use of different scales, operator errors, or simply various recording styles. The idea presented in this paper aims to alleviate this issue through modifications incorporated into mixture models. While the proposed methodology is applicable to a broad class of mixture models, in this paper it is illustrated on Gaussian mixtures. Several simulation studies and an application to a real-life data set are considered, yielding promising results.","Mixture modeling, K-means, Cluster analysis, Measurement inconsistency, EM algorithm, Hand-written digits",https://link.springer.com//article/10.1007/s11634-020-00393-9,1163
Mixtures of skewed matrix variate bilinear factor analyzers,"In recent years, data have become increasingly higher dimensional and, therefore, an increased need has arisen for dimension reduction techniques for clustering. Although such techniques are firmly established in the literature for multivariate data, there is a relative paucity in the area of matrix variate, or three-way, data. Furthermore, the few methods that are available all assume matrix variate normality, which is not always sensible if cluster skewness or excess kurtosis is present. Mixtures of bilinear factor analyzers using skewed matrix variate distributions are proposed. In all, four such mixture models are presented, based on matrix variate skew-t, generalized hyperbolic, variance-gamma, and normal inverse Gaussian distributions, respectively.","Clustering, Factor analysis, Kurtosis, Skewed, Matrix variate distribution, Mixture models",https://link.springer.com//article/10.1007/s11634-019-00377-4,1163
Data projections by skewness maximization under scale mixtures of skew-normal vectors,"Multivariate scale mixtures of skew-normal distributions are flexible models that account for the non-normality of data by means of a tail weight parameter and a shape vector representing the asymmetry of the model in a directional fashion. Its stochastic representation involves a skew-normal vector and a non negative mixing scalar variable, independent of the skew-normal vector, that injects tail weight behavior into the model. In this paper we look into the problem of finding the projection that maximizes skewness for vectors that follow a scale mixture of skew-normal distribution; when a simple condition on the moments of the mixing variable is fulfilled, it can be shown that the direction yielding the maximal skewness is proportional to the shape vector. This finding stresses the directional nature of the shape vector to regulate the asymmetry; it also provides the theoretical foundations motivating the skewness based projection pursuit problem in this class of distributions. Some examples that illustrate the application of our results are also given; they include a simulation experiment with artificial data, which sheds light on the usefulness and implications of our results, and the application to real data.","Skew-normal, Scale mixtures of skew-normal distributions, Maximal skewness projection",https://link.springer.com//article/10.1007/s11634-020-00388-6,1163
ParticleMDI: particle Monte Carlo methods for the cluster analysis of multiple datasets with applications to cancer subtype identification,"We present a novel nonparametric Bayesian approach for performing cluster analysis in a context where observational units have data arising from multiple sources. Our approach uses a particle Gibbs sampler for inference in which cluster allocations are jointly updated using a conditional particle filter within a Gibbs sampler, improving the mixing of the MCMC chain. We develop several approaches to improving the computational performance of our algorithm. These methods can achieve greater than an order-of-magnitude improvement in performance at no cost to accuracy and can be applied more broadly to Bayesian inference for mixture models with a single dataset. We apply our algorithm to the discovery of risk cohorts amongst 243 patients presenting with kidney renal clear cell carcinoma, using samples from the Cancer Genome Atlas, for which there are gene expression, copy number variation, DNA methylation, protein expression and microRNA data. We identify 4 distinct consensus subtypes and show they are prognostic for survival rate (\(p < 0.0001\)).","Cluster analysis, Mixture models, Bayesian inference, Particle Monte Carlo",https://link.springer.com//article/10.1007/s11634-020-00401-y,1163
A stochastic block model for interaction lengths,"We propose a new stochastic block model that focuses on the analysis of interaction lengths in dynamic networks. The model does not rely on a discretization of the time dimension and may be used to analyze networks that evolve continuously over time. The framework relies on a clustering structure on the nodes, whereby two nodes belonging to the same latent group tend to create interactions and non-interactions of similar lengths. We introduce a variational expectation–maximization algorithm to perform inference, and adapt a widely used clustering criterion to perform model choice. Finally, we validate our methodology using simulated data experiments and showing two illustrative applications concerning face-to-face interaction data and a bike sharing network.","Interaction lengths, Stochastic block model, Variational inference, Integrated completed likelihood, Social network analysis",https://link.springer.com//article/10.1007/s11634-020-00403-w,1163
Count regression trees,"Count data frequently appear in many scientific studies. In this article, we propose a regression tree method called CORE for analyzing such data. At each node, besides a Poisson regression, a count regression such as hurdle, negative binomial, or zero-inflated regression which can accommodate over-dispersion and/or excess zeros is fitted. A likelihood-based procedure is suggested to select split variables and split sets. Node deviance is then used in the tree pruning process to avoid overfitting. CORE is able to eliminate variable selection bias. In the simulations and real data studies, we show that CORE has some advantages over the existing method, MOB.","Hurdle model, GUIDE, MOB, Negative binomial model, Score residual, Zero-inflated model",https://link.springer.com//article/10.1007/s11634-019-00358-7,1163
Learning a metric when clustering data points in the presence of constraints,"Learning an appropriate distance measure under supervision of side information has become a topic of significant interest within machine learning community. In this paper, we address the problem of metric learning for constrained clustering by considering three important issues: (1) considering importance degree for constraints, (2) preserving the topological structure of data, and (3) preserving some natural distribution properties in the data. This work provides a unified way to handle different issues in constrained clustering by learning an appropriate distance measure. It has modeled the first issue by injecting the importance degree of constraints directly into an objective function. The topological structure of data is preserved by minimizing the reconstruction error of data in the target space. Finally we addressed the issue of preserving natural distribution properties in the data by using the proximity information of data. We have proposed two different methods to address the above mentioned issues. The first approach learns a linear transformation of data into a target space (linear-model) and the second one uses kernel functions to learn an appropriate distance measure (non-linear-model). Experiments show that considering these issues significantly improves clustering accuracy.","Constrained clustering, Metric learning, Instance-level constraints, Weighted constraints",https://link.springer.com//article/10.1007/s11634-019-00359-6,1163
Clustering genomic words in human DNA using peaks and trends of distributions,"In this work we seek clusters of genomic words in human DNA by studying their inter-word lag distributions. Due to the particularly spiked nature of these histograms, a clustering procedure is proposed that first decomposes each distribution into a baseline and a peak distribution. An outlier-robust fitting method is used to estimate the baseline distribution (the ‘trend’), and a sparse vector of detrended data captures the peak structure. A simulation study demonstrates the effectiveness of the clustering procedure in grouping distributions with similar peak behavior and/or baseline features. The procedure is applied to investigate similarities between the distribution patterns of genomic words of lengths 3 and 5 in the human genome. These experiments demonstrate the potential of the new method for identifying words with similar distance patterns.","Classification, Pattern recognition, Robustness, Word distances",https://link.springer.com//article/10.1007/s11634-019-00362-x,1163
Data clustering based on principal curves,"In this contribution we present a new method for data clustering based on principal curves. Principal curves consist of a nonlinear generalization of principal component analysis and may also be regarded as continuous versions of 1D self-organizing maps. The proposed method implements the k-segment algorithm for principal curves extraction. Then, the method divides the principal curves into two or more curves, according to the number of clusters defined by the user. Thus, the distance between the data points and the generate curves is calculated and, afterwards, the classification is performed according to the smallest distance found. The method was applied to nine databases with different dimensionality and number of classes. The results were compared with three clustering algorithms: the k-means algorithm and the 1-D and 2-D self-organizing map algorithms. Experiments show that the method is suitable for clusters with elongated and spherical shapes and achieved significantly better results in some data sets than other clustering algorithms used in this work.","Principal curves, Clustering, Self-organizing maps, Segments",https://link.springer.com//article/10.1007/s11634-019-00363-w,1163
"Ensemble of optimal trees, random forest and random projection ensemble classification","The predictive performance of a random forest ensemble is highly associated with the strength of individual trees and their diversity. Ensemble of a small number of accurate and diverse trees, if prediction accuracy is not compromised, will also reduce computational burden. We investigate the idea of integrating trees that are accurate and diverse. For this purpose, we utilize out-of-bag observations as a validation sample from the training bootstrap samples, to choose the best trees based on their individual performance and then assess these trees for diversity using the Brier score on an independent validation sample. Starting from the first best tree, a tree is selected for the final ensemble if its addition to the forest reduces error of the trees that have already been added. Our approach does not use an implicit dimension reduction for each tree as random project ensemble classification. A total of 35 bench mark problems on classification and regression are used to assess the performance of the proposed method and compare it with random forest, random projection ensemble, node harvest, support vector machine, kNN and classification and regression tree. We compute unexplained variances or classification error rates for all the methods on the corresponding data sets. Our experiments reveal that the size of the ensemble is reduced significantly and better results are obtained in most of the cases. Results of a simulation study are also given where four tree style scenarios are considered to generate data sets with several structures.","Ensemble classification, Ensemble regression, Random forest, Random projection ensemble classification, Accuracy and diversity",https://link.springer.com//article/10.1007/s11634-019-00364-9,1163
A fragmented-periodogram approach for clustering big data time series,"We propose and study a new frequency-domain procedure for characterizing and comparing large sets of long time series. Instead of using all the information available from data, which would be computationally very expensive, we propose some regularization rules in order to select and summarize the most relevant information for clustering purposes. Essentially, we suggest to use a fragmented periodogram computed around the driving cyclical components of interest and to compare the various estimates. This procedure is computationally simple, but able to condense relevant information of the time series. A simulation exercise shows that the smoothed fragmented periodogram works in general better than the non-smoothed one and not worse than the complete periodogram for medium to large sample sizes. We illustrate this procedure in a study of the evolution of several stock markets indices. We further show the effect of recent financial crises over these indices behaviour.","Big data, Fragmented periodogram, Spectral clustering, Smoothed periodogram, Time series clustering",https://link.springer.com//article/10.1007/s11634-019-00365-8,1163
How well do SEM algorithms imitate EM algorithms? A non-asymptotic analysis for mixture models,"In this paper, we present a theoretical and an experimental comparison of EM and SEM algorithms for different mixture models. The SEM algorithm is a stochastic variant of the EM algorithm. The qualitative intuition behind the SEM algorithm is simple: If the number of observations is large enough, then we expect that an update step of the stochastic SEM algorithm is similar to the corresponding update step of the deterministic EM algorithm. In this paper, we quantify this intuition. We show that with high probability the update equations of any EM-like algorithm and its stochastic variant are similar, given that the input set satisfies certain properties. For instance, this result applies to the well-known EM and SEM algorithm for Gaussian mixture models and EM-like and SEM-like heuristics for multivariate power exponential distributions. Our experiments confirm that our theoretical results also hold for a large number of successive update steps. Thereby we complement the known asymptotic results for the SEM algorithm. We also show that, for multivariate Gaussian and multivariate Laplacian mixture models, an update step of SEM runs nearly twice as fast as an EM update set.","EM algorithm, SEM algorithm, Mixture models, Non-asymptotic analysis",https://link.springer.com//article/10.1007/s11634-019-00366-7,1163
Optimal arrangements of hyperplanes for SVM-based multiclass classification,"In this paper, we present a novel SVM-based approach to construct multiclass classifiers by means of arrangements of hyperplanes. We propose different mixed integer (linear and non linear) programming formulations for the problem using extensions of widely used measures for misclassifying observations where the kernel trick can be adapted to be applicable. Some dimensionality reductions and variable fixing strategies are also developed for these models. An extensive battery of experiments has been run which reveal the powerfulness of our proposal as compared with other previously proposed methodologies.","Multiclass support vector machines, Mixed integer non linear programming, Classification, hyperplanes",https://link.springer.com//article/10.1007/s11634-019-00367-6,1163
Classification using sequential order statistics,"Whereas discrimination methods and their error probabilities were broadly investigated for common data distributions such as the multivariate normal or t-distributions, this paper considers the case when the recorded data are assumed to be observations from sequential order statistics. Random vectors of sequential order statistics describe, e.g., successive failures in a k-out-of-n system or in other coherent and load sharing systems allowing for changes of underlying lifetime distributions caused by component failures. Within this framework, the Bayesian two-class discrimination approach with known prior probabilities and class parameters is considered, and exact and asymptotic formulas for the error probabilities in terms of Erlang and hypoexponential distributions are derived. Since the Bayesian classifier is closely related to Kullback–Leibler’s information distance, this approach is extended by invoking other divergence measures such as Jeffreys and Rényi’s distance. While exact formulas for the misclassification rates of the resulting distance-based classifiers are not available, inequalities among the corresponding error probabilities are derived. The performance of the applied classifiers is illustrated by some simulation results.","Classification, Exponential families, Hypoexponential distribution, Jeffrey’s divergence, Kullback–Leibler divergence, Matusita’s affinity, Sequential order statistics",https://link.springer.com//article/10.1007/s11634-019-00368-5,1163
Orthogonal nonnegative matrix tri-factorization based on Tweedie distributions,"Orthogonal nonnegative matrix tri-factorization (ONMTF) is a biclustering method using a given nonnegative data matrix and has been applied to document-term clustering, collaborative filtering, and so on. In previously proposed ONMTF methods, it is assumed that the error distribution is normal. However, the assumption of normal distribution is not always appropriate for nonnegative data. In this paper, we propose three new ONMTF methods, which respectively employ the following error distributions: normal, Poisson, and compound Poisson. To develop the new methods, we adopt a k-means based algorithm but not a multiplicative updating algorithm, which was the main method used for obtaining estimators in previous methods. A simulation study and an application involving document-term matrices demonstrate that our method can outperform previous methods, in terms of the goodness of clustering and in the estimation of the factor matrix.","Orthogonal nonnegative matrix tri-factorization, Biclustering, Tweedie family, Compound Poisson distribution, Spherical k-means",https://link.springer.com//article/10.1007/s11634-018-0348-8,1163
Discriminant analysis for discrete variables derived from a tree-structured graphical model,"The purpose of this paper is to illustrate the potential use of discriminant analysis for discrete variables whose dependence structure is assumed to follow, or can be approximated by, a tree-structured graphical model. This is done by comparing its empirical performance, using estimated error rates for real and simulated data, with the well-known Naive Bayes classification rule and with linear logistic regression, both of which do not consider any interaction between variables, and with models that consider interactions like a decomposable and the saturated model. The results show that discriminant analysis based on tree-structured graphical models, a simple nonlinear method including only some of the pairwise interactions between variables, is competitive with, and sometimes superior to, other methods which assume no interactions, and has the advantage over more complex decomposable models of finding the graph structure in a fast way and exact form.","Discrete variables, Discriminant analysis, Error rates, Minimum weight spanning tree, Multinomial distribution, Sparseness, Structure estimation, Tree-structured graphical models",https://link.springer.com//article/10.1007/s11634-019-00352-z,1163
Supervised learning via smoothed Polya trees,"We propose a generative classification model that extends Quadratic Discriminant Analysis (QDA) (Cox in J R Stat Soc Ser B (Methodol) 20:215–242, 1958) and Linear Discriminant Analysis (LDA) (Fisher in Ann Eugen 7:179–188, 1936; Rao in J R Stat Soc Ser B 10:159–203, 1948) to the Bayesian nonparametric setting, providing a competitor to MclustDA (Fraley and Raftery in Am Stat Assoc 97:611–631, 2002). This approach models the data distribution for each class using a multivariate Polya tree and realizes impressive results in simulations and real data analyses. The flexibility gained from further relaxing the distributional assumptions of QDA can greatly improve the ability to correctly classify new observations for models with severe deviations from parametric distributional assumptions, while still performing well when the assumptions hold. The proposed method is quite fast compared to other supervised classifiers and very simple to implement as there are no kernel tricks or initialization steps perhaps making it one of the more user-friendly approaches to supervised learning. This highlights a significant feature of the proposed methodology as suboptimal tuning can greatly hamper classification performance; e.g., SVMs fit with non-optimal kernels perform significantly worse.","Bayesian nonparametric, Density estimation, Classification",https://link.springer.com//article/10.1007/s11634-018-0344-z,1163
Robust and sparse k-means clustering for high-dimensional data,"In real-world application scenarios, the identification of groups poses a significant challenge due to possibly occurring outliers and existing noise variables. Therefore, there is a need for a clustering method which is capable of revealing the group structure in data containing both outliers and noise variables without any pre-knowledge. In this paper, we propose a k-means-based algorithm incorporating a weighting function which leads to an automatic weight assignment for each observation. In order to cope with noise variables, a lasso-type penalty is used in an objective function adjusted by observation weights. We finally introduce a framework for selecting both the number of clusters and variables based on a modified gap statistic. The conducted experiments on simulated and real-world data demonstrate the advantage of the method to identify groups, outliers, and informative variables simultaneously.","Clusters, Outliers, Noise variables, High-dimensions, Gap statistic",https://link.springer.com//article/10.1007/s11634-019-00356-9,1163
Exploration of the variability of variable selection based on distances between bootstrap sample results,"It is well known that variable selection in multiple regression can be unstable and that the model uncertainty can be considerable. The model uncertainty can be quantified and explored by bootstrap resampling, see Sauerbrei et al. (Biom J 57:531–555, 2015). Here approaches are introduced that use the results of bootstrap replications of the variable selection process to obtain more detailed information about the data. Analyses will be based on dissimilarities between the results of the analyses of different bootstrap samples. Dissimilarities are computed between the vector of predictions, and between the sets of selected variables. The dissimilarities are used to map the models by multidimensional scaling, to cluster them, and to construct heatplots. Clusters can point to different interpretations of the data that could arise from different selections of variables supported by different bootstrap samples. A new measure of variable selection instability is also defined. The methodology can be applied to various regression models, estimators, and variable selection methods. It will be illustrated by three real data examples, using linear regression and a Cox proportional hazards model, and model selection by AIC and BIC.","Linear regression, Cox proportional hazards, Cluster analysis, Multidimensional scaling, Heatmaps",https://link.springer.com//article/10.1007/s11634-018-00351-6,1163
A classification tree approach for the modeling of competing risks in discrete time,"Cause-specific hazard models are a popular tool for the analysis of competing risks data. The classical modeling approach in discrete time consists of fitting parametric multinomial logit models. A drawback of this method is that the focus is on main effects only, and that higher order interactions are hard to handle. Moreover, the resulting models contain a large number of parameters, which may cause numerical problems when estimating coefficients. To overcome these problems, a tree-based model is proposed that extends the survival tree methodology developed previously for time-to-event models with one single type of event. The performance of the method, compared with several competitors, is investigated in simulations. The usefulness of the proposed approach is demonstrated by an analysis of age-related macular degeneration among elderly people that were monitored by annual study visits.","Discrete time-to-event data, Competing risks, Recursive partitioning, Cause-specific hazards, Regression modeling",https://link.springer.com//article/10.1007/s11634-018-0345-y,1163
Convex clustering for binary data,"We present a new clustering algorithm for multivariate binary data. The new algorithm is based on the convex relaxation of hierarchical clustering, which is achieved by considering the binomial likelihood as a natural distribution for binary data and by formulating convex clustering using a pairwise penalty on prototypes of clusters. Under convex clustering, we show that the typical \(\ell _1\) pairwise fused penalty results in ineffective cluster formation. In an attempt to promote the clustering performance and select the relevant clustering variables, we propose the penalized maximum likelihood estimation with an \(\ell _2\) fused penalty on the fusion parameters and an \(\ell _1\) penalty on the loading matrix. We provide an efficient algorithm to solve the optimization by using majorization-minimization algorithm and alternative direction method of multipliers. Numerical studies confirmed its good performance and real data analysis demonstrates the practical usefulness of the proposed method.","Binary data, Convex clustering, Dimension reduction, Fused penalty",https://link.springer.com//article/10.1007/s11634-018-0350-1,1163
Bayesian shrinkage in mixture-of-experts models: identifying robust determinants of class membership,A method for implicit variable selection in mixture-of-experts frameworks is proposed. We introduce a prior structure where information is taken from a set of independent covariates. Robust class membership predictors are identified using a normal gamma prior. The resulting model setup is used in a finite mixture of Bernoulli distributions to find homogenous clusters of women in Mozambique based on their information sources on HIV. Fully Bayesian inference is carried out via the implementation of a Gibbs sampler.,"Mixture-of-experts, Classification, Shrinkage, Bayesian inference, Normal gamma prior",https://link.springer.com//article/10.1007/s11634-019-00353-y,1163
"Finite mixture-of-gamma distributions: estimation, inference, and model-based clustering","Finite mixtures of (multivariate) Gaussian distributions have broad utility, including their usage for model-based clustering. There is increasing recognition of mixtures of asymmetric distributions as powerful alternatives to traditional mixtures of Gaussian and mixtures of t distributions. The present work contributes to that assertion by addressing some facets of estimation and inference for mixtures-of-gamma distributions, including in the context of model-based clustering. Maximum likelihood estimation of mixtures of gammas is performed using an expectation–conditional–maximization (ECM) algorithm. The Wilson–Hilferty normal approximation is employed as part of an effective starting value strategy for the ECM algorithm, as well as provides insight into an effective model-based clustering strategy. Inference regarding the appropriateness of a common-shape mixture-of-gammas distribution is motivated by theory from research on infant habituation. We provide extensive simulation results that demonstrate the strong performance of our routines as well as analyze two real data examples: an infant habituation dataset and a whole genome duplication dataset.","ECM algorithms, Finite mixture models, Identifiability, Mixturegram, Multivariate Gaussian copula, Starting values",https://link.springer.com//article/10.1007/s11634-019-00361-y,1163
A Kendall correlation coefficient between functional data,"Measuring dependence is a very important tool to analyze pairs of functional data. The coefficients currently available to quantify association between two sets of curves show a non robust behavior under the presence of outliers. We propose a new robust numerical measure of association for bivariate functional data. We extend in this paper Kendall coefficient for finite dimensional observations to the functional setting. We also study its statistical properties. An extensive simulation study shows the good behavior of this new measure for different types of functional data. Moreover, we apply it to establish association for real data, including microarrays time series in genetics.","Concordance, Dependence, Functional data, Kendall’s tau",https://link.springer.com//article/10.1007/s11634-019-00360-z,1163
Directional co-clustering,"Co-clustering addresses the problem of simultaneous clustering of both dimensions of a data matrix. When dealing with high dimensional sparse data, co-clustering turns out to be more beneficial than one-sided clustering even if one is interested in clustering along one dimension only. Aside from being high dimensional and sparse, some datasets, such as document-term matrices, exhibit directional characteristics, and the \(L_2\) normalization of such data, so that it lies on the surface of a unit hypersphere, is useful. Popular co-clustering assumptions such as Gaussian or Multinomial are inadequate for this type of data. In this paper, we extend the scope of co-clustering to directional data. We present Diagonal Block Mixture of Von Mises–Fisher distributions (dbmovMFs), a co-clustering model which is well suited for directional data lying on a unit hypersphere. By setting the estimate of the model parameters under the maximum likelihood (ML) and classification ML approaches, we develop a class of EM algorithms for estimating dbmovMFs from data. Extensive experiments, on several real-world datasets, confirm the advantage of our approach and demonstrate the effectiveness of our algorithms.","Co-clustering, Directional data, von Mises-Fisher distribution, EM algorithm, Document clustering",https://link.springer.com//article/10.1007/s11634-018-0323-4,1163
Investigating consumers’ store-choice behavior via hierarchical variable selection,"This paper is concerned with a store-choice model for investigating consumers’ store-choice behavior based on scanner panel data. Our store-choice model enables us to evaluate the effects of the consumer/product attributes not only on the consumer’s store choice but also on his/her purchase quantity. Moreover, we adopt a mixed-integer optimization (MIO) approach to selecting the best set of explanatory variables with which to construct the store-choice model. We devise two MIO models for hierarchical variable selection in which the hierarchical structure of product categories is used to enhance the reliability and computational efficiency of the variable selection. We assess the effectiveness of our MIO models through computational experiments on actual scanner panel data. These experiments are focused on the consumer’s choice among three types of stores in Japan: convenience stores, drugstores, and (grocery) supermarkets. The computational results demonstrate that our method has several advantages over the common methods for variable selection, namely, the stepwise method and \(L_1\)-regularized regression. Furthermore, our analysis reveals that convenience stores are most strongly chosen for gift cards and garbage disposal permits, drugstores are most strongly chosen for products that are specific to drugstores, and supermarkets are most strongly chosen for health food products by women with families.","Store choice, Variable selection, Mixed-integer optimization, Multiple regression analysis, Scanner panel data",https://link.springer.com//article/10.1007/s11634-018-0327-0,1163
Subspace clustering for the finite mixture of generalized hyperbolic distributions,"The finite mixture of generalized hyperbolic distributions is a flexible model for clustering, but its large number of parameters for estimation, especially in high dimensions, can make it computationally expensive to work with. In light of this issue, we provide an extension of the subspace clustering technique developed for finite Gaussian mixtures to that of generalized hyperbolic distribution. The methodology will be demonstrated with numerical experiments.","Dimensionality reduction, Finite mixture models, Generalized hyperbolic distribution",https://link.springer.com//article/10.1007/s11634-018-0333-2,1163
On support vector machines under a multiple-cost scenario,"Support vector machine (SVM) is a powerful tool in binary classification, known to attain excellent misclassification rates. On the other hand, many realworld classification problems, such as those found in medical diagnosis, churn or fraud prediction, involve misclassification costs which may be different in the different classes. However, it may be hard for the user to provide precise values for such misclassification costs, whereas it may be much easier to identify acceptable misclassification rates values. In this paper we propose a novel SVM model in which misclassification costs are considered by incorporating performance constraints in the problem formulation. Specifically, our aim is to seek the hyperplane with maximal margin yielding misclassification rates below given threshold values. Such maximal margin hyperplane is obtained by solving a quadratic convex problem with linear constraints and integer variables. The reported numerical experience shows that our model gives the user control on the misclassification rates in one class (possibly at the expense of an increase in misclassification rates for the other class) and is feasible in terms of running times.","Constrained classification, Misclassification costs, Mixed integer quadratic programming, Sensitivity/specificity trade-off, Support vector machines",https://link.springer.com//article/10.1007/s11634-018-0330-5,1163
Regression trees for detecting preference patterns from rank data,"A regression tree method for analyzing rank data is proposed. A key ingredient of the methodology is to convert ranks into scores by paired comparison. We then utilize the GUIDE tree method on the score vectors to identify the preference patterns in the data. This method is exempt from selection bias and the simulation results show that it is good with respect to the selection of split variables and has a better prediction accuracy than the two other investigated methods in some cases. Furthermore, it is applicable to complex data which may contain incomplete ranks and missing covariate values. We demonstrate its usefulness in two real data studies.","GUIDE regression tree, Machine learning, Missing values, Paired comparison, Scoring system, Selection bias",https://link.springer.com//article/10.1007/s11634-018-0332-3,1163
Generalised linear model trees with global additive effects,"Model-based trees are used to find subgroups in data which differ with respect to model parameters. In some applications it is natural to keep some parameters fixed globally for all observations while asking if and how other parameters vary across subgroups. Existing implementations of model-based trees can only deal with the scenario where all parameters depend on the subgroups. We propose partially additive linear model trees (PALM trees) as an extension of (generalised) linear model trees (LM and GLM trees, respectively), in which the model parameters are specified a priori to be estimated either globally from all observations or locally from the observations within the subgroups determined by the tree. Simulations show that the method has high power for detecting subgroups in the presence of global effects and reliably recovers the true parameters. Furthermore, treatment–subgroup differences are detected in an empirical application of the method to data from a mathematics exam: the PALM tree is able to detect a small subgroup of students that had a disadvantage in an exam with two versions while adjusting for overall ability effects.","Subgroup analysis, Model-based recursive partitioning, GLM, Tree",https://link.springer.com//article/10.1007/s11634-018-0342-1,1163
Greedy Gaussian segmentation of multivariate time series,"We consider the problem of breaking a multivariate (vector) time series into segments over which the data is well explained as independent samples from a Gaussian distribution. We formulate this as a covariance-regularized maximum likelihood problem, which can be reduced to a combinatorial optimization problem of searching over the possible breakpoints, or segment boundaries. This problem can be solved using dynamic programming, with complexity that grows with the square of the time series length. We propose a heuristic method that approximately solves the problem in linear time with respect to this length, and always yields a locally optimal choice, in the sense that no change of any one breakpoint improves the objective. Our method, which we call greedy Gaussian segmentation (GGS), easily scales to problems with vectors of dimension over 1000 and time series of arbitrary length. We discuss methods that can be used to validate such a model using data, and also to automatically choose appropriate values of the two hyperparameters in the method. Finally, we illustrate our GGS approach on financial time series and Wikipedia text data.","Time series analysis, Change-point detection, Financial regimes, Text segmentation, Covariance regularization, Greedy algorithms",https://link.springer.com//article/10.1007/s11634-018-0335-0,1163
A two-stage sparse logistic regression for optimal gene selection in high-dimensional microarray data classification,"The common issues of high-dimensional gene expression data are that many of the genes may not be relevant, and there exists a high correlation among genes. Gene selection has been proven to be an effective way to improve the results of many classification methods. Sparse logistic regression using least absolute shrinkage and selection operator (lasso) or using smoothly clipped absolute deviation is one of the most widely applicable methods in cancer classification for gene selection. However, this method faces a critical challenge in practical applications when there are high correlations among genes. To address this problem, a two-stage sparse logistic regression is proposed, with the aim of obtaining an efficient subset of genes with high classification capabilities by combining the screening approach as a filter method and adaptive lasso with a new weight as an embedded method. In the first stage, sure independence screening method as a screening approach retains those genes representing high individual correlation with the cancer class level. In the second stage, the adaptive lasso with new weight is implemented to address the existence of high correlations among the screened genes in the first stage. Experimental results based on four publicly available gene expression datasets have shown that the proposed method significantly outperforms three state-of-the-art methods in terms of classification accuracy, G-mean, area under the curve, and stability. In addition, the results demonstrate that the top selected genes are biologically related to the cancer type. Thus, the proposed method can be useful for cancer classification using DNA gene expression data in real clinical practice.","Sparse logistic regression, Lasso, SCAD, Cancer classification, Gene selection",https://link.springer.com//article/10.1007/s11634-018-0334-1,1163
Variable selection in discriminant analysis for mixed continuous-binary variables and several groups,"We propose a method for variable selection in discriminant analysis with mixed continuous and binary variables. This method is based on a criterion that permits to reduce the variable selection problem to a problem of estimating suitable permutation and dimensionality. Then, estimators for these parameters are proposed and the resulting method for selecting variables is shown to be consistent. A simulation study that permits to study several properties of the proposed approach and to compare it with an existing method is given, and an example on a real data set is provided.","Variable selection, Discriminant analysis, Classification, Mixed variables",https://link.springer.com//article/10.1007/s11634-018-0343-0,1163
Bayesian nonstationary Gaussian process models via treed process convolutions,"The Gaussian process is a common model in a wide variety of applications, such as environmental modeling, computer experiments, and geology. Two major challenges often arise: First, assuming that the process of interest is stationary over the entire domain often proves to be untenable. Second, the traditional Gaussian process model formulation is computationally inefficient for large datasets. In this paper, we propose a new Gaussian process model to tackle these problems based on the convolution of a smoothing kernel with a partitioned latent process. Nonstationarity can be modeled by allowing a separate latent process for each partition, which approximates a regional clustering structure. Partitioning follows a binary tree generating process similar to that of Classification and Regression Trees. A Bayesian approach is used to estimate the partitioning structure and model parameters simultaneously. Our motivating dataset consists of 11918 precipitation anomalies. Results show that our model has promising prediction performance and is computationally efficient for large datasets.","Spatial statistics, Stochastic modeling, Classification and Regression Trees, Reduced-rank approximation, Heteroscedasticity",https://link.springer.com//article/10.1007/s11634-018-0341-2,1163
Linear components of quadratic classifiers,"We obtain a decomposition of any quadratic classifier in terms of products of hyperplanes. These hyperplanes can be viewed as relevant linear components of the quadratic rule (with respect to the underlying classification problem). As an application, we introduce the associated multidirectional classifier; a piecewise linear classification rule induced by the approximating products. Such a classifier is useful to determine linear combinations of the predictor variables with ability to discriminate. We also show that this classifier can be used as a tool to reduce the dimension of the data and helps identify the most important variables to classify new elements. Finally, we illustrate with a real data set the use of these linear components to construct oblique classification trees.","Supervised classification, Fisher linear discriminant analysis, Quadratic discriminant analysis, Reduction of the dimension, Feature extraction, Oblique classification trees",https://link.springer.com//article/10.1007/s11634-018-0321-6,1163
Mixture model modal clustering,"The two most extended density-based approaches to clustering are surely mixture model clustering and modal clustering. In the mixture model approach, the density is represented as a mixture and clusters are associated to the different mixture components. In modal clustering, clusters are understood as regions of high density separated from each other by zones of lower density, so that they are closely related to certain regions around the density modes. If the true density is indeed in the assumed class of mixture densities, then mixture model clustering allows to scrutinize more subtle situations than modal clustering. However, when mixture modeling is used in a nonparametric way, taking advantage of the denseness of the sieve of mixture densities to approximate any density, then the correspondence between clusters and mixture components may become questionable. In this paper we introduce two methods to adopt a modal clustering point of view after a mixture model fit. Examples are provided to illustrate that mixture modeling can also be used for clustering in a nonparametric sense, as long as clusters are understood as the domains of attraction of the density modes. Finally, a simulation study reveals that the new methods are extremely efficient from a computational point of view, while at the same time they retain a high level of accuracy.","Mixture modeling, Modal clustering, Component merging, Mean shift algorithm",https://link.springer.com//article/10.1007/s11634-018-0308-3,1163
A method for selecting the relevant dimensions for high-dimensional classification in singular vector spaces,"In this paper, we give a new feature selection algorithm for the binary class classification problem in sparse high-dimensional spaces. Singular value decomposition (SVD) is a popular dimension reduction method in higher-dimensional classification. The traditional SVD method begins by ranking the Singular Dimensions (SDs) from largest singular value to the smallest. However, when the number of signals is fewer than the number of noise, the first few ranked SDs are not necessarily the best for classification. We demonstrate, theoretically and empirically, that our method efficiently selects the SDs most appropriate for classification and significantly reduces the misclassification error. We also apply our method to real data text mining applications.","Feature selection, Fisher discriminant, Naive Bayes, Singular value decomposition (SVD), Text mining",https://link.springer.com//article/10.1007/s11634-018-0311-8,1163
Weighted distance-based trees for ranking data,"Within the framework of preference rankings, the interest can lie in finding which predictors and which interactions are able to explain the observed preference structures, because preference decisions will usually depend on the characteristics of both the judges and the objects being judged. This work proposes the use of a univariate decision tree for ranking data based on the weighted distances for complete and incomplete rankings, and considers the area under the ROC curve both for pruning and model assessment. Two real and well-known datasets, the SUSHI preference data and the University ranking data, are used to display the performance of the methodology.","Decision tree, Distance-based methods, Ranking data, Kemeny distance, SUSHI data, University ranking data",https://link.springer.com//article/10.1007/s11634-017-0306-x,1163
Mixtures of restricted skew-t factor analyzers with common factor loadings,"Mixtures of common t factor analyzers (MCtFA) have been shown its effectiveness in robustifying mixtures of common factor analyzers (MCFA) when handling model-based clustering of the high-dimensional data with heavy tails. However, the MCtFA model may still suffer from a lack of robustness against observations whose distributions are highly asymmetric. This paper presents a further robust extension of the MCFA and MCtFA models, called the mixture of common restricted skew-t factor analyzers (MCrstFA), by assuming a restricted multivariate skew-t distribution for the common factors. The MCrstFA model can be used to accommodate severely non-normal (skewed and leptokurtic) random phenomena while preserving its parsimony in factor-analytic representation and performing graphical visualization in low-dimensional plots. A computationally feasible expectation conditional maximization either algorithm is developed to carry out maximum likelihood estimation. The numbers of factors and mixture components are simultaneously determined based on common likelihood penalized criteria. The usefulness of our proposed model is illustrated with simulated and real datasets, and experimental results signify its superiority over some existing competitors.","Clustering, Common factor loadings, Data reduction, ECME algorithm, Factor analyzer, Outliers",https://link.springer.com//article/10.1007/s11634-018-0317-2,1163
Properties of Bangdiwala’s B,"Cohen’s kappa is the most widely used coefficient for assessing interobserver agreement on a nominal scale. An alternative coefficient for quantifying agreement between two observers is Bangdiwala’s B. To provide a proper interpretation of an agreement coefficient one must first understand its meaning. Properties of the kappa coefficient have been extensively studied and are well documented. Properties of coefficient B have been studied, but not extensively. In this paper, various new properties of B are presented. Category B-coefficients are defined that are the basic building blocks of B. It is studied how coefficient B, Cohen’s kappa, the observed agreement and associated category coefficients may be related. It turns out that the relationships between the coefficients are quite different for \(2\times 2\) tables than for agreement tables with three or more categories.","Interrater reliability, Interobserver agreement, Category coefficients, \(2\times 2\) tables, Cohen’s kappa",https://link.springer.com//article/10.1007/s11634-018-0319-0,1163
Comparisons among several methods for handling missing data in principal component analysis (PCA),"Missing data are prevalent in many data analytic situations. Those in which principal component analysis (PCA) is applied are no exceptions. The performance of five methods for handling missing data in PCA is investigated, the missing data passive method, the weighted low rank approximation (WLRA) method, the regularized PCA (RPCA) method, the trimmed scores regression method, and the data augmentation (DA) method. Three complete data sets of varying sizes were selected, in which missing data were created randomly and non-randomly. These data were then analyzed by the five methods, and their parameter recovery capability, as measured by the mean congruence coefficient between loadings obtained from full and missing data, is compared as functions of the number of extracted components (dimensionality) and the proportion of missing data (censor rate). For randomly censored data, all five methods worked well when the dimensionality and censor rate were small. Their performance deteriorated, as the dimensionality and censor rate increased, but the speed of deterioration was distinctly faster with the WLRA method. The RPCA method worked best and the DA method came as a close second in terms of parameter recovery. However, the latter, as implemented here, was found to be extremely time-consuming. For non-randomly censored data, the recovery was also affected by the degree of non-randomness in censoring processes. Again the RPCA method worked best, maintaining good to excellent recoveries when the censor rate was small and the dimensionality of solutions was not too excessive.","Homogeneity criterion, Missing data passive (MDP) method, Alternating least squares (ALS) algorithm, Weighted low rank approximation (WLRA) method, Regularized PCA (RPCA) method, Trimmed scores regression (TSR) method, Data augmentation (DA) method, Congruence coefficient",https://link.springer.com//article/10.1007/s11634-018-0310-9,1163
A bivariate index vector for measuring departure from double symmetry in square contingency tables,"For square contingency tables, a double symmetry model having a matrix structure that combines both symmetry and point symmetry was proposed. Also, an index which represents the degree of departure from double symmetry was proposed. However, this index cannot simultaneously characterize the degree of departure from symmetry and the degree of departure from point symmetry. For measuring the degree of departure from double symmetry, the present paper proposes a bivariate index vector that can simultaneously characterize the degree of departure from symmetry and the degree of departure from point symmetry.","Confidence region, Double symmetry, Index vector, Visualization",https://link.springer.com//article/10.1007/s11634-018-0320-7,1163
New distance measures for classifying X-ray astronomy data into stellar classes,"The classification of the X-ray sources into classes (such as extragalactic sources, background stars,...) is an essential task in astronomy. Typically, one of the classes corresponds to extragalactic radiation, whose photon emission behaviour is well characterized by a homogeneous Poisson process. We propose to use normalized versions of the Wasserstein and Zolotarev distances to quantify the deviation of the distribution of photon interarrival times from the exponential class. Our main motivation is the analysis of a massive dataset from X-ray astronomy obtained by the Chandra Orion Ultradeep Project (COUP). This project yielded a large catalog of 1616 X-ray cosmic sources in the Orion Nebula region, with their series of photon arrival times and associated energies. We consider the plug-in estimators of these metrics, determine their asymptotic distributions, and illustrate their finite-sample performance with a Monte Carlo study. We estimate these metrics for each COUP source from three different classes. We conclude that our proposal provides a striking amount of information on the nature of the photon emitting sources. Further, these variables have the ability to identify X-ray sources wrongly catalogued before. As an appealing conclusion, we show that some sources, previously classified as extragalactic emissions, have a much higher probability of being young stars in Orion Nebula.","Classification, X-ray astronomy, Wasserstein distance, Zolotarev metric, Photon interarrival time, Exponential distribution",https://link.springer.com//article/10.1007/s11634-018-0309-2,1163
Model-based approach for household clustering with mixed scale variables,"The Ministry of Social Development in Mexico is in charge of creating and assigning social programmes targeting specific needs in the population for the improvement of the quality of life. To better target the social programmes, the Ministry is aimed to find clusters of households with the same needs based on demographic characteristics as well as poverty conditions of the household. Available data consists of continuous, ordinal, and nominal variables, all of which come from a non-i.i.d complex design survey sample. We propose a Bayesian nonparametric mixture model that jointly models a set of latent variables, as in an underlying variable response approach, associated to the observed mixed scale data and accommodates for the different sampling probabilities. The performance of the model is assessed via simulated data. A full analysis of socio-economic conditions in households in the Mexican State of Mexico is presented.","Bayes nonparametrics, Complex design, Latent variables, Multivariate normal, Poisson–Dirichlet process",https://link.springer.com//article/10.1007/s11634-018-0313-6,1163
Unifying data units and models in (co-)clustering,"Statisticians are already aware that any task (exploration, prediction) involving a modeling process is largely dependent on the measurement units for the data, to the extent that it should be impossible to provide a statistical outcome without specifying the couple (unit,model). In this work, this general principle is formalized with a particular focus on model-based clustering and co-clustering in the case of possibly mixed data types (continuous and/or categorical and/or counting features), and this opportunity is used to revisit what the related data units are. Such a formalization allows us to raise three important spots: (i) the couple (unit,model) is not identifiable so that different interpretations unit/model of the same whole modeling process are always possible; (ii) combining different “classical” units with different “classical” models should be an interesting opportunity for a cheap, wide and meaningful expansion of the whole modeling process family designed by the couple (unit,model); (iii) if necessary, this couple, up to the non-identifiability property, could be selected by any traditional model selection criterion. Some experiments on real data sets illustrate in detail practical benefits arising from the previous three spots.","Measurement units, Mixed data, Mixture models, Model selection, Non-identifiability",https://link.springer.com//article/10.1007/s11634-018-0325-2,1163
From here to infinity: sparse finite versus Dirichlet process mixtures in model-based clustering,"In model-based clustering mixture models are used to group data points into clusters. A useful concept introduced for Gaussian mixtures by Malsiner Walli et al. (Stat Comput 26:303–324, 2016) are sparse finite mixtures, where the prior distribution on the weight distribution of a mixture with K components is chosen in such a way that a priori the number of clusters in the data is random and is allowed to be smaller than K with high probability. The number of clusters is then inferred a posteriori from the data. The present paper makes the following contributions in the context of sparse finite mixture modelling. First, it is illustrated that the concept of sparse finite mixture is very generic and easily extended to cluster various types of non-Gaussian data, in particular discrete data and continuous multivariate data arising from non-Gaussian clusters. Second, sparse finite mixtures are compared to Dirichlet process mixtures with respect to their ability to identify the number of clusters. For both model classes, a random hyper prior is considered for the parameters determining the weight distribution. By suitable matching of these priors, it is shown that the choice of this hyper prior is far more influential on the cluster solution than whether a sparse finite mixture or a Dirichlet process mixture is taken into consideration.","Mixture distributions, Latent class analysis, Skew distributions, Marginal likelihoods, Count data, Dirichlet prior",https://link.springer.com//article/10.1007/s11634-018-0329-y,1163
Clustering via finite nonparametric ICA mixture models,"We propose a novel extension of nonparametric multivariate finite mixture models by dropping the standard conditional independence assumption and incorporating the independent component analysis (ICA) structure instead. This innovation extends nonparametric mixture model estimation methods to situations in which conditional independence, a necessary assumption for the unique identifiability of the parameters in such models, is clearly violated. We formulate an objective function in terms of penalized smoothed Kullback–Leibler distance and introduce the nonlinear smoothed majorization-minimization independent component analysis algorithm for optimizing this function and estimating the model parameters. Our algorithm does not require any labeled observations a priori; it may be used for fully unsupervised clustering problems in a multivariate setting. We have implemented a practical version of this algorithm, which utilizes the FastICA algorithm, in the R package icamix. We illustrate this new methodology using several applications in unsupervised learning and image processing.","Independent component analysis, Kernel density estimation, Nonparametric estimation, Penalized smoothed likelihood, Unsupervised learning",https://link.springer.com//article/10.1007/s11634-018-0338-x,1163
Finite mixture of regression models for censored data based on scale mixtures of normal distributions,"In statistical analysis, particularly in econometrics, the finite mixture of regression models based on the normality assumption is routinely used to analyze censored data. In this work, an extension of this model is proposed by considering scale mixtures of normal distributions (SMN). This approach allows us to model data with great flexibility, accommodating multimodality and heavy tails at the same time. The main virtue of considering the finite mixture of regression models for censored data under the SMN class is that this class of models has a nice hierarchical representation which allows easy implementation of inferences. We develop a simple EM-type algorithm to perform maximum likelihood inference of the parameters in the proposed model. To examine the performance of the proposed method, we present some simulation studies and analyze a real dataset. The proposed algorithm and methods are implemented in the new R package CensMixReg.","Censoring, EM-type algorithm, Finite mixture of regression models, Scale mixtures of normal distributions",https://link.springer.com//article/10.1007/s11634-018-0337-y,1163
Finite mixture biclustering of discrete type multivariate data,"Many of the methods which deal with clustering in matrices of data are based on mathematical techniques such as distance-based algorithms or matrix decomposition and eigenvalues. In general, it is not possible to use statistical inferences or select the appropriateness of a model via information criteria with these techniques because there is no underlying probability model. This article summarizes some recent model-based methodologies for matrices of binary, count, and ordinal data, which are modelled under a unified statistical framework using finite mixtures to group the rows and/or columns. The model parameter can be constructed from a linear predictor of parameters and covariates through link functions. This likelihood-based one-mode and two-mode fuzzy clustering provides maximum likelihood estimation of parameters and the options of using likelihood information criteria for model comparison. Additionally, a Bayesian approach is presented in which the parameters and the number of clusters are estimated simultaneously from their joint posterior distribution. Visualization tools focused on ordinal data, the fuzziness of the clustering structures, and analogies of various standard plots used in the multivariate analysis are presented. Finally, a set of future extensions is enumerated.","Classification, EM algorithm, Fuzzy clustering, Mixture models, Ordinal data, RJMCMC, Visualisation tools",https://link.springer.com//article/10.1007/s11634-018-0324-3,1163
"Finite mixtures, projection pursuit and tensor rank: a triangulation","Finite mixtures of multivariate distributions play a fundamental role in model-based clustering. However, they pose several problems, especially in the presence of many irrelevant variables. Dimension reduction methods, such as projection pursuit, are commonly used to address these problems. In this paper, we use skewness-maximizing projections to recover the subspace which optimally separates the cluster means. Skewness might then be removed in order to search for other potentially interesting data structures or to perform skewness-sensitive statistical analyses, such as the Hotelling’s \( T^{2}\) test. Our approach is algebraic in nature and deals with the symmetric tensor rank of the third multivariate cumulant. We also derive closed-form expressions for the symmetric tensor rank of the third cumulants of several multivariate mixture models, including mixtures of skew-normal distributions and mixtures of two symmetric components with proportional covariance matrices. Theoretical results in this paper shed some light on the connection between the estimated number of mixture components and their skewness.","Finite mixture, Linear discriminant function, Projection pursuit, Skewness, Symmetrization, Tensor rank",https://link.springer.com//article/10.1007/s11634-018-0336-z,1163
Clustering space-time series: FSTAR as a flexible STAR approach,"The STAR model is widely used to represent the dynamics of a certain variable recorded at several locations at the same time. Its advantages are often discussed in terms of parsimony with respect to space-time VAR structures because it considers a single coefficient for each time and spatial lag. This hypothesis can be very strong; we add a certain degree of flexibility to the STAR model, providing the possibility for coefficients to vary in groups of locations. The new class of models (called Flexible STAR–FSTAR) is compared to the classical STAR and the space-time VAR by simulations and an application.","Clustering, Forecasting, Space–time models, Spatial weight matrix",https://link.springer.com//article/10.1007/s11634-018-0314-5,1163
Robust clustering for functional data based on trimming and constraints,"Many clustering algorithms when the data are curves or functions have been recently proposed. However, the presence of contamination in the sample of curves can influence the performance of most of them. In this work we propose a robust, model-based clustering method that relies on an approximation to the “density function” for functional data. The robustness follows from the joint application of data-driven trimming, for reducing the effect of contaminated observations, and constraints on the variances, for avoiding spurious clusters in the solution. The algorithm is designed to perform clustering and outlier detection simultaneously by maximizing a trimmed “pseudo” likelihood. The proposed method has been evaluated and compared with other existing methods through a simulation study. Better performance for the proposed methodology is shown when a fraction of contaminating curves is added to a non-contaminated sample. Finally, an application to a real data set that has been previously considered in the literature is given.","Functional data analysis, Clustering, Robustness, Trimming, Functional principal components analysis",https://link.springer.com//article/10.1007/s11634-018-0312-7,1163
Assessing trimming methodologies for clustering linear regression data,"We assess the performance of state-of-the-art robust clustering tools for regression structures under a variety of different data configurations. We focus on two methodologies that use trimming and restrictions on group scatters as their main ingredients. We also give particular care to the data generation process through the development of a flexible simulation tool for mixtures of regressions, where the user can control the degree of overlap between the groups. Level of trimming and restriction factors are input parameters for which appropriate tuning is required. Since we find that incorrect specification of the second-level trimming in the Trimmed CLUSTering REGression model (TCLUST-REG) can deteriorate the performance of the method, we propose an improvement where the second-level trimming is not fixed in advance but is data dependent. We then compare our adaptive version of TCLUST-REG with the Trimmed Cluster Weighted Restricted Model (TCWRM) which provides a powerful extension of the robust clusterwise regression methodology. Our overall conclusion is that the two methods perform comparably, but with notable differences due to the inherent degree of modeling implied by them.","Robust clustering, Clusterwise regression, Mixture modeling, TCLUST-REG, TCWRM, Monte Carlo experiment, MixSimReg",https://link.springer.com//article/10.1007/s11634-018-0331-4,1163
Variable selection in model-based clustering and discriminant analysis with a regularization approach,"Several methods for variable selection have been proposed in model-based clustering and classification. These make use of backward or forward procedures to define the roles of the variables. Unfortunately, such stepwise procedures are slow and the resulting algorithms inefficient when analyzing large data sets with many variables. In this paper, we propose an alternative regularization approach for variable selection in model-based clustering and classification. In our approach the variables are first ranked using a lasso-like procedure in order to avoid slow stepwise algorithms. Thus, the variable selection methodology of Maugis et al. (Comput Stat Data Anal 53:3872–3882, 2000b) can be efficiently applied to high-dimensional data sets.","Variable selection, Lasso, Gaussian mixture, Clustering, Classification",https://link.springer.com//article/10.1007/s11634-018-0322-5,1163
Random effects clustering in multilevel modeling: choosing a proper partition,A novel criterion for estimating a latent partition of the observed groups based on the output of a hierarchical model is presented. It is based on a loss function combining the Gini income inequality ratio and the predictability index of Goodman and Kruskal in order to achieve maximum heterogeneity of random effects across groups and maximum homogeneity of predicted probabilities inside estimated clusters. The index is compared with alternative approaches in a simulation study and applied in a case study concerning the role of hospital level variables in deciding for a cesarean section.,"Hierarchical modelling, Model based clustering, Label switching, Bayesian nonparametric, Gini income inequality ratio, Goodman and Kruskal predictability index",https://link.springer.com//article/10.1007/s11634-018-0347-9,1163
sARI: a soft agreement measure for class partitions incorporating assignment probabilities,"Agreement indices are commonly used to summarize the performance of both classification and clustering methods. The easy interpretation/intuition and desirable properties that result from the Rand and adjusted Rand indices, has led to their popularity over other available indices. While more algorithmic clustering approaches like k-means and hierarchical clustering produce hard partition assignments (assigning observations to a single cluster), other techniques like model-based clustering include information about the certainty of allocation of objects through class membership probabilities (soft partitions). To assess performance using traditional indices, e.g., the adjusted Rand index (ARI), the soft partition is mapped to a hard set of assignments, which commonly overstates the certainty of correct assignments. This paper proposes an extension of the ARI, the soft adjusted Rand index (sARI), with similar intuition and interpretation but also incorporating information from one or two soft partitions. It can be used in conjunction with the ARI, comparing the similarities of hard to soft, or soft to soft partitions to the similarities of the mapped hard partitions. Simulation study results support the intuition that in general, mapping to hard partitions tends to increase the measure of similarity between partitions. In applications, the sARI more accurately reflects the cluster boundary overlap commonly seen in real data.","Adjusted Rand index, Model-based clustering, Mixture models, Soft partition, Posterior probabilities, Class membership probabilities",https://link.springer.com//article/10.1007/s11634-018-0346-x,1163
Studying crime trends in the USA over the years 2000–2012,"Studying crime trends and tendencies is an important problem that helps to identify socioeconomic patterns and relationships of crucial significance. Finite mixture models are famous for their flexibility in modeling heterogeneity in data. A novel approach designed for accounting for skewness in the distributions of matrix observations is proposed and applied to the United States crime data collected between 2000 and 2012 years. Then, the model is further extended by incorporating explanatory variables. A step-by-step model development demonstrates differences and improvements associated with every stage of the process. Results obtained by the final model are illustrated and thoroughly discussed. Multiple interesting conclusions have been drawn based on the developed model and obtained model-based clustering partition.","Crime data, Finite mixture model, Matrix normal distribution, Manly transformation, EM algorithm",https://link.springer.com//article/10.1007/s11634-018-0326-1,1163
Ensemble of a subset of kNN classifiers,"Combining multiple classifiers, known as ensemble methods, can give substantial improvement in prediction performance of learning algorithms especially in the presence of non-informative features in the data sets. We propose an ensemble of subset of kNN classifiers, ESkNN, for classification task in two steps. Firstly, we choose classifiers based upon their individual performance using the out-of-sample accuracy. The selected classifiers are then combined sequentially starting from the best model and assessed for collective performance on a validation data set. We use bench mark data sets with their original and some added non-informative features for the evaluation of our method. The results are compared with usual kNN, bagged kNN, random kNN, multiple feature subset method, random forest and support vector machines. Our experimental comparisons on benchmark classification problems and simulated data sets reveal that the proposed ensemble gives better classification performance than the usual kNN and its ensembles, and performs comparable to random forest and support vector machines.","Ensemble methods, Bagging, Nearest neighbour classifier,  Non-informative features",https://link.springer.com//article/10.1007/s11634-015-0227-5,1163
Understanding non-linear modeling of measurement invariance in heterogeneous populations,"This study examined how a non-linear modeling of ordered categorical variables within multiple-group confirmatory factor analysis supported measurement invariance. A four-item classroom disciplinary climate scale used in cross-cultural framework was empirically investigated. In the first part of the analysis, a separated categorical confirmatory factor analysis was initially applied to account for the complex structure of the relationships between the observed measures in each country. The categorical multiple-group confirmatory factor analysis (MGCFA) was then used to conduct a cross-country examination of full measurement invariance namely the configural, metric, and scalar levels of invariance in the classroom discipline climate measures. The categorical MGCFA modeling supported configural and metric invariances as well as scalar invariance for the latent factor structure of classroom disciplinary climate. This finding implying meaningful cross-country comparisons on the scale means, on the associations of classroom disciplinary climate scale with other scales and on the item-factor latent structure. Application of the categorical modeling appeared to correctly specify the factor structure of the scale, thereby promising the appropriateness of reporting comparisons such as rankings of many groups, and illustrating league tables of different heterogeneous groups. Limitations of the modeling in this study and future suggestions for measurement invariance testing in studies with large numbers of groups are discussed.","Measurement invariance, Non-linear modeling, Ordinal multivariate data",https://link.springer.com//article/10.1007/s11634-016-0240-3,1163
A comparative study on large scale kernelized support vector machines,"Kernelized support vector machines (SVMs) belong to the most widely used classification methods. However, in contrast to linear SVMs, the computation time required to train such a machine becomes a bottleneck when facing large data sets. In order to mitigate this shortcoming of kernel SVMs, many approximate training algorithms were developed. While most of these methods claim to be much faster than the state-of-the-art solver LIBSVM, a thorough comparative study is missing. We aim to fill this gap. We choose several well-known approximate SVM solvers and compare their performance on a number of large benchmark data sets. Our focus is to analyze the trade-off between prediction error and runtime for different learning and accuracy parameter settings. This includes simple subsampling of the data, the poor-man’s approach to handling large scale problems. We employ model-based multi-objective optimization, which allows us to tune the parameters of learning machine and solver over the full range of accuracy/runtime trade-offs. We analyze (differences between) solvers by studying and comparing the Pareto fronts formed by the two objectives classification error and training time. Unsurprisingly, given more runtime most solvers are able to find more accurate solutions, i.e., achieve a higher prediction accuracy. It turns out that LIBSVM with subsampling of the data is a strong baseline. Some solvers systematically outperform others, which allows us to give concrete recommendations of when to use which solver.","Support vector machine, Multi-objective optimization, Supervised learning, Machine learning, Large scale, Nonlinear SVM, Parameter tuning",https://link.springer.com//article/10.1007/s11634-016-0265-7,1163
A computationally fast variable importance test for random forests for high-dimensional data,"Random forests are a commonly used tool for classification and for ranking candidate predictors based on the so-called variable importance measures. These measures attribute scores to the variables reflecting their importance. A drawback of variable importance measures is that there is no natural cutoff that can be used to discriminate between important and non-important variables. Several approaches, for example approaches based on hypothesis testing, were developed for addressing this problem. The existing testing approaches require the repeated computation of random forests. While for low-dimensional settings those approaches might be computationally tractable, for high-dimensional settings typically including thousands of candidate predictors, computing time is enormous. In this article a computationally fast heuristic variable importance test is proposed that is appropriate for high-dimensional data where many variables do not carry any information. The testing approach is based on a modified version of the permutation variable importance, which is inspired by cross-validation procedures. The new approach is tested and compared to the approach of Altmann and colleagues using simulation studies, which are based on real data from high-dimensional binary classification settings. The new approach controls the type I error and has at least comparable power at a substantially smaller computation time in the studies. Thus, it might be used as a computationally fast alternative to existing procedures for high-dimensional data settings where many variables do not carry any information. The new approach is implemented in the R package vita.","Gene selection, Feature selection, Random forests, Variable importance, Variable selection, Variable importance test",https://link.springer.com//article/10.1007/s11634-016-0276-4,1163
Rank-based classifiers for extremely high-dimensional gene expression data,"Predicting phenotypes on the basis of gene expression profiles is a classification task that is becoming increasingly important in the field of precision medicine. Although these expression signals are real-valued, it is questionable if they can be analyzed on an interval scale. As with many biological signals their influence on e.g. protein levels is usually non-linear and thus can be misinterpreted. In this article we study gene expression profiles with up to 54,000 dimensions. We analyze these measurements on an ordinal scale by replacing the real-valued profiles by their ranks. This type of rank transformation can be used for the construction of invariant classifiers that are not affected by noise induced by data transformations which can occur in the measurement setup. Our 10 \(\times \) 10 fold cross-validation experiments on 86 different data sets and 19 different classification models indicate that classifiers largely benefit from this transformation. Especially random forests and support vector machines achieve improved classification results on a significant majority of datasets.","Rank-based classification, Invariance, High-dimensional data, Gene expression data",https://link.springer.com//article/10.1007/s11634-016-0277-3,1163
Ensemble feature selection for high dimensional data: a new method and a comparative study,The curse of dimensionality is based on the fact that high dimensional data is often difficult to work with. A large number of features can increase the noise of the data and thus the error of a learning algorithm. Feature selection is a solution for such problems where there is a need to reduce the data dimensionality. Different feature selection algorithms may yield feature subsets that can be considered local optima in the space of feature subsets. Ensemble feature selection combines independent feature subsets and might give a better approximation to the optimal subset of features. We propose an ensemble feature selection approach based on feature selectors’ reliability assessment. It aims at providing a unique and stable feature selection without ignoring the predictive accuracy aspect. A classification algorithm is used as an evaluator to assign a confidence to features selected by ensemble members based on their associated classification performance. We compare our proposed approach to several existing techniques and to individual feature selection algorithms. Results show that our approach often improves classification performance and feature selection stability for high dimensional data sets.,"Feature selection, Ensemble methods, Classification, Stability, High dimensionality",https://link.springer.com//article/10.1007/s11634-017-0285-y,1163
An efficient random forests algorithm for high dimensional data classification,"In this paper, we propose a new random forest (RF) algorithm to deal with high dimensional data for classification using subspace feature sampling method and feature value searching. The new subspace sampling method maintains the diversity and randomness of the forest and enables one to generate trees with a lower prediction error. A greedy technique is used to handle cardinal categorical features for efficient node splitting when building decision trees in the forest. This allows trees to handle very high cardinality meanwhile reducing computational time in building the RF model. Extensive experiments on high dimensional real data sets including standard machine learning data sets and image data sets have been conducted. The results demonstrated that the proposed approach for learning RFs significantly reduced prediction errors and outperformed most existing RFs when dealing with high-dimensional data.","Classification, Image classification, High dimensional data, Random forests, Data mining",https://link.springer.com//article/10.1007/s11634-018-0318-1,1163
Equi-Clustream: a framework for clustering time evolving mixed data,"In data stream environment, most of the conventional clustering algorithms are not sufficiently efficient, since large volumes of data arrive in a stream and these data points unfold with time. The problem of clustering time-evolving metric data and categorical time-evolving data has separately been well explored in recent years, but the problem of clustering mixed type time-evolving data remains a challenging issue due to an awkward gap between the structure of metric and categorical attributes. In this paper, we devise a generalized framework, termed Equi-Clustream to dynamically cluster mixed type time-evolving data, which comprises three algorithms: a Hybrid Drifting Concept Detection Algorithm that detects the drifting concept between the current sliding window and previous sliding window, a Hybrid Data Labeling Algorithm that assigns an appropriate cluster label to each data vector of the current non-drifting window based on the clustering result of the previous sliding window, and a visualization algorithm that analyses the relationship between the clusters at different timestamps and also visualizes the evolving trends of the clusters. The efficacy of the proposed framework is shown by experiments on synthetic and real world datasets.","Clustering, Data streams, Time-evolving data, Data mining",https://link.springer.com//article/10.1007/s11634-018-0316-3,1163
"Mutual information, phi-squared and model-based co-clustering for contingency tables","Many of the datasets encountered in statistics are two-dimensional in nature and can be represented by a matrix. Classical clustering procedures seek to construct separately an optimal partition of rows or, sometimes, of columns. In contrast, co-clustering methods cluster the rows and the columns simultaneously and organize the data into homogeneous blocks (after suitable permutations). Methods of this kind have practical importance in a wide variety of applications such as document clustering, where data are typically organized in two-way contingency tables. Our goal is to offer coherent frameworks for understanding some existing criteria and algorithms for co-clustering contingency tables, and to propose new ones. We look at two different frameworks for the problem of co-clustering. The first involves minimizing an objective function based on measures of association and in particular on phi-squared and mutual information. The second uses a model-based co-clustering approach, and we consider two models: the block model and the latent block model. We establish connections between different approaches, criteria and algorithms, and we highlight a number of implicit assumptions in some commonly used algorithms. Our contribution is illustrated by numerical experiments on simulated and real-case datasets that show the relevance of the presented methods in the document clustering field.","Co-clustering, Biclustering, Contingency table, Information theory",https://link.springer.com//article/10.1007/s11634-016-0274-6,1163
Model selection for Gaussian latent block clustering with the integrated classification likelihood,"Block clustering aims to reveal homogeneous block structures in a data table. Among the different approaches of block clustering, we consider here a model-based method: the Gaussian latent block model for continuous data which is an extension of the Gaussian mixture model for one-way clustering. For a given data table, several candidate models are usually examined, which differ for example in the number of clusters. Model selection then becomes a critical issue. To this end, we develop a criterion based on an approximation of the integrated classification likelihood for the Gaussian latent block model, and propose a Bayesian information criterion-like variant following the same pattern. We also propose a non-asymptotic exact criterion, thus circumventing the controversial definition of the asymptotic regime arising from the dual nature of the rows and columns in co-clustering. The experimental results show steady performances of these criteria for medium to large data tables.","Co-clustering, Latent block model, Model selection, Continuous data, Integrated classification likelihood, BIC",https://link.springer.com//article/10.1007/s11634-013-0161-3,1163
Discovering patterns in time-varying graphs: a triclustering approach,"This paper introduces a novel technique to track structures in time varying graphs. The method uses a maximum a posteriori approach for adjusting a three-dimensional co-clustering of the source vertices, the destination vertices and the time, to the data under study, in a way that does not require any hyper-parameter tuning. The three dimensions are simultaneously segmented in order to build clusters of source vertices, destination vertices and time segments where the edge distributions across clusters of vertices follow the same evolution over the time segments. The main novelty of this approach lies in that the time segments are directly inferred from the evolution of the edge distribution between the vertices, thus not requiring the user to make any a priori quantization. Experiments conducted on artificial data illustrate the good behavior of the technique, and a study of a real-life data set shows the potential of the proposed approach for exploratory data analysis.","Co-clustering, Time-varying graph, Graph mining, Model selection",https://link.springer.com//article/10.1007/s11634-015-0218-6,1163
Cluster-based sparse topical coding for topic mining and document clustering,"In this paper, we introduce a document clustering method based on Sparse Topical Coding, called Cluster-based Sparse Topical Coding. Topic modeling is capable of improving textual document clustering by describing documents via bag-of-words models and projecting them into a topic space. The latent semantic descriptions derived by the topic model can be utilized as features in a clustering process. In our proposed method, document clustering and topic modeling are integrated in a unified framework in order to achieve the highest performance. This framework includes Sparse Topical Coding, which is responsible for topic mining, and K-means that discovers the latent clusters in documents collection. Experimental results on widely-used datasets show that our proposed method significantly outperforms the traditional and other topic model based clustering methods. Our method achieves from 4 to 39% improvement in clustering accuracy and from 2% to more than 44% improvement in normalized mutual information.","Document clustering, Topic model, Sparse topical coding, K-means",https://link.springer.com//article/10.1007/s11634-017-0280-3,1163
Sparsest factor analysis for clustering variables: a matrix decomposition approach,"We propose a new procedure for sparse factor analysis (FA) such that each variable loads only one common factor. Thus, the loading matrix has a single nonzero element in each row and zeros elsewhere. Such a loading matrix is the sparsest possible for certain number of variables and common factors. For this reason, the proposed method is named sparsest FA (SSFA). It may also be called FA-based variable clustering, since the variables loading the same common factor can be classified into a cluster. In SSFA, all model parts of FA (common factors, their correlations, loadings, unique factors, and unique variances) are treated as fixed unknown parameter matrices and their least squares function is minimized through specific data matrix decomposition. A useful feature of the algorithm is that the matrix of common factor scores is re-parameterized using QR decomposition in order to efficiently estimate factor correlations. A simulation study shows that the proposed procedure can exactly identify the true sparsest models. Real data examples demonstrate the usefulness of the variable clustering performed by SSFA.","Exploratory factor analysis, Sparsest loadings, Matrix decomposition factor analysis, Variable clustering, QR re-parameterization",https://link.springer.com//article/10.1007/s11634-017-0284-z,1163
Minimum distance method for directional data and outlier detection,"In this paper, we propose estimators based on the minimum distance for the unknown parameters of a parametric density on the unit sphere. We show that these estimators are consistent and asymptotically normally distributed. Also, we apply our proposal to develop a method that allows us to detect potential atypical values. The behavior under small samples of the proposed estimators is studied using Monte Carlo simulations. Two applications of our procedure are illustrated with real data sets.","Directional data, Robust estimation, Outlier detection, Asymptotic properties",https://link.springer.com//article/10.1007/s11634-017-0287-9,1163
Statistical inference in constrained latent class models for multinomial data based on \(\phi \)-divergence measures,"In this paper we explore the possibilities of applying \(\phi \)-divergence measures in inferential problems in the field of latent class models (LCMs) for multinomial data. We first treat the problem of estimating the model parameters. As explained below, minimum \(\phi \)-divergence estimators (M\(\phi \)Es) considered in this paper are a natural extension of the maximum likelihood estimator (MLE), the usual estimator for this problem; we study the asymptotic properties of M\(\phi \)Es, showing that they share the same asymptotic distribution as the MLE. To compare the efficiency of the M\(\phi \)Es when the sample size is not big enough to apply the asymptotic results, we have carried out an extensive simulation study; from this study, we conclude that there are estimators in this family that are competitive with the MLE. Next, we deal with the problem of testing whether a LCM for multinomial data fits a data set; again, \(\phi \)-divergence measures can be used to generate a family of test statistics generalizing both the classical likelihood ratio test and the chi-squared test statistics. Finally, we treat the problem of choosing the best model out of a sequence of nested LCMs; as before, \(\phi \)-divergence measures can handle the problem and we derive a family of \(\phi \)-divergence test statistics based on them; we study the asymptotic behavior of these test statistics, showing that it is the same as the classical test statistics. A simulation study for small and moderate sample sizes shows that there are some test statistics in the family that can compete with the classical likelihood ratio and the chi-squared test statistics.","Latent class models, Minimum \(\phi \)-divergence estimator, Maximum likelihood estimator, \(\phi \)-Divergence test statistics, Goodness-of-fit, Nested latent class models",https://link.springer.com//article/10.1007/s11634-017-0289-7,1163
A divisive clustering method for functional data with special consideration of outliers,"This paper presents DivClusFD, a new divisive hierarchical method for the non-supervised classification of functional data. Data of this type present the peculiarity that the differences among clusters may be caused by changes as well in level as in shape. Different clusters can be separated in different subregion and there may be no subregion in which all clusters are separated. In each step of division, the DivClusFD method explores the functions and their derivatives at several fixed points, seeking the subregion in which the highest number of clusters can be separated. The number of clusters is estimated via the gap statistic. The functions are assigned to the new clusters by combining the k-means algorithm with the use of functional boxplots to identify functions that have been incorrectly classified because of their atypical local behavior. The DivClusFD method provides the number of clusters, the classification of the observed functions into the clusters and guidelines that may be for interpreting the clusters. A simulation study using synthetic data and tests of the performance of the DivClusFD method on real data sets indicate that this method is able to classify functions accurately.","Hierarchical clustering, Functional boxplot, Gap statistic",https://link.springer.com//article/10.1007/s11634-017-0290-1,1163
Signal classification with a point process distance on the space of persistence diagrams,"In this paper, we consider the problem of signal classification. First, the signal is translated into a persistence diagram through the use of delay-embedding and persistent homology. Endowing the data space of persistence diagrams with a metric from point processes, we show that it admits statistical structure in the form of Fréchet means and variances and a classification scheme is established. In contrast with the Wasserstein distance, this metric accounts for changes in small persistence and changes in cardinality. The classification results using this distance are benchmarked on both synthetic data and real acoustic signals and it is demonstrated that this classifier outperforms current signal classification techniques.","Classification of time series, Data space of persistence diagrams, Wasserstein metric, Cardinality, Persistent homology",https://link.springer.com//article/10.1007/s11634-017-0294-x,1163
Rethinking an ROC partial area index for evaluating the classification performance at a high specificity range,"The area under a receiver operating characteristic (ROC) curve is valuable for evaluating the classification performance described by the entire ROC curve in many fields including decision making and medical diagnosis. However, this can be misleading when clinical tasks demand a restricted specificity range. The partial area under a portion of the ROC curve (\({ pAUC}\)) has more practical relevance in such situations, but it is usually transformed to overcome some drawbacks and improve its interpretation. The standardized \({ pAUC}\) (\({ SpAUC}\)) index is considered as a meaningful relative measure of predictive accuracy. Nevertheless, this \({ SpAUC}\) index might still show some limitations due to ROC curves crossing the diagonal line, and to the problem when comparing two tests with crossing ROC curves in the same restricted specificity range. This paper provides an alternative \({ pAUC}\) index which overcomes these limitations. Tighter bounds for the \({ pAUC}\) of an ROC curve are derived, and then a modified \({ pAUC}\) index for any restricted specificity range is established. In addition, the proposed tighter partial area index (\({ TpAUC}\)) is also shown for classifier when high specificity must be clinically maintained. The variance of the \({ TpAUC}\) is also studied analytically and by simulation studies in a theoretical framework based on the most typical assumption of a binormal model, and estimated by using nonparametric bootstrap resampling in the empirical examples. Simulated and real datasets illustrate the practical utility of the \({ TpAUC}\).","ROC curve, Partial area under ROC curve, Classification performance, Binormal model, Bootstrap, Predictive accuracy",https://link.springer.com//article/10.1007/s11634-017-0295-9,1163
Archetypal shapes based on landmarks and extension to handle missing data,"Archetype and archetypoid analysis are extended to shapes. The objective is to find representative shapes. Archetypal shapes are pure (extreme) shapes. We focus on the case where the shape of an object is represented by a configuration matrix of landmarks. As shape space is not a vectorial space, we work in the tangent space, the linearized space about the mean shape. Then, each observation is approximated by a convex combination of actual observations (archetypoids) or archetypes, which are a convex combination of observations in the data set. These tools can contribute to the understanding of shapes, as in the usual multivariate case, since they lie somewhere between clustering and matrix factorization methods. A new simplex visualization tool is also proposed to provide a picture of the archetypal analysis results. We also propose new algorithms for performing archetypal analysis with missing data and its extension to incomplete shapes. A well-known data set is used to illustrate the methodologies developed. The proposed methodology is applied to an apparel design problem in children.","Statistical shape analysis, Archetype analysis, Archetypoid analysis, Anthropometric data, Children’s wear, Missing data",https://link.springer.com//article/10.1007/s11634-017-0297-7,1163
Tree-structured modelling of categorical predictors in generalized additive regression,"Generalized linear and additive models are very efficient regression tools but many parameters have to be estimated if categorical predictors with many categories are included. The method proposed here focusses on the main effects of categorical predictors by using tree type methods to obtain clusters of categories. When the predictor has many categories one wants to know in particular which of the categories have to be distinguished with respect to their effect on the response. The tree-structured approach allows to detect clusters of categories that share the same effect while letting other predictors, in particular metric predictors, have a linear or additive effect on the response. An algorithm for the fitting is proposed and various stopping criteria are evaluated. The preferred stopping criterion is based on p values representing a conditional inference procedure. In addition, stability of clusters is investigated and the relevance of predictors is investigated by bootstrap methods. Several applications show the usefulness of the tree-structured approach and small simulation studies demonstrate that the fitting procedure works well.","Categorical predictors, Tree-structured clustering, Recursive partitioning, Partially linear tree-based regression",https://link.springer.com//article/10.1007/s11634-017-0298-6,1163
Non-symmetrical composite-based path modeling,"Partial least squares path modeling presents some inconsistencies in terms of coherence with the predictive directions specified in the inner model (i.e. the path directions), because the directions of the links in the inner model are not taken into account in the iterative algorithm. In fact, the procedure amplifies interdependence among blocks and fails to distinguish between dependent and explanatory blocks. The method proposed in this paper takes into account and respects the specified path directions, with the aim of improving the predictive ability of the model and to maintain the hypothesized theoretical inner model. To highlight its properties, the proposed method is compared to the classical PLS path modeling in terms of explained variability, predictive relevance and interpretation using artificial data through a real data application. A further development of the method allows to treat multi-dimensional blocks in composite-based path modeling.","PLS path modeling, Non-symmetrical analysis, Predictive composite-based methods",https://link.springer.com//article/10.1007/s11634-017-0302-1,1163
Outlier detection in interval data,"A multivariate outlier detection method for interval data is proposed that makes use of a parametric approach to model the interval data. The trimmed maximum likelihood principle is adapted in order to robustly estimate the model parameters. A simulation study demonstrates the usefulness of the robust estimates for outlier detection, and new diagnostic plots allow gaining deeper insight into the structure of real world interval data.","Outliers, Robust statistics, Interval data, Mahalanobis distance",https://link.springer.com//article/10.1007/s11634-017-0305-y,1163
Probabilistic clustering via Pareto solutions and significance tests,"The present paper proposes a new strategy for probabilistic (often called model-based) clustering. It is well known that local maxima of mixture likelihoods can be used to partition an underlying data set. However, local maxima are rarely unique. Therefore, it remains to select the reasonable solutions, and in particular the desired one. Credible partitions are usually recognized by separation (and cohesion) of their clusters. We use here the p values provided by the classical tests of Wilks, Hotelling, and Behrens–Fisher to single out those solutions that are well separated by location. It has been shown that reasonable solutions to a clustering problem are related to Pareto points in a plot of scale balance vs. model fit of all local maxima. We briefly review this theory and propose as solutions all well-fitting Pareto points in the set of local maxima separated by location in the above sense. We also design a new iterative, parameter-free cutting plane algorithm for the multivariate Behrens–Fisher problem.","Cluster analysis, Probabilistic models, Mixture model, Classification model, Pareto solutions, Behrens–Fisher problem, Hotelling’s \(T^2\) statistic, Wilks’ lambda",https://link.springer.com//article/10.1007/s11634-016-0278-2,1163
Eigenvalues and constraints in mixture modeling: geometric and computational issues,"This paper presents a review about the usage of eigenvalues restrictions for constrained parameter estimation in mixtures of elliptical distributions according to the likelihood approach. The restrictions serve a twofold purpose: to avoid convergence to degenerate solutions and to reduce the onset of non interesting (spurious) local maximizers, related to complex likelihood surfaces. The paper shows how the constraints may play a key role in the theory of Euclidean data clustering. The aim here is to provide a reasoned survey of the constraints and their applications, considering the contributions of many authors and spanning the literature of the last 30 years.","Mixture model, EM algorithm, Eigenvalues, Model-based clustering",https://link.springer.com//article/10.1007/s11634-017-0293-y,1163
A data driven equivariant approach to constrained Gaussian mixture modeling,"Maximum likelihood estimation of Gaussian mixture models with different class-specific covariance matrices is known to be problematic. This is due to the unboundedness of the likelihood, together with the presence of spurious maximizers. Existing methods to bypass this obstacle are based on the fact that unboundedness is avoided if the eigenvalues of the covariance matrices are bounded away from zero. This can be done imposing some constraints on the covariance matrices, i.e. by incorporating a priori information on the covariance structure of the mixture components. The present work introduces a constrained approach, where the class conditional covariance matrices are shrunk towards a pre-specified target matrix \(\varvec{\varPsi }.\) Data-driven choices of the matrix \(\varvec{\varPsi },\) when a priori information is not available, and the optimal amount of shrinkage are investigated. Then, constraints based on a data-driven \(\varvec{\varPsi }\) are shown to be equivariant with respect to linear affine transformations, provided that the method used to select the target matrix be also equivariant. The effectiveness of the proposal is evaluated on the basis of a simulation study and an empirical example.","Model based clustering, Gaussian mixture models, Equivariant estimators",https://link.springer.com//article/10.1007/s11634-016-0279-1,1163
Clustering of imbalanced high-dimensional media data,"Media content in large repositories usually exhibits multiple groups of strongly varying sizes. Media of potential interest often form notably smaller groups. Such media groups differ so much from the remaining data that it may be worthy to look at them in more detail. In contrast, media with popular content appear in larger groups. Identifying groups of varying sizes is addressed by clustering of imbalanced data. Clustering highly imbalanced media groups is additionally challenged by the high dimensionality of the underlying features. In this paper, we present the imbalanced clustering (IClust) algorithm designed to reveal group structures in high-dimensional media data. IClust employs an existing clustering method in order to find an initial set of a large number of potentially highly pure clusters which are then successively merged. The main advantage of IClust is that the number of clusters does not have to be pre-specified and that no specific assumptions about the cluster or data characteristics need to be made. Experiments on real-world media data demonstrate that in comparison to existing methods, IClust is able to better identify media groups, especially groups of small sizes.","Clustering, Imbalanced data, High-dimensional data, Media data, LOF",https://link.springer.com//article/10.1007/s11634-017-0292-z,1163
Clusterwise analysis for multiblock component methods,"Multiblock component methods are applied to data sets for which several blocks of variables are measured on a same set of observations with the goal to analyze the relationships between these blocks of variables. In this article, we focus on multiblock component methods that integrate the information found in several blocks of explanatory variables in order to describe and explain one set of dependent variables. In the following, multiblock PLS and multiblock redundancy analysis are chosen, as particular cases of multiblock component methods when one set of variables is explained by a set of predictor variables that is organized into blocks. Because these multiblock techniques assume that the observations come from a homogeneous population they will provide suboptimal results when the observations actually come from different populations. A strategy to palliate this problem—presented in this article—is to use a technique such as clusterwise regression in order to identify homogeneous clusters of observations. This approach creates two new methods that provide clusters that have their own sets of regression coefficients. This combination of clustering and regression improves the overall quality of the prediction and facilitates the interpretation. In addition, the minimization of a well-defined criterion—by means of a sequential algorithm—ensures that the algorithm converges monotonously. Finally, the proposed method is distribution-free and can be used when the explanatory variables outnumber the observations within clusters. The proposed clusterwise multiblock methods are illustrated with of a simulation study and a (simulated) example from marketing.","Multiblock component method, Clusterwise regression, Typological regression, Cluster analysis, Dimension reduction",https://link.springer.com//article/10.1007/s11634-017-0296-8,1163
Asymptotic comparison of semi-supervised and supervised linear discriminant functions for heteroscedastic normal populations,"It has been reported that using unlabeled data together with labeled data to construct a discriminant function works successfully in practice. However, theoretical studies have implied that unlabeled data can sometimes adversely affect the performance of discriminant functions. Therefore, it is important to know what situations call for the use of unlabeled data. In this paper, asymptotic relative efficiency is presented as the measure for comparing analyses with and without unlabeled data under the heteroscedastic normality assumption. The linear discriminant function maximizing the area under the receiver operating characteristic curve is considered. Asymptotic relative efficiency is evaluated to investigate when and how unlabeled data contribute to improving discriminant performance under several conditions. The results show that asymptotic relative efficiency depends mainly on the heteroscedasticity of the covariance matrices and the stochastic structure of observing the labels of the cases.","Area under the ROC curve, Labeling mechanism, Linear discriminant function, Missing data, Receiver operating characteristic curve, Semi-supervised learning",https://link.springer.com//article/10.1007/s11634-016-0266-6,1163
Local generalized quadratic distance metrics: application to the k-nearest neighbors classifier,"Finding the set of nearest neighbors for a query point of interest appears in a variety of algorithms for machine learning and pattern recognition. Examples include k nearest neighbor classification, information retrieval, case-based reasoning, manifold learning, and nonlinear dimensionality reduction. In this work, we propose a new approach for determining a distance metric from the data for finding such neighboring points. For a query point of interest, our approach learns a generalized quadratic distance (GQD) metric based on the statistical properties in a “small” neighborhood for the point of interest. The locally learned GQD metric captures information such as the density, curvature, and the intrinsic dimensionality for the points falling in this particular neighborhood. Unfortunately, learning the GQD parameters under such a local learning mechanism is a challenging problem with a high computational overhead. To address these challenges, we estimate the GQD parameters using the minimum volume covering ellipsoid (MVCE) for a set of points. The advantage of the MVCE is two-fold. First, the MVCE together with the local learning approach approximate the functionality of a well known robust estimator for covariance matrices. Second, computing the MVCE is a convex optimization problem which, in addition to having a unique global solution, can be efficiently solved using a first order optimization algorithm. We validate our metric learning approach on a large variety of datasets and show that the proposed metric has promising results when compared with five algorithms from the literature for supervised metric learning.","Query-based operations, k Nearest neighbors, Distance metric learning, Minimum volume covering ellipsoid, Minimum volume ellipsoid estimator",https://link.springer.com//article/10.1007/s11634-017-0286-x,1163
Unsupervised classification of children’s bodies using currents,"Object classification according to their shape and size is of key importance in many scientific fields. This work focuses on the case where the size and shape of an object is characterized by a current. A current is a mathematical object which has been proved relevant to the modeling of geometrical data, like submanifolds, through integration of vector fields along them. As a consequence of the choice of a vector-valued reproducing kernel Hilbert space (RKHS) as a test space for integrating manifolds, it is possible to consider that shapes are embedded in this Hilbert Space. A vector-valued RKHS is a Hilbert space of vector fields; therefore, it is possible to compute a mean of shapes, or to calculate a distance between two manifolds. This embedding enables us to consider size-and-shape clustering algorithms. These algorithms are applied to a 3D database obtained from an anthropometric survey of the Spanish child population with a potential application to online sales of children’s wear.","Currents, Statistical shape analysis, Reproducing kernel Hilbert space, Children’s body shapes, k-Means",https://link.springer.com//article/10.1007/s11634-017-0283-0,1163
A semiparametric Bayesian joint model for multiple mixed-type outcomes: an application to acute myocardial infarction,"We propose a Bayesian semiparametric regression model to represent mixed-type multiple outcomes concerning patients affected by Acute Myocardial Infarction. Our approach is motivated by data coming from the ST-Elevation Myocardial Infarction (STEMI) Archive, a multi-center observational prospective clinical study planned as part of the Strategic Program of Lombardy, Italy. We specifically consider a joint model for a variable measuring treatment time and in-hospital and 60-day survival indicators. One of our main motivations is to understand how the various hospitals differ in terms of the variety of information collected as part of the study. To do so we postulate a semiparametric random effects model that incorporates dependence on a location indicator that is used to explicitly differentiate among hospitals in or outside the city of Milano. The model is based on the two parameter Poisson-Dirichlet prior, also known as the Pitman-Yor process prior. We discuss the resulting posterior inference, including sensitivity analysis, and a comparison with the particular sub-model arising when a Dirichlet process prior is assumed.","Bayesian clustering, Bayesian nonparametrics, Two parameter Poisson-Dirichlet process prior, Random-effects models, Random partition models, Unbalanced binary outcomes",https://link.springer.com//article/10.1007/s11634-016-0273-7,1163
D-trace estimation of a precision matrix using adaptive Lasso penalties,"The accurate estimation of a precision matrix plays a crucial role in the current age of high-dimensional data explosion. To deal with this problem, one of the prominent and commonly used techniques is the \(\ell _1\) norm (Lasso) penalization for a given loss function. This approach guarantees the sparsity of the precision matrix estimate for properly selected penalty parameters. However, the \(\ell _1\) norm penalization often fails to control the bias of obtained estimator because of its overestimation behavior. In this paper, we introduce two adaptive extensions of the recently proposed \(\ell _1\) norm penalized D-trace loss minimization method. They aim at reducing the produced bias in the estimator. Extensive numerical results, using both simulated and real datasets, show the advantage of our proposed estimators.","Adaptive thresholding, D-trace loss, Gaussian graphical model, Gene expression data, High-dimensionality",https://link.springer.com//article/10.1007/s11634-016-0272-8,1163
Methods for the analysis of asymmetric pairwise relationships,Asymmetric pairwise relationships are frequently observed in experimental and non-experimental studies. They can be analysed with different aims and approaches. A brief review of models and methods of multidimensional scaling and cluster analysis able to deal with asymmetric proximities is provided taking a ‘data-analytic’ approach and emphasizing data visualization.,"Asymmetry, Multidimensional scaling, Visualization, Cluster analysis",https://link.springer.com//article/10.1007/s11634-017-0307-9,1163
Skew symmetry in retrospect,The paper gives a short account of how I became interested in analysing asymmetry in square tables. The early history of the canonical analysis of skew-symmetry and the associated development of its geometrical interpretation are described.,"Skew-symmetry, Canonical analysis of skew-symmetric matrices, Singular value decomposition of skew matrices, Hedra, Bimensions, Triangle diagrams",https://link.springer.com//article/10.1007/s11634-014-0181-7,1163
CLUSKEXT: CLUstering model for SKew-symmetric data including EXTernal information,"A CLUstering model for SKew-symmetric data including EXTernal information (CLUSKEXT) is proposed, which relies on the decomposition of a skew-symmetric matrix into within and between cluster effects which are further decomposed into regression and residual effects when possible external information on the objects is available. In order to fit the imbalances between objects, the model jointly searches for a partition of objects and appropriate weights which are in turn linearly linked to the external variables. The proposal is fitted in a least-squares framework and a decomposition of the fit is derived. An appropriate Alternating Least-Squares algorithm is provided to fit the model to illustrative real and artificial data.","Asymmetric data, Skew-symmetric matrix, Clustering,  External information",https://link.springer.com//article/10.1007/s11634-015-0203-0,1163
Hierarchical clustering of asymmetric networks,"This paper considers networks where relationships between nodes are represented by directed dissimilarities. The goal is to study methods that, based on the dissimilarity structure, output hierarchical clusters, i.e., a family of nested partitions indexed by a connectivity parameter. Our construction of hierarchical clustering methods is built around the concept of admissible methods, which are those that abide by the axioms of value—nodes in a network with two nodes are clustered together at the maximum of the two dissimilarities between them—and transformation—when dissimilarities are reduced, the network may become more clustered but not less. Two particular methods, termed reciprocal and nonreciprocal clustering, are shown to provide upper and lower bounds in the space of admissible methods. Furthermore, alternative clustering methodologies and axioms are considered. In particular, modifying the axiom of value such that clustering in two-node networks occurs at the minimum of the two dissimilarities entails the existence of a unique admissible clustering method. Finally, the developed clustering methods are implemented to analyze the internal migration in the United States.","Hierarchical clustering, Asymmetric network, Directed graph, Axiomatic construction, Reciprocal clustering, Nonreciprocal clustering",https://link.springer.com//article/10.1007/s11634-017-0299-5,1163
Transitional modeling of experimental longitudinal data with missing values,"Longitudinal categorical data are often collected using an experimental design where the interest is in the differential development of the treatment group compared to the control group. Such differential development is often assessed based on average growth curves but can also be based on transitions. For longitudinal multinomial data we describe a transitional methodology for the statistical analysis based on a distance model. Such a distance approach has two advantages compared to a multinomial regression model: (1) sparse data can be handled more efficiently; (2) a graphical representation of the model can be made to enhance interpretation. Within this approach it is possible to jointly model the observations and missing values by adding a new category to the response variable representing the missingness condition. This approach is investigated in a Monte Carlo simulation study. The results show this is a promising way to deal with missing data, although the mechanism is not yet completely understood in all cases. Finally, an empirical example is presented where the advantages of the modeling procedure are highlighted.","Longitudinal data, Missing values, Multinomial data, Multidimensional scaling, Multinomial regression",https://link.springer.com//article/10.1007/s11634-015-0226-6,1163
Assessing the asymmetric effects on branch rivalry of Spanish financial sector restructuring,"Spanish financial institutions have been heavily affected by the banking crisis that began in 2008. Many of them, especially Spanish savings banks (or Cajas), had to merge with other institutions or had to be rescued. We address the question of up to what point the nature of competition in this sector has changed as a result of the crisis. Although institutions compete in many ways, we concentrate on their presence in the main street through bank branches open to the public (i.e., retail banking competition). Our measure of inter-firm rivalry is based on a geographical proximity measure that we calculate for the years 2008 (before the crisis) and 2012 (the last available data set). The technical approach is based on multidimensional unfolding, a methodology which allows us to graphically represent the asymmetric nature of such rivalry. These maps visualise the salient aspects of the system during the two dates analysed, and can be understood without a detailed technical knowledge.","Strategy, Competitive market structure, Asymmetric competition, Multidimensional unfolding, Bank branches",https://link.springer.com//article/10.1007/s11634-014-0186-2,1163
Relating brand confusion to ad similarities and brand strengths through image data analysis and classification,"Brand confusion occurs when a consumer is exposed to an advertisement (ad) for brand A but believes that it is for brand B. If more consumers are confused in this direction than in the other one (assuming that an ad for B is for A), this asymmetry is a disadvantage for A. Consequently, the confusion potential and structure of ads has to be checked: A sample of consumers is exposed to a sample of ads. For each ad the consumers have to specify their guess about the advertised brand. Then, the collected data are aggregated and analyzed using, e.g., MDS or two-mode clustering. In this paper we compare this approach to a new one where image data analysis and classification is applied: The confusion potential and structure of ads is related to featurewise distances between ads and—to model asymmetric effects—to the strengths of the advertised brands. A sample application for the German beer market is presented, the results are encouraging.","Brand confusion, Confusion experiment, Image data analysis and classification, Multinomial logit model, Two-mode hierarchical cluster analysis",https://link.springer.com//article/10.1007/s11634-017-0282-1,1163
Parametric classification with soft labels using the evidential EM algorithm: linear discriminant analysis versus logistic regression,"Partially supervised learning extends both supervised and unsupervised learning, by considering situations in which only partial information about the response variable is available. In this paper, we consider partially supervised classification and we assume the learning instances to be labeled by Dempster–Shafer mass functions, called soft labels. Linear discriminant analysis and logistic regression are considered as special cases of generative and discriminative parametric models. We show that the evidential EM algorithm can be particularized to fit the parameters in each of these models. We describe experimental results with simulated data sets as well as with two real applications: K-complex detection in sleep EEGs signals and facial expression recognition. These results confirm the interest of using soft labels for classification as compared to potentially erroneous crisp labels, when the true class membership is partially unknown or ill-defined.","Partially supervised learning, Belief functions, Dempster–Shafer theory, Machine learning, Uncertain data, Discriminant analysis, Logistic regression",https://link.springer.com//article/10.1007/s11634-017-0301-2,1163
A fuzzy approach to robust regression clustering,A new robust fuzzy regression clustering method is proposed. We estimate coefficients of a linear regression model in each unknown cluster. Our method aims to achieve robustness by trimming a fixed proportion of observations. Assignments to clusters are fuzzy: observations contribute to estimates in more than one single cluster. We describe general criteria for tuning the method. The proposed method seems to be robust with respect to different types of contamination.,"Robustness, Fuzzy clustering, Trimming, Regression clustering",https://link.springer.com//article/10.1007/s11634-016-0271-9,1163
Fuzzy rule based classification systems for big data with MapReduce: granularity analysis,"Due to the vast amount of information available nowadays, and the advantages related to the processing of this data, the topics of big data and data science have acquired a great importance in the current research. Big data applications are mainly about scalability, which can be achieved via the MapReduce programming model.It is designed to divide the data into several chunks or groups that are processed in parallel, and whose result is “assembled” to provide a single solution. Among different classification paradigms adapted to this new framework, fuzzy rule based classification systems have shown interesting results with a MapReduce approach for big data. It is well known that the performance of these types of systems has a strong dependence on the selection of a good granularity level for the Data Base. However, in the context of MapReduce this parameter is even harder to determine as it can be also related with the number of Maps chosen for the processing stage. In this paper, we aim at analyzing the interrelation between the number of labels of the fuzzy variables and the scarcity of the data due to the data sampling in MapReduce. Specifically, we consider that as the partitioning of the initial instance set grows, the level of granularity necessary to achieve a good performance also becomes higher. The experimental results, carried out for several Big Data problems, and using the Chi-FRBCS-BigData algorithms, support our claims.","Big data, Fuzzy rule based classification systems, Granularity, MapReduce, Hadoop",https://link.springer.com//article/10.1007/s11634-016-0260-z,1163
Robust scale estimators for fuzzy data,"Observations distant from the majority or deviating from the general pattern often appear in datasets. Classical estimates such as the sample mean or the sample variance can be substantially affected by these observations (outliers). Even a single outlier can have huge distorting influence. However, when one deals with real-valued data there exist robust measures/estimates of location and scale (dispersion) which reduce the influence of these atypical values and provide approximately the same results as the classical estimates applied to the typical data without outliers. In real-life, data to be analyzed and interpreted are not always precisely defined and they cannot be properly expressed by using a numerical scale of measurement. Frequently, some of these imprecise data could be suitably described and modelled by considering a fuzzy rating scale of measurement. In this paper, several well-known scale (dispersion) estimators in the real-valued case are extended for random fuzzy numbers (i.e., random mechanisms generating fuzzy-valued data), and some of their properties as estimators for dispersion are examined. Furthermore, their robust behaviour is analyzed using two powerful tools, namely, the finite sample breakdown point and the sensitivity curves. Simulations, including empirical bias curves, are performed to complete the study.","Finite sample breakdown point, Empirical bias curves , Fuzzy numbers, Random fuzzy numbers, Robustness, Scale estimation, Sensitivity curves",https://link.springer.com//article/10.1007/s11634-015-0210-1,1163
A novel method for forecasting time series based on fuzzy logic and visibility graph,"Time series attracts much attention for its remarkable forecasting potential. This paper discusses how fuzzy logic improves accuracy when forecasting time series using visibility graph and presents a novel method to make more accurate predictions. In the proposed method, historical data is firstly converted into a visibility graph. Then, the strategy of link prediction is utilized to preliminarily forecast the future data. Eventually, the future data is revised based on fuzzy logic. To demonstrate the performance, the proposed method is applied to forecast Construction Cost Index, Taiwan Stock Index and student enrollments. The results show that fuzzy logic is able to improve the accuracy by designing appropriate fuzzy rules. In addition, through comparison, it is proved that our method has high flexibility and predictability. It is expected that our work will not only make contributions to the theoretical study of time series forecasting, but also be beneficial to practical areas such as economy and engineering by providing more accurate predictions.","Forecasting, Time series, Fuzzy logic, Visibility graph, Link prediction",https://link.springer.com//article/10.1007/s11634-017-0300-3,1163
On ill-conceived initialization in archetypal analysis,"We show that an improper initialization of the matrix of prototypes, \({\mathbf {V}}\), can be misleading, and potentially gives rise to a degenerate fuzzy partition when performing fuzzy clustering by means of an archetypal analysis. Subsequently, we propose an algorithm to correct the initial guess for \({\mathbf {V}}\), which is grounded in two theoretical results on convex hulls. A numerical experiment carried out to assess its accuracy, and involving more than 200,000 initializations, shows a failure rate of below 0.8%.","Matrix factorization, Fuzzy clustering, Archetypal analysis, Initialization, Polytopes",https://link.springer.com//article/10.1007/s11634-017-0303-0,1163
Multivariate and functional classification using depth and distance,"We construct classifiers for multivariate and functional data. Our approach is based on a kind of distance between data points and classes. The distance measure needs to be robust to outliers and invariant to linear transformations of the data. For this purpose we can use the bagdistance which is based on halfspace depth. It satisfies most of the properties of a norm but is able to reflect asymmetry when the class is skewed. Alternatively we can compute a measure of outlyingness based on the skew-adjusted projection depth. In either case we propose the DistSpace transform which maps each data point to the vector of its distances to all classes, followed by k-nearest neighbor (kNN) classification of the transformed data points. This combines invariance and robustness with the simplicity and wide applicability of kNN. The proposal is compared with other methods in experiments with real and simulated data.","Bagdistance, Projection depth, Skew-adjusted projection depth, Nearest neighbors",https://link.springer.com//article/10.1007/s11634-016-0269-3,1163
Benchmarking different clustering algorithms on functional data,"Theoretical knowledge of clustering functions is still scarce and only few models are available in form of applicable code. In literature, most methods are based on the projection of the functions onto a basis and building fixed or random effects models of the basis coefficients. They involve various parameters, among them number of basis functions, projection dimension, number of iterations etc. They usually work well on the data presented in the articles, but their performance has in most cases not been tested objectively on other data sets, nor against each other. The purpose of this paper is to give an overview of several existing methods to cluster functional data. An outline of their theoretic concepts is given and the meaning of their hyperparameters is explained. A simulation study was set up to analyze the parameters’ efficiency and sensitivity on different types of data sets, that were registered on regular and on irregular grids. For each method, a linear model of the clustering results was evaluated with different parameter levels as predictors. Later, the methods’ performances were compared to each other with the help of a visualization tool, to identify which method works the best on a specific kind of data.","Functional clustering, Benchmarking, Hyperparameters",https://link.springer.com//article/10.1007/s11634-016-0261-y,1163
Constrained clustering with a complex cluster structure,"In this contribution we present a novel constrained clustering method, Constrained clustering with a complex cluster structure (C4s), which incorporates equivalence constraints, both positive and negative, as the background information. C4s is capable of discovering groups of arbitrary structure, e.g. with multi-modal distribution, since at the initial stage the equivalence classes of elements generated by the positive constraints are split into smaller parts. This provides a detailed description of elements, which are in positive equivalence relation. In order to enable an automatic detection of the number of groups, the cross-entropy clustering is applied for each partitioning process. Experiments show that the proposed method achieves significantly better results than previous constrained clustering approaches. The advantage of our algorithm increases when we are focusing on finding partitions with complex structure of clusters.","Constrained clustering, Model-based clustering, Mixture of models, Pairwise equivalence constraints, Semi-supervised learning, Cross-entropy clustering",https://link.springer.com//article/10.1007/s11634-016-0254-x,1163
A fuzzy neural network based framework to discover user access patterns from web log data,"Clustering data from web user sessions is extensively applied to extract customer usage behavior to serve customized content to individual users. Due to the human involvement, web usage data usually contain noisy, incomplete and vague information. Neural networks have the capability to extract embedded knowledge in the form of user session clusters from the huge web usage data. Moreover, they provide tolerance against imperfect and noisy data. Fuzzy sets are another popular tool utilized for handling uncertainty and vagueness hidden in the data. In this paper a fuzzy neural clustering network (FNCN) based framework is proposed that makes use of the fuzzy membership concept of fuzzy c-means (FCM) clustering and the learning rate of a modified self-organizing map (MSOM) neural network model and tries to minimize the weighted sum of the squared error. FNCN is applied to cluster the users’ web access data extracted from the web logs of an educational institution’s proxy web server. The performance of FNCN is compared with FCM and MSOM based clustering methods using various validity indexes. Our results show that FNCN produces better quality of clusters than FCM and MSOM.","Fuzzy neural clustering, Self organizing map, Web usage analysis, Fuzzy cluster validity",https://link.springer.com//article/10.1007/s11634-015-0228-4,1163
Dense traffic flow patterns mining in bi-directional road networks using density based trajectory clustering,"Due to the rapid growth of wireless communications and positioning technologies, trajectory data have become increasingly popular, posing great challenges to the researchers of data mining and machine learning community. Trajectory data are obtained using GPS devices that capture the position of an object at specific time intervals. These enormous amounts of data necessitates to explore efficient and effective techniques to extract useful information to solve real world problems. Traffic flow pattern mining is one of the challenging issues for many applications. In a literature significant number of approaches are available to cluster the trajectory data, however the clustering has not been explored for trajectories pattern mining in bi-directional road networks. This paper presents a novel technique for excavating heavy traffic flow patterns in bi-directional road network, i.e. identifying divisions of the roads where the traffic flow is very dense. The proposed technique works in two phases: phase I, finds the clusters of trajectory points based on density of trajectory points; phase II, arranges the clusters in sequence based on spatiotemporal values for each route and directions. These sequences represent the traffic flow patterns. All the routes and sections exceeding a user specified minimum traffic threshold are marked as high dense traffic areas. The experiments are performed on synthetic dataset. The proposed algorithm efficiently and accurately finds the dense traffic in bi-directional roads. Proposed clustering method is compared with the standard k-means clustering algorithm for the performance evaluation.","Data mining, Traffic flow patterns mining, Trajectory patterns, Traffic control, Mobility mining",https://link.springer.com//article/10.1007/s11634-016-0256-8,1163
Disjoint factor analysis with cross-loadings,"Disjoint factor analysis (DFA) is a new latent factor model that we propose here to identify factors that relate to disjoint subsets of variables, thus simplifying the loading matrix structure. Similarly to exploratory factor analysis (EFA), the DFA does not hypothesize prior information on the number of factors and on the relevant relations between variables and factors. In DFA the population variance–covariance structure is hypothesized block diagonal after the proper permutation of variables and estimated by Maximum Likelihood, using an Coordinate Descent type algorithm. Inference on parameters on the number of factors and to confirm the hypothesized simple structure are provided. Properties such as scale equivariance, uniqueness, optimal simplification of loadings are satisfied by DFA. Relevant cross-loadings are also estimated in case they are detected from the best DFA solution. DFA has also the option to constrain a variable to load on a pre-specified factor so that the researcher can assume, a priori, some relations between variables and loadings. A simulation study shows performances of DFA and an application to optimally identify the dimensions of well-being is used to illustrate characteristics of the new methodology. A final discussion concludes the paper.","Exploratory factor analysis, Disjoint factor analysis, Cross-loadings, Sparse loading matrix",https://link.springer.com//article/10.1007/s11634-016-0263-9,1163
General location model with factor analyzer covariance matrix structure and its applications,"General location model (GLOM) is a well-known model for analyzing mixed data. In GLOM one decomposes the joint distribution of variables into conditional distribution of continuous variables given categorical outcomes and marginal distribution of categorical variables. The first version of GLOM assumes that the covariance matrices of continuous multivariate distributions across cells, which are obtained by different combination of categorical variables, are equal. In this paper, the GLOMs are considered in both cases of equality and unequality of these covariance matrices. Three covariance structures are used across cells: the same factor analyzer, factor analyzer with unequal specific variances matrices (in the general and parsimonious forms) and factor analyzers with common factor loadings. These structures are used for both modeling covariance structure and for reducing the number of parameters. The maximum likelihood estimates of parameters are computed via the EM algorithm. As an application for these models, we investigate the classification of continuous variables within cells. Based on these models, the classification is done for usual as well as for high dimensional data sets. Finally, for showing the applicability of the proposed models for classification, results from analyzing three real data sets are presented.","General location model, Factor analyzer, Common factor loadings, Classification, The EM algorithm",https://link.springer.com//article/10.1007/s11634-016-0258-6,1163
Multi-objective retinal vessel localization using flower pollination search algorithm with pattern search,"This paper presents a multi-objective retinal blood vessels localization approach based on flower pollination search algorithm (FPSA) and pattern search (PS) algorithm. FPSA is a new evolutionary algorithm based on the flower pollination process of flowering plants. The proposed multi-objective fitness function uses the flower pollination search algorithm (FPSA) that searches for the optimal clustering of the given retinal image into compact clusters under some constraints. Pattern search (PS) as local search method is then applied to further enhance the segmentation results using another objective function based on shape features. The proposed approach for retinal blood vessels localization is applied on public database namely DRIVE data set. Results demonstrate that the performance of the proposed approach is comparable with state of the art techniques in terms of accuracy, sensitivity, and specificity with many extendable features.","Flower pollination search algorithm, Pattern search, Multi-objective retinal vessel localization, Bio-inspired optimization, Evolutionary computation",https://link.springer.com//article/10.1007/s11634-016-0257-7,1163
A new approach for determining the prior probabilities in the classification problem by Bayesian method,"In this article, we suggest a new algorithm to identify the prior probabilities for classification problem by Bayesian method. The prior probabilities are determined by combining the information of populations in training set and the new observations through fuzzy clustering method (FCM) instead of using uniform distribution or the ratio of sample or Laplace method as the existing ones. We next combine the determined prior probabilities and the estimated likelihood functions to classify the new object. In practice, calculations are performed by Matlab procedures. The proposed algorithm is tested by the three numerical examples including bench mark and real data sets. The results show that the new approach is reasonable and gives more efficient than existing ones.","Classification, Bayes error, BayesC, Prior probability",https://link.springer.com//article/10.1007/s11634-016-0253-y,1163
Exploratory data analysis for interval compositional data,"Compositional data are considered as data where relative contributions of parts on a whole, conveyed by (log-)ratios between them, are essential for the analysis. In Symbolic Data Analysis (SDA), we are in the framework of interval data when elements are characterized by variables whose values are intervals on \(\mathbb {R}\) representing inherent variability. In this paper, we address the special problem of the analysis of interval compositions, i.e., when the interval data are obtained by the aggregation of compositions. It is assumed that the interval information is represented by the respective midpoints and ranges, and both sources of information are considered as compositions. In this context, we introduce the representation of interval data as three-way data. In the framework of the log-ratio approach from compositional data analysis, it is outlined how interval compositions can be treated in an exploratory context. The goal of the analysis is to represent the compositions by coordinates which are interpretable in terms of the original compositional parts. This is achieved by summarizing all relative information (logratios) about each part into one coordinate from the coordinate system. Based on an example from the European Union Statistics on Income and Living Conditions (EU-SILC), several possibilities for an exploratory data analysis approach for interval compositions are outlined and investigated.","Interval data, Symbolic data analysis, Aitchison geometry on the simplex, Orthonormal coordinates, Outlier detection, Principal component analysis",https://link.springer.com//article/10.1007/s11634-016-0245-y,1163
Model-based regression clustering for high-dimensional data: application to functional data,"Finite mixture regression models are useful for modeling the relationship between response and predictors arising from different subpopulations. In this article, we study high-dimensional predictors and high-dimensional response and propose two procedures to cluster observations according to the link between predictors and the response. To reduce the dimension, we propose to use the Lasso estimator, which takes into account the sparsity and a maximum likelihood estimator penalized by the rank, to take into account the matrix structure. To choose the number of components and the sparsity level, we construct a collection of models, varying those two parameters and we select a model among this collection with a non-asymptotic criterion. We extend these procedures to functional data, where predictors and responses are functions. For this purpose, we use a wavelet-based approach. For each situation, we provide algorithms and apply and evaluate our methods both on simulated and real datasets, to understand how they work in practice.","Model-based clustering, Regression, High-dimension , Functional data",https://link.springer.com//article/10.1007/s11634-016-0242-1,1163
Mixture models for ordinal responses to account for uncertainty of choice,In CUB models the uncertainty of choice is explicitly modelled as a Combination of discrete Uniform and shifted Binomial random variables. The basic concept to model the response as a mixture of a deliberate choice of a response category and an uncertainty component that is represented by a uniform distribution on the response categories is extended to a much wider class of models. The deliberate choice can in particular be determined by classical ordinal response models as the cumulative and adjacent categories model. Then one obtains the traditional and flexible models as special cases when the uncertainty component is irrelevant. It is shown that the effect of explanatory variables is underestimated if the uncertainty component is neglected in a cumulative type mixture model. Visualization tools for the effects of variables are proposed and the modelling strategies are evaluated by use of real data sets. It is demonstrated that the extended class of models frequently yields better fit than classical ordinal response models without an uncertainty component.,"Ordinal responses, Rating analysis, CUP model, CUB model",https://link.springer.com//article/10.1007/s11634-016-0247-9,1163
Logistic biplot for nominal data,"Classical biplot methods allow for the simultaneous representation of individuals (rows) and variables (columns) of a data matrix. For binary data, logistic biplots have been recently developed. When data are nominal, both classical and binary logistic biplots are not adequate and techniques such as multiple correspondence analysis (MCA), latent trait analysis (LTA) or item response theory (IRT) for nominal items should be used instead. In this paper we extend the binary logistic biplot to nominal data. The resulting method is termed “nominal logistic biplot”(NLB), although the variables are represented as convex prediction regions rather than vectors. Using the methods from computational geometry, the set of prediction regions is converted to a set of points in such a way that the prediction for each individual is established by its closest “category point”. Then interpretation is based on distances rather than on projections. We study the geometry of such a representation and construct computational algorithms for the estimation of parameters and the calculation of prediction regions. Nominal logistic biplots extend both MCA and LTA in the sense that they give a graphical representation for LTA similar to the one obtained in MCA.","Biplot, Categorical variables, Logistic responses , Latent traits, Computational geometry, Inverse Voronoi problem",https://link.springer.com//article/10.1007/s11634-016-0249-7,1163
Principal component analysis for histogram-valued data,"This paper introduces a principal component methodology for analysing histogram-valued data under the symbolic data domain. Currently, no comparable method exists for this type of data. The proposed method uses a symbolic covariance matrix to determine the principal component space. The resulting observations on principal component space are presented as polytopes for visualization. Numerical representation of the resulting polytopes via histogram-valued output is also presented. The necessary algorithms are included. The technique is illustrated on a weather data set.","Principal components, Histogram observations, Polytopes",https://link.springer.com//article/10.1007/s11634-016-0255-9,1163
T3C: improving a decision tree classification algorithm’s interval splits on continuous attributes,"This paper proposes, describes and evaluates T3C, a classification algorithm that builds decision trees of depth at most three, and results in high accuracy whilst keeping the size of the tree reasonably small. T3C is an improvement over algorithm T3 in the way it performs splits on continuous attributes. When run against publicly available data sets, T3C achieved lower generalisation error than T3 and the popular C4.5, and competitive results compared to Random Forest and Rotation Forest.","Data mining, Classification, Decision trees, Interval splits",https://link.springer.com//article/10.1007/s11634-016-0246-x,1163
"ADCLUS and INDCLUS: analysis, experimentation, and meta-heuristic algorithm extensions","The ADCLUS and INDCLUS models, along with associated fitting techniques, can be used to extract an overlapping clustering structure from similarity data. In this paper, we examine the scalability of these models. We test the SINDLCUS algorithm and an adapted version of the SYMPRES algorithm on medium size datasets and try to infer their scalability and the degree of the local optima problem as the problem size increases. We describe several meta-heuristic approaches to minimizing the INDCLUS and ADCLUS loss functions.","Overlapping clustering, Optimization, NP-Hard",https://link.springer.com//article/10.1007/s11634-016-0244-z,1163
A sequential distance-based approach for imputing missing data: Forward Imputation,"Missing data recurrently affect datasets in almost every field of quantitative research. The subject is vast and complex and has originated a literature rich in very different approaches to the problem. Within an exploratory framework, distance-based methods such as nearest-neighbour imputation (NNI), or procedures involving multivariate data analysis (MVDA) techniques seem to treat the problem properly. In NNI, the metric and the number of donors can be chosen at will. MVDA-based procedures expressly account for variable associations. The new approach proposed here, called Forward Imputation, ideally meets these features. It is designed as a sequential procedure that imputes missing data in a step-by-step process involving subsets of units according to their “completeness rate”. Two methods within this context are developed for the imputation of quantitative data. One applies NNI with the Mahalanobis distance, the other combines NNI and principal component analysis. Statistical properties of the two methods are discussed, and their performance is assessed, also in comparison with alternative imputation methods. To this purpose, a simulation study in the presence of different data patterns along with an application to real data are carried out, and practical hints for users are also provided.","Iterative PCA, Mahalanobis distance, MCAR missing data, MissForest, Nearest-neighbour imputation, Skew data",https://link.springer.com//article/10.1007/s11634-016-0243-0,1163
Backtransformation: a new representation of data processing chains with a scalar decision function,"Data processing often transforms a complex signal using a set of different preprocessing algorithms to a single value as the outcome of a final decision function. Still, it is challenging to understand and visualize the interplay between the algorithms performing this transformation. Especially when dimensionality reduction is used, the original data structure (e.g., spatio-temporal information) is hidden from subsequent algorithms. To tackle this problem, we introduce the backtransformation concept suggesting to look at the combination of algorithms as one transformation which maps the original input signal to a single value. Therefore, it takes the derivative of the final decision function and transforms it back through the previous processing steps via backward iteration and the chain rule. The resulting derivative of the composed decision function in the sample of interest represents the complete decision process. Using it for visualizations might improve the understanding of the process. Often, it is possible to construct a feasible processing chain with affine mappings which simplifies the calculation for the backtransformation and the interpretation of the result a lot. In this case, the affine backtransformation provides the complete parameterization of the processing chain. This article introduces the theory, provides implementation guidelines, and presents three application examples.","Affine transformations, Function composition, Processing chain interpretation, Processing chain visualization",https://link.springer.com//article/10.1007/s11634-015-0229-3,1163
On visual distances for spectrum-type functional data,"A functional distance \({\mathbb H}\), based on the Hausdorff metric between the function hypographs, is proposed for the space \({\mathcal E}\) of non-negative real upper semicontinuous functions on a compact interval. The main goal of the paper is to show that the space \(({\mathcal E},{\mathbb H})\) is particularly suitable in some statistical problems with functional data which involve functions with very wiggly graphs and narrow, sharp peaks. A typical example is given by spectrograms, either obtained by magnetic resonance or by mass spectrometry. On the theoretical side, we show that \(({\mathcal E},{\mathbb H})\) is a complete, separable locally compact space and that the \({\mathbb H}\)-convergence of a sequence of functions implies the convergence of the respective maximum values of these functions. The probabilistic and statistical implications of these results are discussed, in particular regarding the consistency of k-NN classifiers for supervised classification problems with functional data in \({\mathbb H}\). On the practical side, we provide the results of a small simulation study and check also the performance of our method in two real data problems of supervised classification involving mass spectra.","Supervised classification, Functional data analysis , Hausdorff metric",https://link.springer.com//article/10.1007/s11634-015-0217-7,1163
NMF versus ICA for blind source separation,"Blind source separation (BSS) is a problem of recovering source signals from signal mixtures without or very limited information about the sources and the mixing process. From literatures, nonnegative matrix factorization (NMF) and independent component analysis (ICA) seem to be the mainstream techniques for solving the BSS problems. Even though the using of NMF and ICA for BSS is well studied, there is still a lack of works that compare the performances of these techniques. Moreover, the nonuniqueness property of NMF is rarely mentioned even though this property actually can make the reconstructed signals vary significantly, and thus introduces the difficulty on how to choose the representative reconstructions from several possible outcomes. In this paper, we compare the performances of NMF and ICA as BSS methods using some standard NMF and ICA algorithms, and point out the difficulty in choosing the representative reconstructions originated from the nonuniqueness property of NMF.","Blind source separation, Independent component analysis,  Nonnegative matrix factorization",https://link.springer.com//article/10.1007/s11634-014-0192-4,1163
Dichotomic lattices and local discretization for Galois lattices,"The present paper deals with supervised classification methods based on Galois lattices and decision trees. Such ordered structures require attributes discretization and it is known that, for decision trees, local discretization improves the classification performance compared with global discretization. While most literature on discretization for Galois lattices relies on global discretization, the presented work introduces a new local discretization algorithm for Galois lattices which hinges on a property of some specific lattices that we introduce as dichotomic lattices. Their properties, co-atomicity and \(\vee \)-complementarity are proved along with their links with decision trees. Finally, some quantitative and qualitative evaluations of the local discretization are proposed.","Discretization, Galois lattices, Decision trees , Classification",https://link.springer.com//article/10.1007/s11634-015-0225-7,1163
Minimum Class Variance SVM+ for data classification,"In this paper, a new Support Vector Machine Plus (SVM+) type model called Minimum Class Variance SVM+ (MCVSVM+) is presented. Similar to SVM+, the proposed model utilizes the group information in the training data. We show that MCVSVM+ has both the advantages of SVM+ and Minimum Class Variance Support Vector Machine (MCVSVM). That is, MCVSVM+ not only considers class distribution characteristics in its optimization problem but also utilizes the additional information (i.e. group information) hidden in the data, in contrast to SVM+ that takes into consideration only the samples that are in the class boundaries. The experimental results demonstrate the validity and advantage of the new model compared with the standard SVM, SVM+ and MCVSVM.","Support vector machine, SVM+, Group information, Class variance",https://link.springer.com//article/10.1007/s11634-015-0212-z,1163
A uniform framework for the combination of penalties in generalized structured models,"Penalized estimation has become an established tool for regularization and model selection in regression models. A variety of penalties with specific features are available and effective algorithms for specific penalties have been proposed. But not much is available to fit models with a combination of different penalties. When modeling the rent data of Munich as in our application, various types of predictors call for a combination of a Ridge, a group Lasso and a Lasso-type penalty within one model. We propose to approximate penalties that are (semi-)norms of scalar linear transformations of the coefficient vector in generalized structured models—such that penalties of various kinds can be combined in one model. The approach is very general such that the Lasso, the fused Lasso, the Ridge, the smoothly clipped absolute deviation penalty, the elastic net and many more penalties are embedded. The computation is based on conventional penalized iteratively re-weighted least squares algorithms and hence, easy to implement. New penalties can be incorporated quickly. The approach is extended to penalties with vector based arguments. There are several possibilities to choose the penalty parameter(s). A software implementation is available. Some illustrative examples show promising results.","Model selection, Penalties, Generalized linear model (GLM), Structured regression, Ridge, Lasso, Group Lasso, SCAD, Elastic net,  Fused Lasso",https://link.springer.com//article/10.1007/s11634-015-0205-y,1163
Advances in credit scoring: combining performance and interpretation in kernel discriminant analysis,"Due to the recent financial turmoil, a discussion in the banking sector about how to accomplish long term success, and how to follow an exhaustive and powerful strategy in credit scoring is being raised up. Recently, the significant theoretical advances in machine learning algorithms have pushed the application of kernel-based classifiers, producing very effective results. Unfortunately, such tools have an inability to provide an explanation, or comprehensible justification, for the solutions they supply. In this paper, we propose a new strategy to model credit scoring data, which exploits, indirectly, the classification power of the kernel machines into an operative field. A reconstruction process of the kernel classifier is performed via linear regression, if all predictors are numerical, or via a general linear model, if some or all predictors are categorical. The loss of performance, due to such approximation, is balanced by better interpretability for the end user, which is able to order, understand and to rank the influence of each category of the variables set in the prediction. An Italian bank case study has been illustrated and discussed; empirical results reveal a promising performance of the introduced strategy.","Credit scoring, Kernel discriminant analysis, DISQUAL,  Small and medium enterprises",https://link.springer.com//article/10.1007/s11634-015-0213-y,1163
A generalized maximum entropy estimator to simple linear measurement error model with a composite indicator,"We extend the simple linear measurement error model through the inclusion of a composite indicator by using the generalized maximum entropy estimator. A Monte Carlo simulation study is proposed for comparing the performances of the proposed estimator to his counterpart the ordinary least squares “Adjusted for attenuation”. The two estimators are compared in term of correlation with the true latent variable, standard error and root mean of squared error. Two illustrative case studies are reported in order to discuss the results obtained on the real data set, and relate them to the conclusions drawn via simulation study.","Simple linear measurement error model, Generalized maximum entropy, Composite indicator, Global innovation index, Manager performance",https://link.springer.com//article/10.1007/s11634-016-0237-y,1163
Evaluation of the evolution of relationships between topics over time,"Topics that attract public attention can originate from current events or developments, might be influenced by situations in the past, and often continue to be of interest in the future. When respective information is made available textually, one possibility of detecting such topics of public importance consists in scrutinizing, e.g., appropriate press articles using—given the continual growth of information—text processing techniques enriched by computer routines which examine present-day textual material, check historical publications, find newly emerging topics, and are able to track topic trends over time. Information clustering based on content-(dis)similarity of the underlying textual material and graph-theoretical considerations to deal with the network of relationships between content-similar topics are described and combined in a new approach. Explanatory examples of topic detection and tracking in online news articles illustrate the usefulness of the approach in different situations.","Topic relationships, Topic trend detection, Text processing, Content-(dis)similarity, Information clustering",https://link.springer.com//article/10.1007/s11634-016-0241-2,1163
Supervised box clustering,"In this work we address a technique for effectively clustering points in specific convex sets, called homogeneous boxes, having sides aligned with the coordinate axes (isothetic condition). The proposed clustering approach is based on homogeneity conditions, not according to some distance measure, and, even if it was originally developed in the context of the logical analysis of data, it is now placed inside the framework of Supervised clustering. First, we introduce the basic concepts in box geometry; then, we consider a generalized clustering algorithm based on a class of graphs, called incompatibility graphs. For supervised classification problems, we consider classifiers based on box sets, and compare the overall performances to the accuracy levels of competing methods for a wide range of real data sets. The results show that the proposed method performs comparably with other supervised learning methods in terms of accuracy.","Supervised clustering, Classification problems, Incompatibility graphs, Homogeneous boxes",https://link.springer.com//article/10.1007/s11634-016-0233-2,1163
Multiple straight-line fitting using a Bayes factor,"This paper introduces a Bayesian approach to solve the problem of fitting multiple straight lines to a set of 2D points. Other approaches use many arbitrary parameters and threshold values, the proposed criterion uses only the parameters of the measurement errors. Models with multiple lines are useful in many applications, this paper analyzes the performance of the new approach to solve a classical problem in robotics: finding a map of lines from laser measurements. Tests show that the Bayesian approach obtains reliable models.","Multiple lines fitting, Line maps, Line features, Robot sensing",https://link.springer.com//article/10.1007/s11634-016-0236-z,1163
A mixture of generalized hyperbolic factor analyzers,"The mixture of factor analyzers model, which has been used successfully for the model-based clustering of high-dimensional data, is extended to generalized hyperbolic mixtures. The development of a mixture of generalized hyperbolic factor analyzers is outlined, drawing upon the relationship with the generalized inverse Gaussian distribution. An alternating expectation-conditional maximization algorithm is used for parameter estimation, and the Bayesian information criterion is used to select the number of factors as well as the number of components. The performance of our generalized hyperbolic factor analyzers model is illustrated on real and simulated data, where it performs favourably compared to its Gaussian analogue and other approaches.","Clustering, Generalized hyperbolic distribution, Mixture of factor analyzers, AECM algorithm",https://link.springer.com//article/10.1007/s11634-015-0204-z,1163
Factor probabilistic distance clustering (FPDC): a new clustering method,"Factor clustering methods have been developed in recent years thanks to improvements in computational power. These methods perform a linear transformation of data and a clustering of the transformed data, optimizing a common criterion. Probabilistic distance (PD)-clustering is an iterative, distribution free, probabilistic clustering method. Factor PD-clustering (FPDC) is based on PD-clustering and involves a linear transformation of the original variables into a reduced number of orthogonal ones using a common criterion with PD-clustering. This paper demonstrates that Tucker3 decomposition can be used to accomplish this transformation. Factor PD-clustering alternatingly exploits Tucker3 decomposition and PD-clustering on transformed data until convergence is achieved. This method can significantly improve the PD-clustering algorithm performance; large data sets can thus be partitioned into clusters with increasing stability and robustness of the results. Real and simulated data sets are used to compare FPDC with its main competitors, where it performs equally well when clusters are elliptically shaped but outperforms its competitors with non-Gaussian shaped clusters or noisy data.","Factor clustering, Probabilistic distance clustering, Tucker3, k-means",https://link.springer.com//article/10.1007/s11634-015-0219-5,1163
Human capital estimation in higher education,"The concept of human capital (HC) could be defined, from an economical viewpoint, as a stock variable representing the capacity of an individual to produce a sustained flow of income due to its investment in (higher) education and work experience. This paper focuses on the empirical estimation of the graduates’ latent variable HC, composed of two principal dimensions, Educational HC and Work Experience HC, within a realistic structural model, allowing causal relationship among endogenous and exogenous indicators, taking into account possible effects of external covariates. New administrative archives and a novel methodological approach are used. The methodology is applied to estimate HC of graduates in several universities of the Milan area in the early stages of their working career. The empirical results confirm the structure of the Italian job market, where investment in HC through higher education plays only a marginal role for explaining the economic performance.","Human capital, Redundancy analysis, Concomitant indicators",https://link.springer.com//article/10.1007/s11634-016-0259-5,1163
Quantile composite-based path modeling,The paper aims at introducing a quantile approach in the Partial Least Squares path modeling framework. This is a well known composite-based method for the analysis of complex phenomena measurable through a network of relationships among observed and unobserved variables. The proposal intends to enhance potentialities of the Partial Least Squares path models overcoming the classical exploration of average effects. The introduction of Quantile Regression and Correlation in the estimation phases of the model allows highlighting how and if the relationships among observed and unobserved variables change according to the explored quantile of interest. The proposed method is applied to two real datasets in the customer satisfaction measurement and in the sensory analysis framework but it proves to be useful also in other applicative contexts.,"Quantile regression, PLS path modeling, Multi-block data",https://link.springer.com//article/10.1007/s11634-015-0231-9,1163
Exponential family mixed membership models for soft clustering of multivariate data,"For several years, model-based clustering methods have successfully tackled many of the challenges presented by data-analysts. However, as the scope of data analysis has evolved, some problems may be beyond the standard mixture model framework. One such problem is when observations in a dataset come from overlapping clusters, whereby different clusters will possess similar parameters for multiple variables. In this setting, mixed membership models, a soft clustering approach whereby observations are not restricted to single cluster membership, have proved to be an effective tool. In this paper, a method for fitting mixed membership models to data generated by a member of an exponential family is outlined. The method is applied to count data obtained from an ultra running competition, and compared with a standard mixture model approach.","Mixed membership models, Model based clustering, Mixture models, Variational Bayes",https://link.springer.com//article/10.1007/s11634-016-0267-5,1163
The determination of uncertainty levels in robust clustering of subjects with longitudinal observations using the Dirichlet process mixture,In this paper we introduce a new method to the cluster analysis of longitudinal data focusing on the determination of uncertainty levels for cluster memberships. The method uses the Dirichlet-t distribution which notably utilizes the robustness feature of the student-t distribution in the framework of a Bayesian semi-parametric approach together with robust clustering of subjects evaluates the uncertainty level of subjects memberships to their clusters. We let the number of clusters and the uncertainty levels be unknown while fitting Dirichlet process mixture models. Two simulation studies are conducted to demonstrate the proposed methodology. The method is applied to cluster a real data set taken from gene expression studies.,"Dirichlet process mixture, Dirichlet-t distribution, Mixed-effects models, Model-based clustering",https://link.springer.com//article/10.1007/s11634-016-0262-x,1163
An effective strategy for initializing the EM algorithm in finite mixture models,"Finite mixture models represent one of the most popular tools for modeling heterogeneous data. The traditional approach for parameter estimation is based on maximizing the likelihood function. Direct optimization is often troublesome due to the complex likelihood structure. The expectation–maximization algorithm proves to be an effective remedy that alleviates this issue. The solution obtained by this procedure is entirely driven by the choice of starting parameter values. This highlights the importance of an effective initialization strategy. Despite efforts undertaken in this area, there is no uniform winner found and practitioners tend to ignore the issue, often finding misleading or erroneous results. In this paper, we propose a simple yet effective tool for initializing the expectation–maximization algorithm in the mixture modeling setting. The idea is based on model averaging and proves to be efficient in detecting correct solutions even in those cases when competitors perform poorly. The utility of the proposed methodology is shown through comprehensive simulation study and applied to a well-known classification dataset with good results.","Finite mixture models, EM algorithm, Initialization, Model averaging, BIC",https://link.springer.com//article/10.1007/s11634-016-0264-8,1163
Pruning boxes in a box-based classification method,"In this work we address an extension of box clustering in supervised classification problems that makes use of optimization problems to refine the results obtained by agglomerative techniques. The central concept of box clustering is that of homogeneous boxes that give rise to overtrained classifiers under some conditions. Thus, we focus our attentions on the issue of pruning out redundant boxes, using the information gleaned from the other boxes generated under the hypothesis that such a choice would identify simpler models with good predictive power. We propose a pruning method based on an integer optimization problem and a family of sub problems derived from the main one. The overall performances are then compared to the accuracy levels of competing methods on a wide range of real data sets. The method has proven to be robust, making it possible to derive a more compact system of boxes in the instance space with good performance on training and test data.","Supervised classification, Homogeneous boxes, Overtraining, Pruning",https://link.springer.com//article/10.1007/s11634-014-0193-3,1163
Marginal and simultaneous predictive classification using stratified graphical models,"An inductive probabilistic classification rule must generally obey the principles of Bayesian predictive inference, such that all observed and unobserved stochastic quantities are jointly modeled and the parameter uncertainty is fully acknowledged through the posterior predictive distribution. Several such rules have been recently considered and their asymptotic behavior has been characterized under the assumption that the observed features or variables used for building a classifier are conditionally independent given a simultaneous labeling of both the training samples and those from an unknown origin. Here we extend the theoretical results to predictive classifiers acknowledging feature dependencies either through graphical models or sparser alternatives defined as stratified graphical models. We show through experimentation with both synthetic and real data that the predictive classifiers encoding dependencies have the potential to substantially improve classification accuracy compared with both standard discriminative classifiers and the predictive classifiers based on solely conditionally independent features. In most of our experiments stratified graphical models show an advantage over ordinary graphical models.","Classification, Context-specific independence, Graphical model, Predictive inference",https://link.springer.com//article/10.1007/s11634-015-0199-5,1163
Semi-supervised model-based clustering with positive and negative constraints,"Cluster analysis is a popular technique in statistics and computer science with the objective of grouping similar observations in relatively distinct groups generally known as clusters. Semi-supervised clustering assumes that some additional information about group memberships is available. Under the most frequently considered scenario, labels are known for some portion of data and unavailable for the rest of observations. In this paper, we discuss a general type of semi-supervised clustering defined by so called positive and negative constraints. Under positive constraints, some data points are required to belong to the same cluster. On the contrary, negative constraints specify that particular points must represent different data groups. We outline a general framework for semi-supervised clustering with constraints naturally incorporating the additional information into the EM algorithm traditionally used in mixture modeling and model-based clustering. The developed methodology is illustrated on synthetic and classification datasets. A dendrochronology application is considered and thoroughly discussed.","Semi-supervised clustering, Model-based clustering ,  Finite mixture models, Positive and negative constraints, BIC",https://link.springer.com//article/10.1007/s11634-015-0200-3,1163
The forward search interactive outlier detection in cointegrated VAR analysis,"Cointegration analysis is particularly sensitive to outlying observations. Traditional robust approaches rely on parameter estimates based on weighting schemes to penalize aberrant units. This, in particular, is the idea underlying pseudo maximum likelihood robust estimators. Atypical observations, however, can reveal useful information about the investigated phenomenon. Aiming to detect these observations, we extend the forward search procedure to the cointegrated vector autoregressive model. The analysis is carried out by building up subsets of increasing dimension and monitoring suitable statistics at each subset size. Simulation experiments and real data analysis highlight that our forward search is more effective than the pseudo maximum likelihood in detecting atypical units and data structures.","Confidence threshold, Data structure detection, Forward search, Outliers, Pseudo maximum likelihood weights, Robust statistics, Vector equilibrium correction model",https://link.springer.com//article/10.1007/s11634-015-0216-8,1163
Quantile regression with group lasso for classification,"Applications of regression models for binary response are very common and models specific to these problems are widely used. Quantile regression for binary response data has recently attracted attention and regularized quantile regression methods have been proposed for high dimensional problems. When the predictors have a natural group structure, such as in the case of categorical predictors converted into dummy variables, then a group lasso penalty is used in regularized methods. In this paper, we present a Bayesian Gibbs sampling procedure to estimate the parameters of a quantile regression model under a group lasso penalty for classification problems with a binary response. Simulated and real data show a good performance of the proposed method in comparison to mean-based approaches and to quantile-based approaches which do not exploit the group structure of the predictors.","Quantile regression, Binary regression, Regularized regression,  Gibbs sampling",https://link.springer.com//article/10.1007/s11634-015-0206-x,1163
Clustering of time series using quantile autocovariances,"Time series clustering is an active research topic with applications in many fields. Unlike conventional clustering on multivariate data, time series often change over time so that the similarity concept between objects must take into account the dynamic of the series. In this paper, a distance measure aimed to compare quantile autocovariance functions is proposed to perform clustering of time series. Quantile autocovariances provide information about the serial dependence structure at different pairs of quantile levels, require no moment condition and allow to identify dependence features that covariance-based methods are unable to detect. Results from an extensive simulation study show that the proposed metric outperforms or is highly competitive with a range of dissimilarities reported in the literature, particularly exhibiting high capability to cluster time series generated from a broad range of dependence models. Estimation of the optimal number of clusters is also addressed. For illustrative purposes, our methodology is applied to a real dataset involving financial time series.","Dissimilarity between time series, Clustering,  Quantile autocovariances, Optimal number of clusters",https://link.springer.com//article/10.1007/s11634-015-0208-8,1163
Micro–macro multilevel latent class models with multiple discrete individual-level variables,"An existing micro–macro method for a single individual-level variable is extended to the multivariate situation by presenting two multilevel latent class models in which multiple discrete individual-level variables are used to explain a group-level outcome. As in the univariate case, the individual-level data are summarized at the group-level by constructing a discrete latent variable at the group level and this group-level latent variable is used as a predictor for the group-level outcome. In the first extension, that is referred to as the Direct model, the multiple individual-level variables are directly used as indicators for the group-level latent variable. In the second extension, referred to as the Indirect model, the multiple individual-level variables are used to construct an individual-level latent variable that is used as an indicator for the group-level latent variable. This implies that the individual-level variables are used indirectly at the group-level. The within- and between components of the (co)varn the individual-level variables are independent in the Direct model, but dependent in the Indirect model. Both models are discussed and illustrated with an empirical data example.","Latent class analysis, Micro-macro analysis, Multilevel analysis, Discrete data",https://link.springer.com//article/10.1007/s11634-016-0234-1,1163
Model based clustering for mixed data: clustMD,"A model based clustering procedure for data of mixed type, clustMD, is developed using a latent variable model. It is proposed that a latent variable, following a mixture of Gaussian distributions, generates the observed data of mixed type. The observed data may be any combination of continuous, binary, ordinal or nominal variables. clustMD employs a parsimonious covariance structure for the latent variables, leading to a suite of six clustering models that vary in complexity and provide an elegant and unified approach to clustering mixed data. An expectation maximisation (EM) algorithm is used to estimate clustMD; in the presence of nominal data a Monte Carlo EM algorithm is required. The clustMD model is illustrated by clustering simulated mixed type data and prostate cancer patients, on whom mixed data have been recorded.","Latent variables, Mixture model, Mixed data, Monte Carlo EM",https://link.springer.com//article/10.1007/s11634-016-0238-x,1163
Beyond the number of classes: separating substantive from non-substantive dependence in latent class analysis,"Latent class analysis (LCA) for categorical data is a model-based clustering and classification technique applied in a wide range of fields including the social sciences, machine learning, psychiatry, public health, and epidemiology. Its central assumption is conditional independence of the indicators given the latent class, i.e. “local independence”; violations can appear as model misfit, often leading LCA practitioners to increase the number of classes. However, when not all of the local dependence is of substantive scientific interest this leads to two options, that are both problematic: modeling uninterpretable classes, or retaining a lower number of substantive classes but incurring bias in the final results and classifications of interest due to remaining assumption violations. This paper suggests an alternative procedure, applicable in cases when the number of substantive classes is known in advance, or when substantive interest is otherwise well-defined. I suggest, in such cases, to model substantive local dependencies as additional discrete latent variables, while absorbing nuisance dependencies in additional parameters. An example application to the estimation of misclassification and turnover rates of the decision to vote in elections of 9510 Dutch residents demonstrates the advantages of this procedure relative to increasing the number of classes.","Latent class analysis, Local dependence, Bivariate residual,  Score test, Information criteria, Vote misclassification",https://link.springer.com//article/10.1007/s11634-015-0211-0,1163
Latent class model with conditional dependency per modes to cluster categorical data,"We propose a parsimonious extension of the classical latent class model to cluster categorical data by relaxing the conditional independence assumption. Under this new mixture model, named conditional modes model (CMM), variables are grouped into conditionally independent blocks. Each block follows a parsimonious multinomial distribution where the few free parameters model the probabilities of the most likely levels, while the remaining probability mass is uniformly spread over the other levels of the block. Thus, when the conditional independence assumption holds, this model defines parsimonious versions of the standard latent class model. Moreover, when this assumption is violated, the proposed model brings out the main intra-class dependencies between variables, summarizing thus each class with relatively few characteristic levels. The model selection is carried out by an hybrid MCMC algorithm that does not require preliminary parameter estimation. Then, the maximum likelihood estimation is performed via an EM algorithm only for the best model. The model properties are illustrated on simulated data and on three real data sets by using the associated R package CoModes. The results show that this model allows to reduce biases involved by the conditional independence assumption while providing meaningful parameters.","Categorical data, Clustering, Integrated complete-data likelihood, MCMC algorithm, Mixture models, Model selection",https://link.springer.com//article/10.1007/s11634-016-0250-1,1163
Power analysis for the bootstrap likelihood ratio test for the number of classes in latent class models,"Latent class (LC) analysis is used to construct empirical evidence on the existence of latent subgroups based on the associations among a set of observed discrete variables. One of the tests used to infer about the number of underlying subgroups is the bootstrap likelihood ratio test (BLRT). Although power analysis is rarely conducted for this test, it is important to identify, clarify, and specify the design issues that influence the statistical inference on the number of latent classes based on the BLRT. This paper proposes a computationally efficient ‘short-cut’ method to evaluate the power of the BLRT, as well as presents a procedure to determine a required sample size to attain a specific power level. Results of our numerical study showed that this short-cut method yields reliable estimates of the power of the BLRT. The numerical study also showed that the sample size required to achieve a specified power level depends on various factors of which the class separation plays a dominant role. In some situations, a sample size of 200 may be enough, while in others 2000 or more subjects are required to achieve the required power.","Bootstrap, Latent class models, Likelihood ratio test, Power, Sample size",https://link.springer.com//article/10.1007/s11634-016-0251-0,1163
Varying uncertainty in CUB models,"This paper presents a generalization of a mixture model used for the analysis of ratings and preferences by introducing a varying uncertainty component. According to the standard mixture model, called CUB model, the response probabilities are defined as a convex combination of shifted Binomial and discrete Uniform random variables. Our proposal introduces uncertainty distributions with different shapes, which could capture response style and indecision of respondents with greater effectiveness. Since we consider several alternative specifications that are nonnested, we suggest the implementation of a Vuong test for choosing among them. In this regard, some simulation experiments and real case studies confirm the usefulness of the approach.","Ordinal data, CUB models, Latent classification , Mixture models",https://link.springer.com//article/10.1007/s11634-016-0235-0,1163
Item selection by latent class-based methods: an application to nursing home evaluation,"The evaluation of nursing homes is usually based on the administration of questionnaires made of a large number of polytomous items to their patients. In such a context, the latent class model represents a useful tool for clustering subjects in homogenous groups corresponding to different degrees of impairment of the health conditions. It is known that the performance of model-based clustering and the accuracy of the choice of the number of latent classes may be affected by the presence of irrelevant or noise variables. In this paper, we show the application of an item selection algorithm to a dataset collected within a project, named ULISSE, on the quality-of-life of elderly patients hosted in Italian nursing homes. This algorithm, which is closely related to that proposed by Dean and Raftery in 2010, is aimed at finding the subset of items which provides the best clustering according to the Bayesian Information Criterion. At the same time, it allows us to select the optimal number of latent classes. Given the complexity of the ULISSE study, we perform a validation of the results by means of a sensitivity analysis, with respect to different specifications of the initial subset of items, and of a resampling procedure.","Bayesian information criterion, Expectation-maximization algorithm, Polytomous items, Quality-of-life, ULISSE project",https://link.springer.com//article/10.1007/s11634-016-0232-3,1163
Dynamic segmentation with growth mixture models,"This paper proposes a new approach to dynamically segment markets. Dynamic segmentation i of key importance in many markets where it is unrealistic to assume stationary segments due to the dynamics in consumers’ needs and product choices. The main goal of the study is to analyse the dynamic process of financial product ownership under the assumption of heterogeneous growth in different segments taking into account significant determinants of growth trajectories. Using data from 2002 to 2010 collected by the Survey of Household Income and Wealth conducted by the Bank of Italy, this article shows that the Italian market of financial products is segmented and that this behavior’s trajectories over time are significantly influenced by the area of the country where the family lives and head of household’s education and gender.","Latent growth, Segmentation, Financial market, Mixture model",https://link.springer.com//article/10.1007/s11634-015-0230-x,1163
A principal component method to impute missing values for mixed data,"We propose a new method to impute missing values in mixed data sets. It is based on a principal component method, the factorial analysis for mixed data, which balances the influence of all the variables that are continuous and categorical in the construction of the principal components. Because the imputation uses the principal axes and components, the prediction of the missing values is based on the similarity between individuals and on the relationships between variables. The properties of the method are illustrated via simulations and the quality of the imputation is assessed using real data sets. The method is compared to a recent method (Stekhoven and Buhlmann Bioinformatics 28:113–118, 2011) based on random forest and shows better performance especially for the imputation of categorical variables and situations with highly linear relationships between continuous variables.","Missing values, Mixed data, Imputation, Principal component method, Factorial analysis of mixed data",https://link.springer.com//article/10.1007/s11634-014-0195-1,1163
Extreme logistic regression,"Kernel logistic regression (KLR) is a very powerful algorithm that has been shown to be very competitive with many state-of the art machine learning algorithms such as support vector machines (SVM). Unlike SVM, KLR can be easily extended to multi-class problems and produces class posterior probability estimates making it very useful for many real world applications. However, the training of KLR using gradient based methods or iterative re-weighted least squares can be unbearably slow for large datasets. Coupled with poor conditioning and parameter tuning, training KLR can quickly design matrix become infeasible for some real datasets. The goal of this paper is to present simple, fast, scalable, and efficient algorithms for learning KLR. First, based on a simple approximation of the logistic function, a least square algorithm for KLR is derived that avoids the iterative tuning of gradient based methods. Second, inspired by the extreme learning machine (ELM) theory, an explicit feature space is constructed through a generalized single hidden layer feedforward network and used for training iterative re-weighted least squares KLR (IRLS-KLR) and the newly proposed least squares KLR (LS-KLR). Finally, for large-scale and/or poorly conditioned problems, a robust and efficient preconditioned learning technique is proposed for learning the algorithms presented in the paper. Numerical results on a series of artificial and 12 real bench-mark datasets show first that LS-KLR compares favorable with SVM and traditional IRLS-KLR in terms of accuracy and learning speed. Second, the extension of ELM to KLR results in simple, scalable and very fast algorithms with comparable generalization performance to their original versions. Finally, the introduced preconditioned learning method can significantly increase the learning speed of IRLS-KLR.","Kernel logistic regression, Extreme learning machine, Classification, Least squares, Kernel matrix , Preconditioner",https://link.springer.com//article/10.1007/s11634-014-0194-2,1163
A multilevel finite mixture item response model to cluster examinees and schools,"Within the educational context, a key goal is to assess students’ acquired skills and to cluster students according to their ability level. In this regard, a relevant element to be accounted for is the possible effect of the school students come from. For this aim, we provide a methodological tool which takes into account the multilevel structure of the data (i.e., students in schools) and allows us to cluster both students and schools into homogeneous classes of ability and effectiveness, and to assess the effect of certain students’ and school characteristics on the probability to belong to such classes. The proposed approach relies on an extended class of multidimensional latent class IRT models characterised by: (i) latent traits defined at student and school level, (ii) latent traits represented through random vectors with a discrete distribution, (iii) the inclusion of covariates at student and school level, and (iv) a two-parameter logistic parametrisation for the conditional probability of a correct response given the ability. The approach is applied for the analysis of data collected by two national tests administered in Italy to middle school students in June 2009: the INVALSI Language Test and the Mathematics Test.","EM algorithm, INVALSI Tests, Latent class model, Multilevel multidimensional item response models, Two-parameter logistic model",https://link.springer.com//article/10.1007/s11634-014-0196-0,1163
A comparison of reliability coefficients for psychometric tests that consist of two parts,"If a test consists of two parts the Spearman–Brown formula and Flanagan’s coefficient (Cronbach’s alpha) are standard tools for estimating the reliability. However, the coefficients may be inappropriate if their associated measurement models fail to hold. We study the robustness of reliability estimation in the two-part case to coefficient misspecification. We compare five reliability coefficients and study various conditions on the standard deviations and lengths of the parts. Various conditional upper bounds of the differences between the coefficients are derived. It is shown that the difference between the Spearman–Brown formula and Horst’s formula is negligible in many cases. We conclude that all five reliability coefficients can be used if there are only small or moderate differences between the standard deviations and the lengths of the parts.","Spearman–Brown formula, Cronbach’s alpha, Flanagan’s coefficient, Angoff–Feldt coefficient, Raju’s beta, Horst’s formula",https://link.springer.com//article/10.1007/s11634-015-0198-6,1163
Supervised clustering of variables,"In predictive modelling, highly correlated predictors lead to unstable models that are often difficult to interpret. The selection of features, or the use of latent components that reduce the complexity among correlated observed variables, are common strategies. Our objective with the new procedure that we advocate here is to achieve both purposes: to highlight the group structure among the variables and to identify the most relevant groups of variables for prediction. The proposed procedure is an iterative adaptation of a method developed for the clustering of variables around latent variables (CLV). Modification of the standard CLV algorithm leads to a supervised procedure, in the sense that the variable to be predicted plays an active role in the clustering. The latent variables associated with the groups of variables, selected for their “proximity” to the variable to be predicted and their “internal homogeneity”, are progressively added in a predictive model. The features of the methodology are illustrated based on a simulation study and a real-world application.","Prediction, Clustering of variables around latent variables (CLV), Forward regression model, Sparse PLS regression",https://link.springer.com//article/10.1007/s11634-014-0191-5,1163
The \(k\)-means algorithm for 3D shapes with an application to apparel design,"Clustering of objects according to shapes is of key importance in many scientific fields. In this paper we focus on the case where the shape of an object is represented by a configuration matrix of landmarks. It is well known that this shape space has a finite-dimensional Riemannian manifold structure (non-Euclidean) which makes it difficult to work with. Papers about clustering on this space are scarce in the literature. The basic foundation of the \(k\)-means algorithm is the fact that the sample mean is the value that minimizes the Euclidean distance from each point to the centroid of the cluster to which it belongs, so, our idea is integrating the Procrustes type distances and Procrustes mean into the \(k\)-means algorithm to adapt it to the shape analysis context. As far as we know, there have been just two attempts in that way. In this paper we propose to adapt the classical \(k\)-means Lloyd algorithm to the context of Shape Analysis, focusing on the three dimensional case. We present a study comparing its performance with the Hartigan-Wong \(k\)-means algorithm, one that was previously adapted to the field of Statistical Shape Analysis. We demonstrate the better performance of the Lloyd version and, finally, we propose to add a trimmed procedure. We apply both to a 3D database obtained from an anthropometric survey of the Spanish female population conducted in this country in 2006. The algorithms presented in this paper are available in the Anthropometry R package, whose most current version is always available from the Comprehensive R Archive Network.","Shape space, Statistical shape analysis, \(k\)-means algorithm,  Procrustes type distances, Procrustes mean shape, Sizing systems",https://link.springer.com//article/10.1007/s11634-014-0187-1,1163
Maximum likelihood estimation of Gaussian mixture models without matrix operations,"The Gaussian mixture model (GMM) is a popular tool for multivariate analysis, in particular, cluster analysis. The expectation–maximization (EM) algorithm is generally used to perform maximum likelihood (ML) estimation for GMMs due to the M-step existing in closed form and its desirable numerical properties, such as monotonicity. However, the EM algorithm has been criticized as being slow to converge and thus computationally expensive in some situations. In this article, we introduce the linear regression characterization (LRC) of the GMM. We show that the parameters of an LRC of the GMM can be mapped back to the natural parameters, and that a minorization–maximization (MM) algorithm can be constructed, which retains the desirable numerical properties of the EM algorithm, without the use of matrix operations. We prove that the ML estimators of the LRC parameters are consistent and asymptotically normal, like their natural counterparts. Furthermore, we show that the LRC allows for simple handling of singularities in the ML estimation of GMMs. Using numerical simulations in the R programming environment, we then demonstrate that the MM algorithm can be faster than the EM algorithm in various large data situations, where sample sizes range in the tens to hundreds of thousands and for estimating models with up to 16 mixture components on multivariate data with up to 16 variables.","Gaussian mixture model, Minorization–maximization algorithm, matrix operation-free, Linear Regression",https://link.springer.com//article/10.1007/s11634-015-0209-7,1163
Probabilistic assessment of model-based clustering,"Finite mixtures provide a powerful tool for modeling heterogeneous data. Model-based clustering is a broadly used grouping technique that assumes the existence of the one-to-one correspondence between clusters and mixture model components. Although there are many directions of active research in the model-based clustering framework, very little attention has been paid to studying the specific nature of detected clustering solutions. In this paper, we develop an approach for assessing the variability in classifications carried out by the Bayes decision rule. The proposed technique allows assessing significance of each assignment made. We also apply the developed instrument for identifying influential observations that have impact on the structure of the detected partitioning. The proposed diagnostic methodology is studied and illustrated on synthetic data and applied to the analysis of three well-known classification datasets.","Model-based clustering classification, Influential observations, Diagnostics, Gaussian mixture models",https://link.springer.com//article/10.1007/s11634-015-0215-9,1163
Robust model-based clustering via mixtures of skew-t distributions with missing information,"Multivariate mixture modeling approach using the skew-t distribution has emerged as a powerful and flexible tool for robust model-based clustering. The occurrence of missing data is a ubiquitous problem in almost every scientific field. In this paper, we offer a computationally flexible EM-type procedure for learning multivariate skew-t mixture models to deal with missing data under missing at random mechanisms. Further, we present an information-based approach to approximating the asymptotic covariance matrix of the maximum likelihood estimators using the outer product of the scores. To assist the development and ease the implementation of our algorithm, two auxiliary permutation matrices are utilized for fast determination of the observed and missing parts of each observation. The practical usefulness of the proposed methodology is illustrated through simulations with varying proportions of artificial missing values and a real data example with genuine missing values.","Classifier, ECM algorithm, Imputation, Missing at random, MST distribution",https://link.springer.com//article/10.1007/s11634-015-0221-y,1163
Improved initialisation of model-based clustering using Gaussian hierarchical partitions,"Initialisation of the EM algorithm in model-based clustering is often crucial. Various starting points in the parameter space often lead to different local maxima of the likelihood function and, so to different clustering partitions. Among the several approaches available in the literature, model-based agglomerative hierarchical clustering is used to provide initial partitions in the popular mclustR package. This choice is computationally convenient and often yields good clustering partitions. However, in certain circumstances, poor initial partitions may cause the EM algorithm to converge to a local maximum of the likelihood function. We propose several simple and fast refinements based on data transformations and illustrate them through data examples.","Model-based clustering, Model-based agglomerative hierarchical clustering, mclust, Data transformation",https://link.springer.com//article/10.1007/s11634-015-0220-z,1163
Simulating mixtures of multivariate data with fixed cluster overlap in FSDA library,"We extend the capabilities of MixSim, a framework which is useful for evaluating the performance of clustering algorithms, on the basis of measures of agreement between data partitioning and flexible generation methods for data, outliers and noise. The peculiarity of the method is that data are simulated from normal mixture distributions on the basis of pre-specified synthesis statistics on an overlap measure, defined as a sum of pairwise misclassification probabilities. We provide new tools which enable us to control additional overlapping statistics and departures from homogeneity and sphericity among groups, together with new outlier contamination schemes. The output of this extension is a more flexible framework for generation of data to better address modern robust clustering scenarios in presence of possible contamination. We also study the properties and the implications that this new way of simulating clustering data entails in terms of coverage of space, goodness of fit to theoretical distributions, and degree of convergence to nominal values. We demonstrate the new features using our MATLAB implementation that we have integrated in the Flexible Statistics for Data Analysis (FSDA) toolbox for MATLAB. With MixSim, FSDA now integrates in the same environment state of the art robust clustering algorithms and principled routines for their evaluation and calibration. A spin off of our work is a general complex routine, translated from C language to MATLAB, to compute the distribution function of a linear combinations of non central \(\chi ^2\) random variables which is at the core of MixSim and has its own interest for many test statistics.","MixSim, FSDA, Synthetic data, Mixture models , Robust clustering",https://link.springer.com//article/10.1007/s11634-015-0223-9,1163
Latent drop-out based transitions in linear quantile hidden Markov models for longitudinal responses with attrition,"Longitudinal data are characterized by the dependence between observations from the same individual. In a regression perspective, such a dependence can be usefully ascribed to unobserved features (covariates) specific to each individual. On these grounds, random parameter models with time-constant or time-varying structure are now well established in the generalized linear model context. In the quantile regression framework, specifications based on random parameters have only recently known a flowering interest. We start from the recent proposal by Farcomeni (Stat Comput 22:141–152, 2012) on longitudinal quantile hidden Markov models, and extend it to handle potentially informative missing data mechanisms. In particular, we focus on monotone missingness which may lead to selection bias and, therefore, to unreliable inferences on model parameters. We detail the proposed approach by re-analyzing a well known dataset on the dynamics of CD4 cell counts in HIV seroconverters and by means of a simulation study reported in the supplementary material.","Quantile regression, Longitudinal data, Hidden Markov models, Latent drop-out classes",https://link.springer.com//article/10.1007/s11634-015-0222-x,1163
A new biplot procedure with joint classification of objects and variables by fuzzy c-means clustering,"Biplot is a technique for obtaining a low-dimensional configuration of the data matrix in which both the objects and the variables of the data matrix are jointly represented as points and vectors, respectively. However, biplots with a large number of objects and variables remain difficult to interpret. Therefore, in this research, we propose a new biplot procedure that allows us to interpret a large data matrix. In particular, the objects and variables are classified into a small number of clusters by using fuzzy \(c\)-means clustering and the resulting clusters are simultaneously biplotted in lower-dimensional space. This procedure allows us to understand the configurations easily and to grasp the fuzzy memberships of the objects and variables to the clusters. A simulation study and real data example are also provided to demonstrate the effectiveness of the proposed procedure.","Biplot, Fuzzy \(c\)-means clustering, Principal component analysis, Singular value decomposition, Alternating least squares",https://link.springer.com//article/10.1007/s11634-014-0184-4,1163
Probabilistic auto-associative models and semi-linear PCA,"Auto-associative models cover a large class of methods used in data analysis, including for example principal component analysis (PCA) and auto-associative neural networks. In this paper, we describe the general properties of these models when the projection component is linear and we propose and test an easy-to-implement probabilistic semi-linear auto-associative model in a Gaussian setting. We show that it is a generalization of the PCA model to the semi-linear case. Numerical experiments on simulated datasets and a real astronomical application highlight the interest of this approach.","Auto-associative models, Non-linear PCA, Dimension reduction, Probabilistic non-linear PCA",https://link.springer.com//article/10.1007/s11634-014-0185-3,1163
Classifying real-world data with the \({ DD}\alpha \)-procedure,"The \({ DD}\alpha \)-classifier, a nonparametric fast and very robust procedure, is described and applied to fifty classification problems regarding a broad spectrum of real-world data. The procedure first transforms the data from their original property space into a depth space, which is a low-dimensional unit cube, and then separates them by a projective invariant procedure, called \(\alpha \)-procedure. To each data point the transformation assigns its depth values with respect to the given classes. Several alternative depth notions (spatial depth, Mahalanobis depth, projection depth, and Tukey depth, the latter two being approximated by univariate projections) are used in the procedure, and compared regarding their average error rates. With the Tukey depth, which fits the distributions’ shape best and is most robust, ‘outsiders’, that is data points having zero depth in all classes, appear. They need an additional treatment for classification. Evidence is also given about the dimension of the extended feature space needed for linear separation. The \({ DD}\alpha \)-procedure is available as an R-package.","Classification, Supervised learning, Alpha-procedure, Data depth, Spatial depth, Projection depth, Random Tukey depth, Outsiders, Features",https://link.springer.com//article/10.1007/s11634-014-0180-8,1163
Financial clustering in presence of dominant markets,"Clustering financial time series is a recent topic of statistical literature with important fields of applications, in particular portfolio composition and risk evaluation. The risk is generally linked to the volatility of the asset, but its level of predictability also plays a basic role in investment decisions. In particular, the classification of a certain asset could be linked to its dependence on the volatility of a dominant market: movements in the volatility of the dominant market can provide similar movements in the volatility of the asset and its predictability would depend on the strength of this dependence. Working in a model based framework, we base the classification of the volatility of an asset not only on its volatility level, but also on the presence of spillover effects from a dominant market, such as the US one, and on the similarity of the dynamics of the asset and the dominant market. The method is carried out using an extended version of the Multiplicative Error Model and is applied to a set of European assets, also performing a historical simulation experiment.","MEM, Unconditional volatility, Spillover effect , Common dynamics, AR distance",https://link.springer.com//article/10.1007/s11634-014-0189-z,1163
A diffusion model for churn prediction based on sociometric theory,"Churn prediction has received much attention in the last decade. With the evolution of social networks and social network analysis tools in recent years, the consideration of social ties in churn prediction has proven promising. One possibility is to use energy diffusion models to model the spread of influence through a social network. This paper proposes a novel churn prediction diffusion model based on sociometric clique and social status theory. It describes the concept of energy in the diffusion model as an opinion of users, which is transformed to user influence using the derived social status function. Furthermore, a novel diffusion model prediction scheme applicable to a single user or a small subset of users is described: the Targeted User Subset Churn Prediction Scheme. The scheme allows fast churn prediction using limited computing resources. The diffusion model is evaluated on a real dataset of users obtained from the largest Slovenian mobile service provider, using the F-measure and lift curve. The empirical results show a significant improvement in prediction accuracy of the proposed method compared with the basic spreading activation technique (SPA) diffusion model. More specifically, our approach outperforms a basic SPA diffusion model by 116 % in terms of lift in the fifth percentile.","Diffusion model, Churn prediction, Sociometric clique, Social status, Telecommunications",https://link.springer.com//article/10.1007/s11634-014-0188-0,1163
Correspondence analysis of textual data involving contextual information: CA-GALT on principal components,"Correspondence analysis on an aggregated lexical table is a typical practice in textual analysis in which a contextual categorical variable is used to aggregate documents, depending on the categories to which they belong. This work generalises this approach and considers several quantitative, categorical or mixed contextual variables. The result is a new method that we have called ‘correspondence analysis on a generalised aggregated lexical table’. A favoured application derives from surveys by questionnaire, including both open-ended and closed questions. The free-text answers are encoded into a respondents \(\times \) words frequency table called a lexical table. The closed questions, either quantitative or categorical, form the contextual variables. The primary objective is to establish a typology of the variables and a typology of the words from their mutual relationships as grasped from jointly analysing the textual and contextual tables. Validation tests are offered, particularly in the form of confidence ellipses. The comprehensive and numerous properties of the method, similar to correspondence analysis properties, are detailed. Promising results are obtained as indicated by an application to a marketing survey conducted among 1,000 respondents.","Correspondence analysis, Textual and contextual data , Textual analysis, Lexical table, Confidence ellipses , Contingency table",https://link.springer.com//article/10.1007/s11634-014-0171-9,1163
Basic statistics for distributional symbolic variables: a new metric-based approach,"In data mining it is usual to describe a group of measurements using summary statistics or through empirical distribution functions. Symbolic data analysis (SDA) aims at the treatment of such kinds of data, allowing the description and the analysis of conceptual data or of macrodata summarizing classical data. In the conceptual framework of SDA, the paper aims at presenting new basic statistics for distribution-valued variables, i.e., variables whose realizations are distributions. The proposed measures extend some classical univariate (mean, variance, standard deviation) and bivariate (covariance and correlation) basic statistics to distribution-valued variables, taking into account the nature and the variability of such data. The novel statistics are based on a distance between distributions: the \(\ell _2\) Wasserstein distance. A comparison with other univariate and bivariate statistics presented in the literature points out some relevant properties of the proposed ones. An application on a clinic dataset shows the main differences in terms of interpretation of results.","Wasserstein metric, Symbolic data, Distribution-valued data, Histogram data, Basic statistics",https://link.springer.com//article/10.1007/s11634-014-0176-4,1163
Enhancing the selection of a model-based clustering with external categorical variables,"In cluster analysis, it can be useful to interpret the partition built from the data in the light of external categorical variables which are not directly involved to cluster the data. An approach is proposed in the model-based clustering context to select a number of clusters which both fits the data well and takes advantage of the potential illustrative ability of the external variables. This approach makes use of the integrated joint likelihood of the data and the partitions at hand, namely the model-based partition and the partitions associated to the external variables. It is noteworthy that each mixture model is fitted by the maximum likelihood methodology to the data, excluding the external variables which are used to select a relevant mixture model only. Numerical experiments illustrate the promising behaviour of the derived criterion.","Mixture models, Model-based clustering, Number of clusters, Penalised criteria, Categorical variables, BIC , ICL, Mixed type variables clustering",https://link.springer.com//article/10.1007/s11634-014-0177-3,1163
Mixture model averaging for clustering,"In mixture model-based clustering applications, it is common to fit several models from a family and report clustering results from only the ‘best’ one. In such circumstances, selection of this best model is achieved using a model selection criterion, most often the Bayesian information criterion. Rather than throw away all but the best model, we average multiple models that are in some sense close to the best one, thereby producing a weighted average of clustering results. Two (weighted) averaging approaches are considered: averaging component membership probabilities and averaging models. In both cases, Occam’s window is used to determine closeness to the best model and weights are computed within a Bayesian model averaging paradigm. In some cases, we need to merge components before averaging; we introduce a method for merging mixture components based on the adjusted Rand index. The effectiveness of our model-based clustering averaging approaches is illustrated using a family of Gaussian mixture models on real and simulated data.","Clustering, Mixture models, Model averaging, Model-based clustering",https://link.springer.com//article/10.1007/s11634-014-0182-6,1163
Spline-based nonlinear biplots,"Biplots are helpful tools to establish the relations between samples and variables in a single plot. Most biplots use a projection interpretation of sample points onto linear lines representing variables. These lines can have marker points to make it easy to find the reconstructed value of the sample point on that variable. For classical multivariate techniques such as principal components analysis, such linear biplots are well established. Other visualization techniques for dimension reduction, such as multidimensional scaling, focus on an often nonlinear mapping in a low dimensional space with emphasis on the representation of the samples. In such cases, the linear biplot can be too restrictive to properly describe the relations between the samples and the variables. In this paper, we propose a simple nonlinear biplot that represents the marker points of a variable on a curved line that is governed by splines. Its main attraction is its simplicity of interpretation: the reconstructed value of a sample point on a variable is the value of the closest marker point on the smooth curved line representing the variable. The proposed spline-based biplot can never lead to a worse overall sample fit of the variable as it contains the linear biplot as a special case.","Biplot, Multidimensional scaling, Principal components analysis, Splines",https://link.springer.com//article/10.1007/s11634-014-0179-1,1163
Lasso-constrained regression analysis for interval-valued data,"A new method of regression analysis for interval-valued data is proposed. The relationship between an interval-valued response variable and a set of interval-valued explanatory variables is investigated by considering two regression models, one for the midpoints and the other one for the radii. The estimation problem is approached by introducing Lasso-based constraints on the regression coefficients. This can improve the prediction accuracy of the model and, taking into account the nature of the constraints, can sometimes produce a parsimonious model with a common subset of regression coefficients for the midpoint and the radius models. The effectiveness of our method, called Lasso-IR (Lasso-based Interval-valued Regression), is shown by a simulation experiment and some applications to real data.","Interval-valued data, Regression, Lasso, Prediction accuracy",https://link.springer.com//article/10.1007/s11634-014-0164-8,1163
Trimmed fuzzy clustering for interval-valued data,"In this paper, following a partitioning around medoids approach, a fuzzy clustering model for interval-valued data, i.e., FCMd-ID, is introduced. Successively, for avoiding the disruptive effects of possible outlier interval-valued data in the clustering process, a robust fuzzy clustering model with a trimming rule, called Trimmed Fuzzy \(C\)-medoids for interval-valued data (TrFCMd-ID), is proposed. In order to show the good performances of the robust clustering model, a simulation study and two applications are provided.","Interval-valued data, Partitioning around medoids, Fuzzy clustering, Robust clustering, Trimming, Web advertising",https://link.springer.com//article/10.1007/s11634-014-0169-3,1163
Modeling and forecasting interval time series with threshold models,"This paper proposes threshold models to analyze and forecast interval-valued time series. A relatively simple algorithm is proposed to obtain least square estimates of the threshold and slope parameters. The construction of forecasts based on the proposed model and methods for the analysis of their forecast performance are also introduced and discussed, as well as forecasting procedures based on the combination of different models. To illustrate the usefulness of the proposed methods, an empirical application on a weekly sample of S&P500 index returns is provided. The results obtained are encouraging and compare very favorably to available procedures.","Interval-valued data, Time series, Nonlinearities , Threshold models, Combined forecasts, S&P500 index",https://link.springer.com//article/10.1007/s11634-014-0170-x,1163
Principal component analysis for probabilistic symbolic data: a more generic and accurate algorithm,"In the symbolic data framework, probabilistic symbolic data are considered as those whose components are random variables with general probability distributions. Intervals (or uniform distributions), histograms (or empirical distributions), Gaussian distribution and Chi-squared distribution are all the special cases of them. The existing approaches devoted to the subject have a common shortcoming since they can not obtain the distributions of linear combinations (i.e., principal components) of random variables especially for not identically distributed ones. This paper will overcome the shortcoming by providing an exact probability density function for each principal component by using the inversion theorem. Further, the paper defines a covariance matrix for probabilistic symbolic data and presents a new principal component analysis based on this variance–covariance structure. The effectiveness of the proposed method is illustrated by a simulated numerical experiment, and two real-life cases including clustering of oils and fats data, and evaluation of indexed journals of Science Citation Index.","Principal component analysis, Symbolic data,  Probabilistic symbolic data, Characteristic function",https://link.springer.com//article/10.1007/s11634-014-0178-2,1163
Linear regression for numeric symbolic variables: a least squares approach based on Wasserstein Distance,"In this paper we present a new linear regression technique for distributional symbolic variables, i.e., variables whose realizations can be histograms, empirical distributions or empirical estimates of parametric distributions. Such data are known as numerical modal data  according to the Symbolic Data Analysis definitions. In order to measure the error between the observed and the predicted distributions, the \(\ell _2\) Wasserstein distance is proposed. Some properties of such a metric are exploited to predict the modal response variable as a linear combination of the explanatory modal variables. Based on the metric, the model uses the quantile functions associated with the data and thus is subject to a positivity constraint of the estimated parameters. We propose solving the linear regression problem by starting from a particular decomposition of the squared distance. Therefore, we estimate the model parameters according to two separate models, one for the averages of the data and one for the centered distributions by a constrained least squares algorithm. Measures of goodness-of-fit are also proposed and discussed. The method is validated by two applications, one on simulated data and one on two real-world datasets.","Modal symbolic variables, Probability distribution function,  Histogram data, Regression, Wasserstein distance",https://link.springer.com//article/10.1007/s11634-015-0197-7,1163
Strategies evaluation in environmental conditions by symbolic data analysis: application in medicine and epidemiology to trachoma,"Trachoma, caused by repeated ocular infections with Chlamydia trachomatis whose vector is a fly, is an important cause of blindness in the world. We are presenting here an application of the Symbolic Data Analysis approach to an interventional study on trachoma conducted in Mali. This study was conducted to choose among three antibiotic strategies those with the best cost-effectiveness ratio and to find the demographic and environmental parameters on which we could try to intervene. The Symbolic Data Analysis approach aims at studying classes of individuals considered as new units. These units are described by variables whose values express for each class the variation of the values taken by each of its individuals. Finally, the results obtained are compared to those previously provided by multiple logistic regression analysis. Symbolic Data Analysis actually provides a new perspective on this study and suggests that some demographic, economics and environmental parameters are related to the disease and its evolution during the treatment, whatever the strategy. Moreover, it is shown that the efficiency of each strategy depends on environmental parameters.","Symbolic Data Analysis, Multiple logistic regression , Trachoma",https://link.springer.com//article/10.1007/s11634-015-0201-2,1163
Clustering of financial time series in risky scenarios,"A methodology is presented for clustering financial time series according to the association in the tail of their distribution. The procedure is based on the calculation of suitable pairwise conditional Spearman’s correlation coefficients extracted from the series. The performance of the method has been tested via a simulation study. As an illustration, an analysis of the components of the Italian FTSE–MIB is presented. The results could be applied to construct financial portfolios that can manage to reduce the risk in case of simultaneous large losses in several markets.","Cluster analysis, Copula, Spearman’s correlation, Tail dependence",https://link.springer.com//article/10.1007/s11634-013-0160-4,1163
Feature selection for fault level diagnosis of planetary gearboxes,"Feature selection is critical to maintain high performance of classification-based fault diagnosis with a large feature size. In this paper, we propose a criterion to evaluate features effectiveness by class separability that is defined on cosine similarity in the kernel space of the Gaussian radial basis function. We develop a feature selection algorithm accordingly using the proposed criterion together with sequential backward selection and a feature re-ranking mechanism. We then employ the proposed feature selection algorithm to determine fault-sensitive features and select them for fault level diagnosis of planetary gearboxes. The experimental results demonstrate that the proposed algorithm can effectively reduce the feature size and improve accuracy of fault level diagnosis simultaneously.","Fault diagnosis, Feature selection, Class separability,  Cosine similarity, Planetary gearbox",https://link.springer.com//article/10.1007/s11634-014-0168-4,1163
A comparison of five recursive partitioning methods to find person subgroups involved in meaningful treatment–subgroup interactions,"In case multiple treatment alternatives are available for some medical problem, the detection of treatment–subgroup interactions (i.e., relative treatment effectiveness varying over subgroups of persons) is of key importance for personalized medicine and the development of optimal treatment assignment strategies. Randomized Clinical Trials (RCT) often go without clear a priori hypotheses on the subgroups involved in treatment–subgroup interactions, and with a large number of pre-treatment characteristics in the data. In such situations, relevant subgroups (defined in terms of pre-treatment characteristics) are to be induced during the actual data analysis. This comes down to a problem of cluster analysis, with the goal of this analysis being to find clusters of persons that are involved in meaningful treatment–person cluster interactions. For such a cluster analysis, five recently proposed methods can be used, all being of a recursive partitioning type. However, these five methods have been developed almost independently, and the relations between them are not yet understood. The present paper closes this gap. It starts by outlining the basic principles behind each method, and by illustrating it with an application on an RCT data set on two treatment strategies for substance abuse problems. Next, it presents a comparison of the methods, hereby focusing on major similarities and differences. The discussion concludes with practical advice for end users with regard to the selection of a suitable method, and with an important challenge for future research in this area.","Treatment heterogeneity, Recursive partitioning, Subgroup analysis, Treatment–subgroup interaction",https://link.springer.com//article/10.1007/s11634-013-0159-x,1163
A latent class analysis of the public attitude towards the euro adoption in Poland,"Latent class analysis can be viewed as a special case of model–based clustering for multivariate discrete data. It is assumed that each observation comes from one of a number of classes, groups or subpopulations, with its own probability distribution. The overall population thus follows a finite mixture model. When observed, data take the form of categorical responses—as, for example, in public opinion or consumer behavior surveys it is often of interest to identify and characterize clusters of similar objects. In the context of marketing research, one will typically interpret the latent number of mixture components as clusters or segments. In fact, LC analysis provides a powerful new tool to identify important market segments in target marketing. We used the model based clustering approach for grouping and detecting inhomogeneities of Polish opinions on the euro adoption. We analyzed data collected as part of the Polish General Social Survey using the R software.","Latent class analysis, Mixture model, Categorical data, Euro adoption",https://link.springer.com//article/10.1007/s11634-013-0156-0,1163
Functional data clustering: a survey,"Clustering techniques for functional data are reviewed. Four groups of clustering algorithms for functional data are proposed. The first group consists of methods working directly on the evaluation points of the curves. The second groups is defined by filtering methods which first approximate the curves into a finite basis of functions and second perform clustering using the basis expansion coefficients. The third groups is composed of methods which perform simultaneously dimensionality reduction of the curves and clustering, leading to functional representation of data depending on clusters. The last group consists of distance-based methods using clustering algorithms based on specific distances for functional data. A software review as well as an illustration of the application of these algorithms on real data are presented.","Functional data, Clustering, Basis expansion,  Functional principal component analysis",https://link.springer.com//article/10.1007/s11634-013-0158-y,1163
Spatial functional normal mixed effect approach for curve classification,"This paper proposes a spatial functional formulation of the normal mixed effect model for the statistical classification of spatially dependent Gaussian curves, in both parametric and state space model frameworks. Fixed effect parameters are represented in terms of a functional multiple regression model whose regression operators can change in space. Local spatial homogeneity of these operators is measured in terms of their Hilbert–Schmidt distances, leading to the classification of fixed effect curves in different groups. Assuming that the Gaussian random effect curves obey a spatial autoregressive dynamics of order one [SARH(1) dynamics], a second functional classification criterion is proposed in order to detect local spatially homogeneous patterns in the mean quadratic functional variation of Gaussian random effect curve increments. Finally, the two criteria are combined to detect local spatially homogeneous patterns in the regression operators and in the functional mean quadratic variation, under a state space approach. A real data example in the financial context is analyzed as an illustration.","Empirical functional variogram, Firm financial structure,  Functional multiple regression, Spatial functional mixed effect models,  Spatial Hilbert-valued Gaussian processes",https://link.springer.com//article/10.1007/s11634-014-0174-6,1163
Principal differential analysis of the Aneurisk65 data set,"We explore the use of principal differential analysis as a tool for performing dimensional reduction of functional data sets. In particular, we compare the results provided by principal differential analysis and by functional principal component analysis in the dimensional reduction of three synthetic data sets, and of a real data set concerning 65 three-dimensional cerebral geometries, the AneuRisk65 data set. The analyses show that principal differential analysis can provide an alternative and effective representation of functional data, easily interpretable in terms of exponential, sinusoidal, or damped-sinusoidal functions and providing a different insight to the functional data set under investigation. Moreover, in the analysis of the AneuRisk65 data set, principal differential analysis is able to detect interesting features of the data, such as the rippling effect of the vessel surface, that functional principal component analysis is not able to detect.","Functional data analysis, Dimensional reduction, Principal differential analysis, Functional principal component analysis",https://link.springer.com//article/10.1007/s11634-014-0175-5,1163
Hilbertian spatial periodically correlated first order autoregressive models,"In this article, we consider Hilbertian spatial periodically correlated autoregressive models. Such a spatial model assumes periodicity in its autocorrelation function. Plausibly, it explains spatial functional data resulted from phenomena with periodic structures, as geological, atmospheric, meteorological and oceanographic data. Our studies on these models include model building, existence, time domain moving average representation, least square parameter estimation and prediction based on the autoregressive structured past data. We also fit a model of this type to a real data of invisible infrared satellite images.","Hilbert spaces, Spatial autoregressive random fields ,  Periodically correlated processes, Functional data",https://link.springer.com//article/10.1007/s11634-014-0172-8,1163
Simplicial band depth for multivariate functional data,"We propose notions of simplicial band depth for multivariate functional data that extend the univariate functional band depth. The proposed simplicial band depths provide simple and natural criteria to measure the centrality of a trajectory within a sample of curves. Based on these depths, a sample of multivariate curves can be ordered from the center outward and order statistics can be defined. Properties of the proposed depths, such as invariance and consistency, can be established. A simulation study shows the robustness of this new definition of depth and the advantages of using a multivariate depth versus the marginal depths for detecting outliers. Real data examples from growth curves and signature data are used to illustrate the performance and usefulness of the proposed depths.","Band depth, Functional boxplot, Functional and image data,  Modified band depth, Multivariate, Simplicial",https://link.springer.com//article/10.1007/s11634-014-0166-6,1163
New insights on permutation approach for hypothesis testing on functional data,"The permutation approach for testing the equality of distributions and thereby comparing two populations of functional data has recently received increasing attention thanks to the flexibility of permutation tests to handle complex testing problems. The purpose of this work is to present some new insights in the context of nonparametric inference on functional data using the permutation approach, more specifically we formally show the equivalence of some permutation procedures proposed in the literature and we suggest the use of the permutation and combination-based approach within the basis function approximation layout. Validation of theoretical results is shown by simulation studies.","Data compression, Functional data, Nonparametric combination, Permutation tests, Permutationally equivalent test statistic",https://link.springer.com//article/10.1007/s11634-013-0162-2,1163
A comparison of some criteria for states selection in the latent Markov model for longitudinal data,"We compare different selection criteria to choose the number of latent states of a multivariate latent Markov model for longitudinal data. This model is based on an underlying Markov chain to represent the evolution of a latent characteristic of a group of individuals over time. Then, the response variables observed at different occasions are assumed to be conditionally independent given this chain. Maximum likelihood estimation of the model is carried out through an Expectation–Maximization algorithm based on forward–backward recursions which are well known in the hidden Markov literature for time series. The selection criteria we consider are based on penalized versions of the maximum log-likelihood or on the posterior probabilities of belonging to each latent state, that is, the conditional probability of the latent state given the observed data. Among the latter criteria, we propose an appropriate entropy measure tailored for the latent Markov models. We show the results of a Monte Carlo simulation study aimed at comparing the performance of the above states selection criteria on the basis of a wide set of model specifications.","Akaike information criterion, Bayesian information criterion, Entropy, Mixture model, Multivariate latent Markov model, Normalized entropy criterion",https://link.springer.com//article/10.1007/s11634-013-0154-2,1163
Graphical tools for model-based mixture discriminant analysis,"The paper introduces a methodology for visualizing on a dimension reduced subspace the classification structure and the geometric characteristics induced by an estimated Gaussian mixture model for discriminant analysis. In particular, we consider the case of mixture of mixture models with varying parametrization which allow for parsimonious models. The approach is an extension of an existing work on reducing dimensionality for model-based clustering based on Gaussian mixtures. Information on the dimension reduction subspace is provided by the variation on class locations and, depending on the estimated mixture model, on the variation on class dispersions. Projections along the estimated directions provide summary plots which help to visualize the structure of the classes and their characteristics. A suitable modification of the method allows us to recover the most discriminant directions, i.e., those that show maximal separation among classes. The approach is illustrated using simulated and real data.","Dimension reduction, Model-based discriminant analysis,  Gaussian mixtures, Canonical variates for mixture modeling",https://link.springer.com//article/10.1007/s11634-013-0147-1,1163
Variational Bayes approximations for clustering via mixtures of normal inverse Gaussian distributions,Parameter estimation for model-based clustering using a finite mixture of normal inverse Gaussian (NIG) distributions is achieved through variational Bayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are considered. The use of variational Bayes approximations here is a substantial departure from the traditional EM approach and alleviates some of the associated computational complexities and uncertainties. Our variational algorithm is applied to simulated and real data. The paper concludes with discussion and suggestions for future work.,"Clustering, MNIG, NIG, Normal inverse Gaussian ,  Variational approximations, Variational Bayes",https://link.springer.com//article/10.1007/s11634-014-0165-7,1163
Threshold optimization for classification in imbalanced data in a problem of gamma-ray astronomy,"We introduce a method to minimize the mean square error (MSE) of an estimator which is derived from a classification. The method chooses an optimal discrimination threshold in the outcome of a classification algorithm and deals with the problem of unequal and unknown misclassification costs and class imbalance. The approach is applied to data from the MAGIC experiment in astronomy for choosing an optimal threshold for signal-background-separation. In this application one is interested in estimating the number of signal events in a dataset with very unfavorable signal to background ratio. Minimizing the MSE of the estimation is a rather general approach which can be adapted to various other applications, in which one wants to derive an estimator from a classification. If the classification depends on other or additional parameters than the discrimination threshold, MSE minimization can be used to optimize these parameters as well. We illustrate this by optimizing the parameters of logistic regression, leading to relevant improvements of the current approach used in the MAGIC experiment.","Classification, Thresholding, MAGIC, Imbalanced data,  Unknown misclassification costs, Random forest",https://link.springer.com//article/10.1007/s11634-014-0167-5,1163
Estimating common principal components in high dimensions,"We consider the problem of minimizing an objective function that depends on an orthonormal matrix. This situation is encountered, for example, when looking for common principal components. The Flury method is a popular approach but is not effective for higher dimensional problems. We obtain several simple majorization–minimization (MM) algorithms that provide solutions to this problem and are effective in higher dimensions. We use mixture model-based clustering applications to illustrate our MM algorithms. We then use simulated data to compare them with other approaches, with comparisons drawn with respect to convergence and computational time.","Clustering, Common principal components, GPCM, Flury, Minimization, Maximization, Mixture, Mixture models, Model-based clustering, MM algorithm.",https://link.springer.com//article/10.1007/s11634-013-0139-1,1163
Robust clustering around regression lines with high density regions,"Robust methods are needed to fit regression lines when outliers are present. In a clustering framework, outliers can be extreme observations, high leverage points, but also data points which lie among the groups. Outliers are also of paramount importance in the analysis of international trade data, which motivate our work, because they may provide information about anomalies like fraudulent transactions. In this paper we show that robust techniques can fail when a large proportion of non-contaminated observations fall in a small region, which is a likely occurrence in many international trade data sets. In such instances, the effect of a high-density region is so strong that it can override the benefits of trimming and other robust devices. We propose to solve the problem by sampling a much smaller subset of observations which preserves the cluster structure and retains the main outliers of the original data set. This goal is achieved by defining the retention probability of each point as an inverse function of the estimated density function for the whole data set. We motivate our proposal as a thinning operation on a point pattern generated by different components. We then apply robust clustering methods to the thinned data set for the purposes of classification and outlier detection. We show the advantages of our method both in empirical applications to international trade examples and through a simulation study.","Anti-fraud, Concentrated noise, International trade,  Orthogonal regression, Outlier detection, Robust clusterwise regression,  TCLUST, Thinning, Trimming",https://link.springer.com//article/10.1007/s11634-013-0151-5,1163
A constrained robust proposal for mixture modeling avoiding spurious solutions,The high prevalence of spurious solutions and the disturbing effect of outlying observations in mixture modeling are well known problems that pose serious difficulties for non-expert practitioners of this kind of models in different applied areas. An approach which combines the use of Trimmed Maximum Likelihood ideas and the imposition of restrictions on the maximization problem will be presented and studied in this paper. The proposed methodology is shown to have nice mathematical properties as well as good performance in avoiding the appearance of spurious solutions in a quite automatic manner.,"Mixture models, Constraints, Robustness, Trimming,  Eigenvalues restrictions, Maximum likelihood estimation",https://link.springer.com//article/10.1007/s11634-013-0153-3,1163
A LASSO-penalized BIC for mixture model selection,"The efficacy of family-based approaches to mixture model-based clustering and classification depends on the selection of parsimonious models. Current wisdom suggests the Bayesian information criterion (BIC) for mixture model selection. However, the BIC has well-known limitations, including a tendency to overestimate the number of components as well as a proclivity for underestimating, often drastically, the number of components in higher dimensions. While the former problem might be soluble by merging components, the latter is impossible to mitigate in clustering and classification applications. In this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome this problem. This approach is illustrated based on applications of extensions of mixtures of factor analyzers, where the LPBIC is used to select both the number of components and the number of latent factors. The LPBIC is shown to match or outperform the BIC in several situations.","BIC, LASSO, Mixture models, Model-based clustering,  Model selection",https://link.springer.com//article/10.1007/s11634-013-0155-1,1163
Classification of brain activation via spatial Bayesian variable selection in fMRI regression,"Functional magnetic resonance imaging (fMRI) is the most popular technique in human brain mapping, with statistical parametric mapping (SPM) as a classical benchmark tool for detecting brain activity. Smith and Fahrmeir (J Am Stat Assoc 102(478):417–431, 2007) proposed a competing method based on a spatial Bayesian variable selection in voxelwise linear regressions, with an Ising prior for latent activation indicators. In this article, we alternatively link activation probabilities to two types of latent Gaussian Markov random fields (GMRFs) via a probit model. Statistical inference in resulting high-dimensional hierarchical models is based on Markov chain Monte Carlo approaches, providing posterior estimates of activation probabilities and enhancing formation of activation clusters. Three algorithms are proposed depending on GMRF type and update scheme. An application to an active acoustic oddball experiment and a simulation study show a substantial increase in sensitivity compared to existing fMRI activation detection methods like classical SPM and the Ising model.","Functional magnetic resonance imaging, Bayesian image analysis, Human brain mapping, Probit link, Markov random field priors,  Markov chain Monte Carlo",https://link.springer.com//article/10.1007/s11634-013-0142-6,1163
Mixtures of biased sentiment analysers,"Modelling bias is an important consideration when dealing with inexpert annotations. We are concerned with training a classifier to perform sentiment analysis on news media articles, some of which have been manually annotated by volunteers. The classifier is trained on the words in the articles and then applied to non-annotated articles. In previous work we found that a joint estimation of the annotator biases and the classifier parameters performed better than estimation of the biases followed by training of the classifier. An important question follows from this result: can the annotators be usefully clustered into either predetermined or data-driven clusters, based on their biases? If so, such a clustering could be used to select, drop or otherwise categorise the annotators in a crowdsourcing task. This paper presents work on fitting a finite mixture model to the annotators’ bias. We develop a model and an algorithm and demonstrate its properties on simulated data. We then demonstrate the clustering that exists in our motivating dataset, namely the analysis of potentially economically relevant news articles from Irish online news sources.","Bias modelling, Crowdsourcing, EM algorithm, Mixture model, Sentiment analysis",https://link.springer.com//article/10.1007/s11634-013-0150-6,1163
Latent class CUB models,"The paper proposes a latent class version of Combination of Uniform and (shifted) Binomial random variables ( CUB ) models for ordinal data to account for unobserved heterogeneity. The extension, called  LC-CUB , is useful when the heterogeneity is originated by clusters of respondents not identified by covariates: this may generate a multimodal response distribution, which cannot be adequately described by a standard  CUB model. The  LC-CUB model is a finite mixture of  CUB models yielding a multimodal theoretical distribution. Model identification is achieved by constraining the uncertainty parameters to be constant across latent classes. A simulation experiment shows the performance of the maximum likelihood estimator, whereas the usefulness of the approach is illustrated by means of a case study on political self-placement measured on an ordinal scale.","Finite mixture, Maximum likelihood, Ordinal data , Simulation, Unobserved heterogeneity",https://link.springer.com//article/10.1007/s11634-013-0143-5,1163
Lagrangian relaxation and pegging test for the clique partitioning problem,"The clique partitioning problem is an NP-hard combinatorial optimization problem with applications to data analysis such as clustering. Though a binary integer linear programming formulation has been known for years, one needs to deal with a huge number of variables and constraints when solving a large instance. In this paper, we propose a size reduction algorithm which is based on the Lagrangian relaxation and the pegging test, and verify its validity through numerical experiments. We modify the conventional subgradient method in order to manage the high dimensionality of the Lagrangian multipliers, and also make an improvement on the ordinary pegging test by taking advantage of the structural property of the clique partitioning problem.","Integer programming, Lagrangian relaxation, Clique partitioning, Clustering",https://link.springer.com//article/10.1007/s11634-013-0135-5,1163
Multinomial logit models with implicit variable selection,"The multinomial logit model is the most widely used model for the unordered multi-category responses. However, applications are typically restricted to the use of few predictors because in the high-dimensional case maximum likelihood estimates frequently do not exist. In this paper we are developing a boosting technique called multinomBoost that performs variable selection and fits the multinomial logit model also when predictors are high-dimensional. Since in multi-category models the effect of one predictor variable is represented by several parameters one has to distinguish between variable selection and parameter selection. A special feature of the approach is that, in contrast to existing approaches, it selects variables not parameters. The method can also distinguish between mandatory predictors and optional predictors. Moreover, it adapts to metric, binary, nominal and ordinal predictors. Regularization within the algorithm allows to include nominal and ordinal variables which have many categories. In the case of ordinal predictors the order information is used. The performance of boosting technique with respect to mean squared error, prediction error and the identification of relevant variables is investigated in a simulation study. The method is applied to the national Indonesia contraceptive prevalence survey and the identification of glass. Results are also compared with the Lasso approach which selects parameters.","False alarm rate, Hit rate, Likelihood-based boosting,  Logistic regression, Multinomial logit, Penalization, Side constraints,  Variable selection",https://link.springer.com//article/10.1007/s11634-013-0136-4,1163
A class of semi-supervised support vector machines by DC programming,"This paper investigate a class of semi-supervised support vector machines (\(\text{ S }^3\mathrm{VMs}\)) with arbitrary norm. A general framework for the \(\text{ S }^3\mathrm{VMs}\) was first constructed based on a robust DC (Difference of Convex functions) program. With different DC decompositions, DC optimization formulations for the linear and nonlinear \(\text{ S }^3\mathrm{VMs}\) are investigated. The resulting DC optimization algorithms (DCA) only require solving simple linear program or convex quadratic program at each iteration, and converge to a critical point after a finite number of iterations. The effectiveness of proposed algorithms are demonstrated on some UCI databases and licorice seed near-infrared spectroscopy data. Moreover, numerical results show that the proposed algorithms offer competitive performances to the existing \(\text{ S }^3\mathrm{VM}\) methods.","Nonconvex optimization, DC programming, Semi-supervised support vector machine",https://link.springer.com//article/10.1007/s11634-013-0141-7,1163
Energy-based function to evaluate data stream clustering,"Severe constraints imposed by the nature of endless sequences of data collected from unstable phenomena have pushed the understanding and the development of automated analysis strategies, such as data clustering techniques. However, current clustering validation approaches are inadequate to data streams due to they do not properly evaluate representation of behavior changes. This paper proposes a novel function to continuously evaluate data stream clustering inspired in Lyapunov energy functions used by techniques such as the Hopfield artificial neural network and the Bidirectional Associative Memory (Bam). The proposed function considers three terms: i) the intra-cluster distance, which allows to evaluate cluster compactness; ii) the inter-cluster distance, which reflects cluster separability; and iii) entropy estimation of the clustering model, which permits the evaluation of the level of uncertainty in data streams. A first set of experiments illustrate the proposed function applied to scenarios of continuous evaluation of data stream clustering. Further experiments were conducted to compare this new function to well-established clustering indices and results confirm our proposal reflects the same information obtained with external clustering indices.","Machine learning, Data streams, Data clustering,  Evaluation of data clustering, Lyapunov energy function",https://link.springer.com//article/10.1007/s11634-013-0145-3,1163
Infinite Dirichlet mixture models learning via expectation propagation,"In this article, we propose a novel Bayesian nonparametric clustering algorithm based on a Dirichlet process mixture of Dirichlet distributions which have been shown to be very flexible for modeling proportional data. The idea is to let the number of mixture components increases as new data to cluster arrive in such a manner that the model selection problem (i.e. determination of the number of clusters) can be answered without recourse to classic selection criteria. Thus, the proposed model can be considered as an infinite Dirichlet mixture model. An expectation propagation inference framework is developed to learn this model by obtaining a full posterior distribution on its parameters. Within this learning framework, the model complexity and all the involved parameters are evaluated simultaneously. To show the practical relevance and efficiency of our model, we perform a detailed analysis using extensive simulations based on both synthetic and real data. In particular, real data are generated from three challenging applications namely images categorization, anomaly intrusion detection and videos summarization.","Clustering, Expectation propagation, Mixture model , Dirichlet process, Images categorization, Anomaly intrusion detection, Videos summarization",https://link.springer.com//article/10.1007/s11634-013-0152-4,1163
On mixtures of skew normal and skew \(t\)-distributions,"Finite mixtures of skew distributions have emerged as an effective tool in modelling heterogeneous data with asymmetric features. With various proposals appearing rapidly in the recent years, which are similar but not identical, the connection between them and their relative performance becomes rather unclear. This paper aims to provide a concise overview of these developments by presenting a systematic classification of the existing skew symmetric distributions into four types, thereby clarifying their close relationships. This also aids in understanding the link between some of the proposed expectation-maximization based algorithms for the computation of the maximum likelihood estimates of the parameters of the models. The final part of this paper presents an illustration of the performance of these mixture models in clustering a real dataset, relative to other non-elliptically contoured clustering methods and associated algorithms for their implementation.","Skew symmetric distributions, Multivariate skew normal , Multivariate skew t-distribution, Mixture models, Maximum likelihood estimation, EM algorithm",https://link.springer.com//article/10.1007/s11634-013-0132-8,1163
A Monte Carlo evaluation of three methods to detect local dependence in binary data latent class models,"Binary data latent class analysis is a form of model-based clustering applied in a wide range of fields. A central assumption of this model is that of conditional independence of responses given latent class membership, often referred to as the “local independence” assumption. The results of latent class analysis may be severely biased when this crucial assumption is violated; investigating the degree to which bivariate relationships between observed variables fit this hypothesis therefore provides vital information. This article evaluates three methods of doing so. The first is the commonly applied method of referring the so-called “bivariate residuals” to a Chi-square distribution. We also introduce two alternative methods that are novel to the investigation of local dependence in latent class analysis: bootstrapping the bivariate residuals, and the asymptotic score test or “modification index”. Our Monte Carlo simulation indicates that the latter two methods perform adequately, while the first method does not perform as intended.","Conditional dependence, Latent variable models, Score test ,  Lagrange multiplier test, Modification index, Bivariate residuals",https://link.springer.com//article/10.1007/s11634-013-0146-2,1163
Model-based clustering of high-dimensional data streams with online mixture of probabilistic PCA,"Model-based clustering is a popular tool which is renowned for its probabilistic foundations and its flexibility. However, model-based clustering techniques usually perform poorly when dealing with high-dimensional data streams, which are nowadays a frequent data type. To overcome this limitation of model-based clustering, we propose an online inference algorithm for the mixture of probabilistic PCA model. The proposed algorithm relies on an EM-based procedure and on a probabilistic and incremental version of PCA. Model selection is also considered in the online setting through parallel computing. Numerical experiments on simulated and real data demonstrate the effectiveness of our approach and compare it to state-of-the-art online EM-based algorithms.","Model-based clustering, Mixture of probabilistic PCA , Data streams, High-dimensional data, Online inference",https://link.springer.com//article/10.1007/s11634-013-0133-7,1163
Model-based clustering of probability density functions,"Complex data such as those where each statistical unit under study is described not by a single observation (or vector variable), but by a unit-specific sample of several or even many observations, are becoming more and more popular. Reducing these sample data by summary statistics, like the average or the median, implies that most inherent information (about variability, skewness or multi-modality) gets lost. Full information is preserved only if each unit is described by a whole distribution. This new kind of data, a.k.a. “distribution-valued data”, require the development of adequate statistical methods. This paper presents a method to group a set of probability density functions (pdfs) into homogeneous clusters, provided that the pdfs have to be estimated nonparametrically from the unit-specific data. Since elements belonging to the same cluster are naturally thought of as samples from the same probability model, the idea is to tackle the clustering problem by defining and estimating a proper mixture model on the space of pdfs. The issue of model building is challenging here because of the infinite-dimensionality and the non-Euclidean geometry of the domain space. By adopting a wavelet-based representation for the elements in the space, the task is accomplished by using mixture models for hyper-spherical data. The proposed solution is illustrated through a simulation experiment and on two real data sets.","Mixture models, Wavelet density estimation, Hyper-spherical data, Von Mises-Fisher distribution",https://link.springer.com//article/10.1007/s11634-013-0140-8,1163
Dimension reduction for model-based clustering via mixtures of multivariate \(t\)-distributions,"We introduce a dimension reduction method for model-based clustering obtained from a finite mixture of \(t\)-distributions. This approach is based on existing work on reducing dimensionality in the case of finite Gaussian mixtures. The method relies on identifying a reduced subspace of the data by considering the extent to which group means and group covariances vary. This subspace contains linear combinations of the original data, which are ordered by importance via the associated eigenvalues. Observations can be projected onto the subspace and the resulting set of variables captures most of the clustering structure available in the data. The approach is illustrated using simulated and real data, where it outperforms its Gaussian analogue.","Dimension reduction, Mixture models, Model-based clustering",https://link.springer.com//article/10.1007/s11634-013-0137-3,1163
Clustering student skill set profiles in a unit hypercube using mixtures of multivariate betas,"This paper presents a finite mixture of multivariate betas as a new model-based clustering method tailored to applications where the feature space is constrained to the unit hypercube. The mixture component densities are taken to be conditionally independent, univariate unimodal beta densities (from the subclass of reparameterized beta densities given by Bagnato and Punzo in Comput Stat 28(4):10.1007/s00180-012-367-4, 2013). The EM algorithm used to fit this mixture is discussed in detail, and results from both this beta mixture model and the more standard Gaussian model-based clustering are presented for simulated skill mastery data from a common cognitive diagnosis model and for real data from the Assistment System online mathematics tutor (Feng et al. in J User Model User Adap Inter 19(3):243–266, 2009). The multivariate beta mixture appears to outperform the standard Gaussian model-based clustering approach, as would be expected on the constrained space. Fewer components are selected (by BIC-ICL) in the beta mixture than in the Gaussian mixture, and the resulting clusters seem more reasonable and interpretable.","Mixture model clustering, Multivariate beta densities , Skill set profiles, Unit hypercube",https://link.springer.com//article/10.1007/s11634-013-0149-z,1163
Interpretable clustering using unsupervised binary trees,"We herein introduce a new method of interpretable clustering that uses unsupervised binary trees. It is a three-stage procedure, the first stage of which entails a series of recursive binary splits to reduce the heterogeneity of the data within the new subsamples. During the second stage (pruning), consideration is given to whether adjacent nodes can be aggregated. Finally, during the third stage (joining), similar clusters are joined together, even if they do not share the same parent originally. Consistency results are obtained, and the procedure is used on simulated and real data sets.","Unsupervised classification, CART, Pattern recognition",https://link.springer.com//article/10.1007/s11634-013-0129-3,1163
Fuzzy spectral clustering by PCCA+: application to Markov state models and data classification,"Given a row-stochastic matrix describing pairwise similarities between data objects, spectral clustering makes use of the eigenvectors of this matrix to perform dimensionality reduction for clustering in fewer dimensions. One example from this class of algorithms is the Robust Perron Cluster Analysis (PCCA+), which delivers a fuzzy clustering. Originally developed for clustering the state space of Markov chains, the method became popular as a versatile tool for general data classification problems. The robustness of PCCA+, however, cannot be explained by previous perturbation results, because the matrices in typical applications do not comply with the two main requirements: reversibility and nearly decomposability. We therefore demonstrate in this paper that PCCA+ always delivers an optimal fuzzy clustering for nearly uncoupled, not necessarily reversible, Markov chains with transition states.","Perron eigenvalues, Perturbation theory, Molecular simulations",https://link.springer.com//article/10.1007/s11634-013-0134-6,1163
A clustering ensemble framework based on elite selection of weighted clusters,"Each clustering algorithm usually optimizes a qualification metric during its progress. The qualification metric in conventional clustering algorithms considers all the features equally important; in other words each feature participates in the clustering process equivalently. It is obvious that some features have more information than others in a dataset. So it is highly likely that some features should have lower importance degrees during a clustering or a classification algorithm; due to their lower information or their higher variances and etc. So it is always a desire for all artificial intelligence communities to enforce the weighting mechanism in any task that identically uses a number of features to make a decision. But there is always a certain problem of how the features can be participated in the clustering process (in any algorithm, but especially in clustering algorithm) in a weighted manner. Recently, this problem is dealt with by locally adaptive clustering (LAC). However, like its traditional competitors the LAC suffers from inefficiency in data with imbalanced clusters. This paper solves the problem by proposing a weighted locally adaptive clustering (WLAC) algorithm that is based on the LAC algorithm. However, WLAC algorithm suffers from sensitivity to its two parameters that should be tuned manually. The performance of WLAC algorithm is affected by well-tuning of its parameters. Paper proposes two solutions. The first is based on a simple clustering ensemble framework to examine the sensitivity of the WLAC algorithm to its manual well-tuning. The second is based on cluster selection method.","Clustering ensemble, Subspace clustering, Weighted clusters,  Features weighting",https://link.springer.com//article/10.1007/s11634-013-0130-x,1163
An isotonic trivariate statistical regression method,"The present research work outlines the main ideas behind statistical regression by a two-independent-variates and one-dependent-variate model based on the invariance of measures in probabilistic spaces. The principle of probabilistic measure invariance, applied under the assumption that the model be isotonic, leads to a system of differential equations. Such differential system is reformulated in terms of an integral equation that affords an iterative numerical solution. Numerical tests performed on the devised statistical regression procedure illustrate its features.","Statistical regression, Dominant independent variates ,  Isotonic regression, Integral equation",https://link.springer.com//article/10.1007/s11634-013-0131-9,1163
Clustering and classification via cluster-weighted factor analyzers,"In model-based clustering and classification, the cluster-weighted model is a convenient approach when the random vector of interest is constituted by a response variable \(Y\) and by a vector \({\varvec{X}}\) of \(p\) covariates. However, its applicability may be limited when \(p\) is high. To overcome this problem, this paper assumes a latent factor structure for \({\varvec{X}}\) in each mixture component, under Gaussian assumptions. This leads to the cluster-weighted factor analyzers (CWFA) model. By imposing constraints on the variance of \(Y\) and the covariance matrix of \({\varvec{X}}\), a novel family of sixteen CWFA models is introduced for model-based clustering and classification. The alternating expectation-conditional maximization algorithm, for maximum likelihood estimation of the parameters of all models in the family, is described; to initialize the algorithm, a 5-step hierarchical procedure is proposed, which uses the nested structures of the models within the family and thus guarantees the natural ranking among the sixteen likelihoods. Artificial and real data show that these models have very good clustering and classification performance and that the algorithm is able to recover the parameters very well.","Cluster-weighted models, Factor analysis, Mixture models, Parsimonious models",https://link.springer.com//article/10.1007/s11634-013-0124-8,1163
Cohen’s weighted kappa with additive weights,"Cohen’s weighted kappa is a popular descriptive statistic for summarizing interrater agreement on an ordinal scale. An agreement table with \(n\in \mathbb N _{\ge 3}\) ordered categories can be collapsed into \(n-1\) distinct \(2\times 2\) tables by combining adjacent categories. Weighted kappa with linear weights is a weighted average of the kappas corresponding to the \(2\times 2\) tables, where the weights are the denominators of the \(2\times 2\) kappas. It is shown that the linearly weighted kappa is a special case of a more general weighted kappa that is a weighted average of the \(2\times 2\) kappas. This weighted kappa has additive weights, that is, given initial weights for pairs of adjacent categories the weight for two non-adjacent categories is obtained by adding the weights of all pairs of adjacent categories between the two.","Cohen’s kappa, Combining categories, Linear weights,  Quadratic weights, \(2\times 2\) Tables, Glasgow outcome scale",https://link.springer.com//article/10.1007/s11634-013-0123-9,1163
Functional fuzzy clusterwise regression analysis,"We propose a functional extension of fuzzy clusterwise regression, which estimates fuzzy memberships of clusters and regression coefficient functions for each cluster simultaneously. The proposed method permits dependent and/or predictor variables to be functional, varying over time, space, and other continua. The fuzzy memberships and clusterwise regression coefficient functions are estimated by minimizing an objective function that adopts a basis function expansion approach to approximating functional data. An alternating least squares algorithm is developed to minimize the objective function. We conduct simulation studies to demonstrate the superior performance of the proposed method compared to its non-functional counterpart and to examine the performance of various cluster validity measures for selecting the optimal number of clusters. We apply the proposed method to real datasets to illustrate the empirical usefulness of the proposed method.","Functional linear models, Fuzzy clusterwise regression model, Alternating least squares algorithm",https://link.springer.com//article/10.1007/s11634-013-0126-6,1163
Random walk distances in data clustering and applications,"In this paper, we develop a family of data clustering algorithms that combine the strengths of existing spectral approaches to clustering with various desirable properties of fuzzy methods. In particular, we show that the developed method “Fuzzy-RW,” outperforms other frequently used algorithms in data sets with different geometries. As applications, we discuss data clustering of biological and face recognition benchmarks such as the IRIS and YALE face data sets.","Spectral clustering, Fuzzy clustering methods, Random walks,  Graph Laplacian, Mahalanobis, Face identification",https://link.springer.com//article/10.1007/s11634-013-0125-7,1163
Regularized logistic discrimination with basis expansions for the early detection of Alzheimer’s disease based on three-dimensional MRI data,"In recent years, evidence has emerged indicating that magnetic resonance imaging (MRI) brain scans provide valuable diagnostic information about Alzheimer’s disease. It has been shown that MRI brain scans are capable of both diagnosing Alzheimer’s disease itself at an early stage and identifying people at risk of developing Alzheimer’s. In this article, we have investigated statistical methods for classifying Alzheimer’s disease patients based on three-dimensional MRI data via L2-type regularized logistic discrimination with basis expansions. Preceding studies adopted an open approach when applying three-dimensional data analysis. Our proposed classification model with dimension reduction techniques offers discriminant functions with excellent prediction performance in terms of sensitivity and specificity.","Regularized logistic discrimination, Three-dimensional MRI data, Alzheimer’s disease, Dimension reduction, Tuning parameter selection",https://link.springer.com//article/10.1007/s11634-013-0127-5,1163
Image data analysis and classification in marketing,"Nowadays, the diffusion of smartphones, tablet computers, and other multipurpose equipment with high-speed Internet access makes new data types available for data analysis and classification in marketing. So, e.g., it is now possible to collect images/snaps, music, or videos instead of ratings. With appropriate algorithms and software at hand, a marketing researcher could simply group or classify respondents according to the content of uploaded images/snaps, music, or videos. However, appropriate algorithms and software are sparsely known in marketing research up to now. The paper tries to close this gap. Algorithms and software from computer science are presented, adapted and applied to data analysis and classification in marketing. The new SPSS-like software package IMADAC is introduced.","Image data analysis, Image classification, Market segmentation",https://link.springer.com//article/10.1007/s11634-012-0116-0,1163
Non parametric statistical models for on-line text classification,"Social media, such as blogs and on-line forums, contain a huge amount of information that is typically unorganized and fragmented. An important issue, that has been raising importance so far, is to classify on-line texts in order to detect possible anomalies. For example on-line texts representing consumer opinions can be, not only very precious and profitable for companies, but can also represent a serious damage if they are negative or faked. In this contribution we present a novel statistical methodology rooted in the context of classical text classification, in order to address such issues. In the literature, several classifiers have been proposed, among them support vector machine and naive Bayes classifiers. These approaches are not effective when coping with the problem of classifying texts belonging to an unknown author. To this aim, we propose to employ a new method, based on the combination of classification trees with non parametric approaches, such as Kruskal–Wallis and Brunner–Dette–Munk test. The main application of what we propose is the capability to classify an author as a new one, that is potentially trustable, or as an old one, that is potentially faked.","Non parametric statistical models, Kruskal–Wallis test,  Brunner–Dette–Munk test, Text analysis, Opinion spam detection",https://link.springer.com//article/10.1007/s11634-012-0122-2,1163
A note on consistency improvements of AHP paired comparison data,"The Analytic Hierarchy Process (AHP) is a popular multicriteria decision-making approach but the ease of AHP paired comparison data collection entails the problem that consistency restrictions have to be fulfilled for the data evaluation task. Quite a lot of consistency improvement techniques are available, however, this note explains why consistency adjustments are not necessarily helpful for computing acceptable weights for the determination of the underlying overall objective function.","Analytic Hierarchy Process, Consistency improvement ,  Multicriteria decision-making, Paired comparison data",https://link.springer.com//article/10.1007/s11634-012-0119-x,1163
Sensory analysis in the food industry as a tool for marketing decisions,"In the food industry, sensory analysis can be useful to direct marketing decisions concerning not only products, for example product positioning with respect to competitors, but also market segmentation, customer relationship management, advertising strategies and price policies. In this paper we show how interesting information useful for marketing management can be obtained by combining the results from cub models and algorithmic data mining techniques (specifically, variable importance measurements from Random Forest). A case study on sensory evaluation of different varieties of Italian espresso is presented.","Sensory analysis, Ordinal data, cub models, Italian coffee",https://link.springer.com//article/10.1007/s11634-012-0120-4,1163
Banking customer satisfaction evaluation: a three-way factor perspective,"As management of a national bank wanted to analyze its retail service competition loss probably due to low customer satisfaction, we carried out an empirical study based on a sample of 27,000 retail customers. The survey aimed to analyze retail service weaknesses and to individuate possible recovery actions measuring their effectiveness across different waves (three time lags). We studied a definition of a new dissimilarity measure exploiting a dimension reduction obtained by a three-way factor analysis (TFA). We had previously focused our attention on the limits of this approach related to the geometrical properties of the TFA applied. We introduced a reassessment of the points to adjust the three-way solution according to the quality of representation of the points. This transformation only rescaled the factor scores producing a local adjustment of the point configuration. We then performed a trajectory analysis of the different waves. The results showed the effectiveness of our approach. Therefore, further study of the derivation of a synthetic measure of cluster routes seems appropriate.","Dynamic principal component, Customer satisfaction ,  Three-way factor analysis, Trajectories analysis, Banking sector",https://link.springer.com//article/10.1007/s11634-012-0118-y,1163
Predicting partial customer churn using Markov for discrimination for modeling first purchase sequences,"Currently, in order to remain competitive companies are adopting customer centered strategies and consequently customer relationship management is gaining increasing importance. In this context, customer retention deserves particular attention. This paper proposes a model for partial churn detection in the retail grocery sector that includes as a predictor the similarity of the products’ first purchase sequence with churner and non-churner sequences. The sequence of first purchase events is modeled using Markov for discrimination. Two classification techniques are used in the empirical study: logistic regression and random forests. A real sample of approximately 95,000 new customers is analyzed taken from the data warehouse of a European retailing company. The empirical results reveal the relevance of the inclusion of a products’ sequence likelihood in partial churn prediction models, as well as the supremacy of logistic regression when compared with random forests.","Customer relationship management, Churn analysis, Retailing, Classification, Logistic regression, Random forests",https://link.springer.com//article/10.1007/s11634-012-0121-3,1163
Analyzing consumers’ shopping behavior using RFID data and pattern mining,"The development of sensor networks has enabled detailed tracking of customer behavior in stores. Shopping path data which records each customer’s position and time information is attracting attention as new marketing data. However, there are no proposed marketing models which can identify good customers from huge amounts of time series data on customer movement in the store. This research aims to use shopping path data resulting from tracking customer behavior in the store, using information on the sequence of visiting each product zone in the store and staying time at each product zone, to find how they affect purchasing. To discover useful knowledge for store management, shopping paths data has been transformed into sequence data including information on visit sequence and staying times in the store, and LCMseq has been applied to them to extract frequent sequence patterns. In this paper, we find characteristic in-store behavior patterns of good customers by using actual data of a Japanese supermarket.","Sequential pattern mining, Path data, LCM sequence , Decision tree, Data mining",https://link.springer.com//article/10.1007/s11634-012-0117-z,1163
Preliminary estimators for a mixture model of ordinal data,"In this paper, we propose preliminary estimators for the parameters of a mixture distribution introduced for the analysis of ordinal data where the mixture components are given by a Combination of a discrete Uniform and a shifted Binomial distribution (cub model). After reviewing some preliminary concepts related to the meaning of parameters which characterize such models, we introduce estimators which are related to the location and heterogeneity of the observed distributions, respectively, in order to accelerate the EM procedure for the maximum likelihood estimation. A simulation experiment has been performed to investigate their main features and to confirm their usefulness. A check of the proposal on real case studies and some comments conclude the paper.","Ordinal data, cub models, Preliminary estimators",https://link.springer.com//article/10.1007/s11634-012-0111-5,1163
Time series classification by class-specific Mahalanobis distance measures,"To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately—for time series data—the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the diagonal approach.","Time-series classification, Distance measure learning, Nearest Neighbor, Mahalanobis distance measure",https://link.springer.com//article/10.1007/s11634-012-0110-6,1163
Wavelet-RKHS-based functional statistical classification,"A functional classification methodology, based on the Reproducing Kernel Hilbert Space (RKHS) theory, is proposed for discrimination of gene expression profiles. The parameter function involved in the definition of the functional logistic regression is univocally and consistently estimated, from the minimization of the penalized negative log-likelihood over a RKHS generated by a suitable wavelet basis. An iterative descendent method, the gradient method, is applied for solving the corresponding minimization problem, i.e., for computing the functional estimate. Temporal gene expression data involved in the yeast cell cycle are classified with the wavelet-RKHS-based discrimination methodology considered. A simulation study is developed for testing the performance of this statistical classification methodology in comparison with other statistical discrimination procedures.","Functional data analysis, Gene expression profiles, Penalized logistic regression, Reproducing kernel Hilbert space, Wavelet decomposition, Yeast cell cycle",https://link.springer.com//article/10.1007/s11634-012-0112-4,1163
Clustering of functional data in a low-dimensional subspace,"To find optimal clusters of functional objects in a lower-dimensional subspace of data, a sequential method called tandem analysis, is often used, though such a method is problematic. A new procedure is developed to find optimal clusters of functional objects and also find an optimal subspace for clustering, simultaneously. The method is based on the k-means criterion for functional data and seeks the subspace that is maximally informative about the clustering structure in the data. An efficient alternating least-squares algorithm is described, and the proposed method is extended to a regularized method. Analyses of artificial and real data examples demonstrate that the proposed method gives correct and interpretable results.","Functional data, Clustering, Low-dimensional space, Dimension reduction, Smoothing",https://link.springer.com//article/10.1007/s11634-012-0113-3,1163
Dissimilarity and similarity measures for comparing dendrograms and their applications,"In this paper we propose a new index Z for measuring the dissimilarity between two hierarchical clusterings (or dendrograms). This index is a metric since it satisfies the axioms of non-negativity, symmetry and triangle inequality. A desirable property of this index is that it can be decomposed into the contributions pertaining to each stage of the hierarchies. We show the relations of such components with the currently used criteria for comparing two partitions. We obtain a global similarity index as the complement to one of the suggested dissimilarity and we derive its adjustment for agreement due to chance. We obtain similarity indexes pertaining to each stage of the hierarchies as the complement to one of the additive parts of the global distance Z. We consider the use of the proposed distance for more than two dendrograms and its use for the consensus of classifications and variable selection in cluster analysis. A series of simulation experiments and an application to a real data set are presented.","Cluster analysis, Consensus of classifications, Distance, Hierarchical trees, L1 norm, Partitions, Similarity of dendrograms, Variable selection",https://link.springer.com//article/10.1007/s11634-012-0106-2,1163
The influence function of the TCLUST robust clustering procedure,"The TCLUST procedure performs robust clustering with the aim of finding clusters with different scatter structures and weights. An Eigenvalues Ratio constraint is considered by TCLUST in order to achieve a wide range of clustering alternatives depending on the allowed differences among cluster scatter matrices. Moreover, this constraint avoids finding uninteresting spurious clusters. In order to guarantee the robustness of the method against the presence of outliers and background noise, the method allows for trimming of a given proportion of observations self-determined by the data. Based on this “impartial trimming”, the procedure is assumed to have good robustness properties. As it was done for the trimmed k-means method, this article studies robustness properties of the TCLUST procedure in the univariate case with two clusters by means of the influence function. The conclusion is that the TCLUST has a robustness behavior close to that of the trimmed k-means in spite of the fact that it addresses a more general clustering approach.","Heterogeneous clustering, Influence function, Trimming, Robustness",https://link.springer.com//article/10.1007/s11634-012-0107-1,1163
Orthogonal rotation in PCAMIX,"Kiers (Psychometrika 56:197–212, 1991) considered the orthogonal rotation in PCAMIX, a principal component method for a mixture of qualitative and quantitative variables. PCAMIX includes the ordinary principal component analysis and multiple correspondence analysis (MCA) as special cases. In this paper, we give a new presentation of PCAMIX where the principal components and the squared loadings are obtained from a Singular Value Decomposition. The loadings of the quantitative variables and the principal coordinates of the categories of the qualitative variables are also obtained directly. In this context, we propose a computationally efficient procedure for varimax rotation in PCAMIX and a direct solution for the optimal angle of rotation. A simulation study shows the good computational behavior of the proposed algorithm. An application on a real data set illustrates the interest of using rotation in MCA. All source codes are available in the R package “PCAmixdata”.","Mixture of qualitative and quantitative data, Principal component analysis, Multiple correspondence analysis, Rotation",https://link.springer.com//article/10.1007/s11634-012-0105-3,1163
Adaptation of interval PCA to symbolic histogram variables,"This paper is an adaptation of symbolic interval Principal Component Analysis (PCA) to histogram data. We proposed two methodologies. The first one involved three steps: the coding of bins of histogram, the ordinary PCA of means of variables and the representation of dispersion of symbolic observations we call concepts. For the representation of dispersion of these concepts we proposed the transformation of histograms into intervals. Then, we suggest the projection of the hypercubes or the interval lengths associated to each concept on the principal axes of the ordinary PCA of means. In the second methodology, we proposed the use of the three previous steps with the angular transformation.","Symbolic histogram variable, Hypercube, Tchebychev inequality, Angular transformation, Procrustean analysis",https://link.springer.com//article/10.1007/s11634-012-0108-0,1163
A latent variables approach for clustering mixed binary and continuous variables within a Gaussian mixture model,"For clustering objects, we often collect not only continuous variables, but binary attributes as well. This paper proposes a model-based clustering approach with mixed binary and continuous variables where each binary attribute is generated by a latent continuous variable that is dichotomized with a suitable threshold value, and where the scores of the latent variables are estimated from the binary data. In economics, such variables are called utility functions and the assumption is that the binary attributes (the presence or the absence of a public service or utility) are determined by low and high values of these functions. In genetics, the latent response is interpreted as the ‘liability’ to develop a qualitative trait or phenotype. The estimated scores of the latent variables, together with the observed continuous ones, allow to use a multivariate Gaussian mixture model for clustering, instead of using a mixture of discrete and continuous distributions. After describing the method, this paper presents the results of both simulated and real-case data and compares the performances of the multivariate Gaussian mixture model and of a mixture of joint multivariate and multinomial distributions. Results show that the former model outperforms the mixture model for variables with different scales, both in terms of classification error rate and reproduction of the clusters means.","Clustering, E-government, Information and communication technologies, Latent variables, Mixed mode data, Scores estimate",https://link.springer.com//article/10.1007/s11634-011-0101-z,1163
Exploring incomplete data using visualization techniques,"Visualization of incomplete data allows to simultaneously explore the data and the structure of missing values. This is helpful for learning about the distribution of the incomplete information in the data, and to identify possible structures of the missing values and their relation to the available information. The main goal of this contribution is to stress the importance of exploring missing values using visualization methods and to present a collection of such visualization techniques for incomplete data, all of which are implemented in the \({{\sf R}}\) package VIM. Providing such functionality for this widely used statistical environment, visualization of missing values, imputation and data analysis can all be done from within \({{\sf R}}\) without the need of additional software.","Visualization, Missing values, Exploring incomplete data, \({{\sf R}}\) software",https://link.springer.com//article/10.1007/s11634-011-0102-y,1163
Analyzing multiset data by the Power STATIS-ACT method,"The STATIS-ACT method is a generalization of principal component analysis used to study simultaneously several data tables measured on the same observation units or variables. The goal of this method is to analyze the relationship between these data tables and to combine them into a compromise matrix corresponding to an optimal agreement between the data. In this paper, we propose a new approach to this method, referred to as the Power STATIS-ACT method, where the compromise matrix is derived from a general s-power based criterion \({(s\geqslant 1)}\) and investigate some of its theoretical and practical properties. Special attention is devoted to the 1-power case which makes the introduction of low rank versions of the compromise possible. We also examine the effect of varying the power parameter s on the compromise solutions. All results are illustrated with a number of real data tables.","STATIS-ACT method, Power based criterion, Multiset data, Three-way data, RV-coefficient",https://link.springer.com//article/10.1007/s11634-011-0085-8,1163
Cohen’s linearly weighted kappa is a weighted average,"An n × n agreement table F = {fij} with n ≥ 3 ordered categories can for fixed m (2 ≤ m ≤ n − 1) be collapsed into \({\binom{n-1}{m-1}}\) distinct m × m tables by combining adjacent categories. It is shown that the components (observed and expected agreement) of Cohen’s weighted kappa with linear weights can be obtained from the m × m subtables. A consequence is that weighted kappa with linear weights can be interpreted as a weighted average of the linearly weighted kappas corresponding to the m × m tables, where the weights are the denominators of the kappas. Moreover, weighted kappa with linear weights can be interpreted as a weighted average of the linearly weighted kappas corresponding to all nontrivial subtables.","Cohen’s kappa, Inter-rater agreement, Merging categories, Linear weights, Quadratic weights, Subtables",https://link.springer.com//article/10.1007/s11634-011-0094-7,1163
Panel data analysis: a survey on model-based clustering of time series,Clustering is a widely used statistical tool to determine subsets in a given data set. Frequently used clustering methods are mostly based on distance measures and cannot easily be extended to cluster time series within a panel or a longitudinal data set. The paper reviews recently suggested approaches to model-based clustering of panel or longitudinal data based on finite mixture models. Several approaches are considered that are suitable both for continuous and for categorical time series observations. Bayesian estimation through Markov chain Monte Carlo methods is described in detail and various criteria to select the number of clusters are reviewed. An application to a panel of marijuana use among teenagers serves as an illustration.,"Markov chain Monte Carlo, Longitudinal data, Panel data, Model selection, Discrete-valued time series, Regularization, Skewed distributions",https://link.springer.com//article/10.1007/s11634-011-0100-0,1163
Model-based clustering of time series in group-specific functional subspaces,"This work develops a general procedure for clustering functional data which adapts the clustering method high dimensional data clustering (HDDC), originally proposed in the multivariate context. The resulting clustering method, called funHDDC, is based on a functional latent mixture model which fits the functional data in group-specific functional subspaces. By constraining model parameters within and between groups, a family of parsimonious models is exhibited which allow to fit onto various situations. An estimation procedure based on the EM algorithm is proposed for determining both the model parameters and the group-specific functional subspaces. Experiments on real-world datasets show that the proposed approach performs better or similarly than classical two-step clustering methods while providing useful interpretations of the groups and avoiding the uneasy choice of the discretization technique. In particular, funHDDC appears to always outperform HDDC applied on spline coefficients.","Functional data, Time series clustering, Model-based clustering, Group-specific functional subspaces, Functional PCA",https://link.springer.com//article/10.1007/s11634-011-0095-6,1163
Model-based clustering and segmentation of time series with changes in regime,"Mixture model-based clustering, usually applied to multidimensional data, has become a popular approach in many data analysis problems, both for its good statistical properties and for the simplicity of implementation of the Expectation–Maximization (EM) algorithm. Within the context of a railway application, this paper introduces a novel mixture model for dealing with time series that are subject to changes in regime. The proposed approach, called ClustSeg, consists in modeling each cluster by a regression model in which the polynomial coefficients vary according to a discrete hidden process. In particular, this approach makes use of logistic functions to model the (smooth or abrupt) transitions between regimes. The model parameters are estimated by the maximum likelihood method solved by an EM algorithm. This approach can also be regarded as a clustering approach which operates by finding groups of time series having common changes in regime. In addition to providing a time series partition, it therefore provides a time series segmentation. The problem of selecting the optimal numbers of clusters and segments is solved by means of the Bayesian Information Criterion. The ClustSeg approach is shown to be efficient using a variety of simulated time series and real-world time series of electrical power consumption from rail switching operations.","Clustering, Time series, Change in regime, Mixture model, Regression mixture, Hidden logistic process, EM algorithm",https://link.springer.com//article/10.1007/s11634-011-0096-5,1163
A tail dependence-based dissimilarity measure for financial time series clustering,"In this paper we propose a clustering procedure aimed at grouping time series with an association between extremely low values, measured by the lower tail dependence coefficient. Firstly, we estimate the coefficient using an Archimedean copula function. Then, we propose a dissimilarity measure based on tail dependence coefficients and a two-step procedure to be used with clustering algorithms which require that the objects we want to cluster have a geometric interpretation. We show how the results of the clustering applied to financial returns could be used to construct defensive portfolios reducing the effect of a simultaneous financial crisis.","Time series, Clustering, Tail dependence, Copula function",https://link.springer.com//article/10.1007/s11634-011-0098-3,1163
Frame potential minimization for clustering short time series,"The short time series expression miner by Ernst et al. (Bioinformatics 21:i159–i168, 2005) assigns time series data to the closest of suitably selected prototypes followed by the selection of significant clusters and eventual grouping. We prove that the proposed dissimilarity measure 1 − ρ, with correlation coefficient ρ, can be interpreted as the distance of projected data onto the (d − 1)-dimensional unit sphere \({\mathcal{S}^{d-1}}\). The choice of prototypes is closely related to classical problems in optimization theory. Moreover, we propose a new functional, which has a data-driven component and connects the choice of prototypes to the theory of finite unit norm tight frames by Benedetto and Fickus (Adv Comput Math 18:357–385, 2003).","Clustering, Finite unit norm tight frames, Frame potential, Gene expression data, Short time series, STEM",https://link.springer.com//article/10.1007/s11634-011-0097-4,1163
Linear dimension reduction in classification: adaptive procedure for optimum results,"Linear dimension reduction plays an important role in classification problems. A variety of techniques have been developed for linear dimension reduction to be applied prior to classification. However, there is no single definitive method that works best under all circumstances. Rather a best method depends on various data characteristics. We develop a two-step adaptive procedure in which a best dimension reduction method is first selected based on the various data characteristics, which is then applied to the data at hand. It is shown using both simulated and real life data that such a procedure can significantly reduce the misclassification rate.","Dimension reduction, Adaptive procedure, Classification, Meta-learning",https://link.springer.com//article/10.1007/s11634-011-0091-x,1163
Correcting Jaccard and other similarity indices for chance agreement in cluster analysis,"Correcting a similarity index for chance agreement requires computing its expectation under fixed marginal totals of a matching counts matrix. For some indices, such as Jaccard, Rogers and Tanimoto, Sokal and Sneath, and Gower and Legendre the expectations cannot be easily found. We show how such similarity indices can be expressed as functions of other indices and expectations found by approximations such that approximate correction is possible. A second approach is based on Taylor series expansion. A simulation study illustrates the effectiveness of the resulting correction of similarity indices using structured and unstructured data generated from bivariate normal distributions.","Similarity indices, Matching counts matrix, Correction for chance agreement, Jaccard index, Cluster analysis, Comparing partitions",https://link.springer.com//article/10.1007/s11634-011-0090-y,1163
Consensus of partitions : a constructive approach,"Given a profile (family) Π of partitions of a set of objects or items X, we try to establish a consensus partition containing a maximum number of joined or separated pairs in X that are also joined or separated in the profile. To do so, we define a score function, SΠ associated to any partition on X. Consensus partitions for Π are those maximizing this function. Therefore, these consensus partitions have the median property for the profile and the symmetric difference distance. This optimization problem can be solved, in certain cases, by integer linear programming. We define a polynomial heuristic which can be applied to partitions on a large set of items. In cases where an optimal solution can be computed, we show that the partitions built by this algorithm are very close to the optimum which is reached in practically all the cases, except for some sets of bipartitions.","Partitions, Consensus, Transfer distance, Simulation",https://link.springer.com//article/10.1007/s11634-011-0087-6,1163
Multiple imputation in principal component analysis,"The available methods to handle missing values in principal component analysis only provide point estimates of the parameters (axes and components) and estimates of the missing values. To take into account the variability due to missing values a multiple imputation method is proposed. First a method to generate multiple imputed data sets from a principal component analysis model is defined. Then, two ways to visualize the uncertainty due to missing values onto the principal component analysis results are described. The first one consists in projecting the imputed data sets onto a reference configuration as supplementary elements to assess the stability of the individuals (respectively of the variables). The second one consists in performing a principal component analysis on each imputed data set and fitting each obtained configuration onto the reference one with Procrustes rotation. The latter strategy allows to assess the variability of the principal component analysis parameters induced by the missing values. The methodology is then evaluated from a real data set.","Principal component analysis, Missing values, EM algorithm, Multiple imputation, Bootstrap, Procrustes rotation",https://link.springer.com//article/10.1007/s11634-011-0086-7,1163
Network ensemble clustering using latent roles,"We present a clustering method for collections of graphs based on the assumptions that graphs in the same cluster have a similar role structure and that the respective roles can be founded on implicit vertex types. Given a network ensemble (a collection of attributed graphs with some substantive commonality), we start by partitioning the set of all vertices based on attribute similarity. Projection of each graph onto the resulting vertex types yields feature vectors of equal dimensionality, irrespective of the original graph sizes. These feature vectors are then subjected to standard clustering methods. This approach is motivated by social network concepts, and we demonstrate its utility on an ensemble of personal networks of migrants, where we extract structurally similar groups and show their resemblance to predicted acculturation strategies.","Social network analysis, Clustering, Network ensembles, Acculturation",https://link.springer.com//article/10.1007/s11634-010-0074-3,1163
On the use of external information in social network analysis,"Network analysis focuses on links among interacting units (actors). Interactions are often derived from the presence of actors at events or activities (two-mode network) and this information is coded and arranged in a typical affiliation matrix. In addition to the relational data, interest may focus on external information gathered on both actors and events. Our aim is to explore the effect of external information on the formation of ties by setting a strategy able to decompose the original affiliation matrix by linear combinations of data vectors representing external information with a suitable matrix of coefficients. This allows to obtain peculiar relational data matrices that include the effect of external information. The derived adjacency matrices can then be analyzed from the network analysis perspective. In particular, we look for groups of structurally equivalent actors obtained through clustering methods. Illustrative examples and a real dataset in the framework of scientific collaboration will give a major insight into the proposed strategy.","Attribute data, Clustering, Positional analysis, Scientific collaboration network, Two-mode network",https://link.springer.com//article/10.1007/s11634-010-0080-5,1163
Web page importance ranking,"An approach is proposed that uses a set of interesting Web pages as starting point for a minimum walk algorithm to provide recommendations of additionally important Web information within a m-clicks-ahead situation. A discussion of known page importance ranking techniques as well as examples of the application of the new algorithm show that Web link structure dependent approaches should be enriched by considerations as to how the analysis of additional data and the use of suited support tools can be incorporated. These considerations include aspects as, e.g., personalization, query dependence and topic sensitivity of the underlying pages, the dynamic nature of the Web, as well as the possibility to perform calculations online.","Page ranking, Web link structure analysis, Content similarity of Web pages, m-Clicks-ahead recommendations",https://link.springer.com//article/10.1007/s11634-011-0088-5,1163
Fast algorithms for determining (generalized) core groups in social networks,"The structure of a large network (graph) can often be revealed by partitioning it into smaller and possibly more dense sub-networks that are easier to handle. One of such decompositions is based on “k-cores”, proposed in 1983 by Seidman. Together with connectivity components, cores are one among few concepts that provide efficient decompositions of large graphs and networks. In this paper we propose an efficient algorithm for determining the cores decomposition of a given network with complexity \({\mathcal{O}(m)}\), where m is the number of lines (edges or arcs). In the second part of the paper the classical concept of k-core is generalized in a way that uses a vertex property function instead of degree of a vertex. For local monotone vertex property functions the corresponding generalized cores can be determined in \({\mathcal{O}(m\cdot\max(\Delta,\log{n}))}\) time, where n is the number of vertices and Δ is the maximum degree. Finally the proposed algorithms are illustrated by the analysis of a collaboration network in the field of computational geometry.","Core, Large network, Decomposition, Graph algorithm",https://link.springer.com//article/10.1007/s11634-010-0079-y,1163
Assessing and accounting for time heterogeneity in stochastic actor oriented models,"This paper explores time heterogeneity in stochastic actor oriented models (SAOM) proposed by Snijders (Sociological methodology. Blackwell, Boston, pp 361–395, 2001) which are meant to study the evolution of networks. SAOMs model social networks as directed graphs with nodes representing people, organizations, etc., and dichotomous relations representing underlying relationships of friendship, advice, etc. We illustrate several reasons why heterogeneity should be statistically tested and provide a fast, convenient method for assessment and model correction. SAOMs provide a flexible framework for network dynamics which allow a researcher to test selection, influence, behavioral, and structural properties in network data over time. We show how the forward-selecting, score type test proposed by Schweinberger (Chapter 4: Statistical modeling of network panel data: goodness of fit. PhD thesis, University of Groningen 2007) can be employed to quickly assess heterogeneity at almost no additional computational cost. One step estimates are used to assess the magnitude of the heterogeneity. Simulation studies are conducted to support the validity of this approach. The ASSIST dataset (Campbell et al. In Lancet 371(9624):1595–1602, 2008) is reanalyzed with the score type test, one step estimators, and a full estimation for illustration. These tools are implemented in the RSiena package, and a brief walkthrough is provided.","Stochastic actor oriented models, Longitudinal analysis of network data, Time heterogeneity, Score-type test",https://link.springer.com//article/10.1007/s11634-010-0076-1,1163
Robust estimation of efficient mean–variance frontiers,"Standard methods for optimal allocation of shares in a financial portfolio are determined by second-order conditions which are very sensitive to outliers. The well-known Markowitz approach, which is based on the input of a mean vector and a covariance matrix, seems to provide questionable results in financial management, since small changes of inputs might lead to irrelevant portfolio allocations. However, existing robust estimators often suffer from masking of multiple influential observations, so we propose a new robust estimator which suitably weights data using a forward search approach. A Monte Carlo simulation study and an application to real data show some advantages of the proposed approach.","Efficient frontiers, Forward search, Multivariate outlier detection, Portfolio allocation",https://link.springer.com//article/10.1007/s11634-010-0082-3,1163
Constrained principal component analysis of standardized data for biplots with unit-length variable vectors,"Principal component analysis (PCA) of an objects ×  variables data matrix is used for obtaining a low-dimensional biplot configuration, where data are approximated by the inner products of the vectors corresponding to objects and variables. Borg and Groenen (Modern multidimensional scaling. Springer, New York, 1997) have suggested another biplot procedure which uses a technique for approximating data by projections of object vectors on variable vectors. This technique is formulated as constraining the variable vectors in PCA to be of unit length and can be called unit-length vector analysis (UVA). However, an algorithm for UVA has not yet been developed. In this paper, we present such an algorithm, discuss the properties of UVA solutions, and demonstrate the advantage of UVA in biplots for standardized data with homogeneous variances among variables. The advantage of UVA-based biplots is that the projections of object vectors onto variable vectors express the approximation of data in an easy way, while in PCA-based biplots we must consider not only the projections, but also the lengths of variable vectors in order to visualize approximations.","Biplots, Unit-length vector analysis, Principal component analysis, Inner product, Projection",https://link.springer.com//article/10.1007/s11634-010-0081-4,1163
Detection of multivariate outliers in business survey data with incomplete information,"Many different methods for statistical data editing can be found in the literature but only few of them are based on robust estimates (for example such as BACON-EEM, epidemic algorithms (EA) and transformed rank correlation (TRC) methods of Béguin and Hulliger). However, we can show that outlier detection is only reasonable if robust methods are applied, because the classical estimates are themselves influenced by the outliers. Nevertheless, data editing is essential to check the multivariate data for possible data problems and it is not deterministic like the traditional micro editing where all records are extensively edited manually using certain rules/constraints. The presence of missing values is more a rule than an exception in business surveys and poses additional severe challenges to the outlier detection. First we review the available multivariate outlier detection methods which can cope with incomplete data. In a simulation study, where a subset of the Austrian Structural Business Statistics is simulated, we compare several approaches. Robust methods based on the Minimum Covariance Determinant (MCD) estimator, S-estimators and OGK-estimator as well as BACON-BEM provide the best results in finding the outliers and in providing a low false discovery rate. Many of the discussed methods are implemented in the R package \({\tt{rrcovNA}}\) which is available from the Comprehensive R Archive Network (CRAN) at http://www.CRAN.R-project.org under the GNU General Public License.","Multivariate outlier detection, Robust statistics, Missing values",https://link.springer.com//article/10.1007/s11634-010-0075-2,1163
Generalized GIPSCAL re-revisited: a fast convergent algorithm with acceleration by the minimal polynomial extrapolation,"Generalized GIPSCAL, like DEDICOM, is a model for the analysis of square asymmetric tables. It is a special case of DEDICOM, but unlike DEDICOM, it ensures the nonnegative definiteness (nnd) of the model matrix, thereby allowing a spatial representation of the asymmetric relationships among “objects”. A fast convergent algorithm was developed for GIPSCAL with acceleration by the minimal polynomial extrapolation. The proposed algorithm was compared with Trendafilov’s algorithm in computational speed. The basic algorithm has been adapted to various extensions of GIPSCAL, including off-diagonal DEDICOM/GIPSCAL, and three-way GIPSCAL.","Asymmetric square tables, DEDICOM, Singular value decomposition (SVD), Dynamical system algorithm, Diagonal estimation, Three-way data",https://link.springer.com//article/10.1007/s11634-010-0083-2,1163
A general approach to handling missing values in Procrustes analysis,"General Procrustes analysis is concerned with transforming a set of given configuration matrices to closest agreement. This paper introduces an approach useful for handling missing values in the configuration matrices in the context of general linear transformations. Centring and/or standardisation are allowed. Simplifications occur in the important case where the transformations are orthogonal. In the most general case, an interesting quadratic constrained optimisation problem appears.","Procrustes analysis, Missing value estimation, Constrained optimisation",https://link.springer.com//article/10.1007/s11634-010-0077-0,1163
Robust classification for skewed data,"In this paper we propose a robust classification rule for skewed unimodal distributions. For low dimensional data, the classifier is based on minimizing the adjusted outlyingness to each group. In the case of high dimensional data, the robustified SIMCA method is adjusted for skewness. The robustness of the methods is investigated through different simulations and by applying it to some datasets.","Robustness, Classification, Skewness",https://link.springer.com//article/10.1007/s11634-010-0066-3,1163
Fuzzy clusterwise quasi-likelihood generalized linear models,"The quasi-likelihood method has emerged as a useful approach to the parameter estimation of generalized linear models (GLM) in circumstances where there is insufficient distributional information to construct a likelihood function. Despite its flexibility, the quasi-likelihood approach to GLM is currently designed for an aggregate-sample analysis based on the assumption that the entire sample of observations is taken from a single homogenous population. Thus, this approach may not be suitable when heterogeneous subgroups exist in the population, which involve qualitatively distinct effects of covariates on the response variable. In this paper, the quasi-likelihood GLM approach is generalized to a fuzzy clustering framework which explicitly accounts for such cluster-level heterogeneity. A simple iterative estimation algorithm is presented to optimize the regularized fuzzy clustering criterion of the proposed method. The performance of the proposed method in recovering parameters is investigated based on a Monte Carlo analysis involving synthetic data. Finally, the empirical usefulness of the proposed method is illustrated through an application to actual data on the coupon usage behaviour of a sample of consumers.","Generalized linear models, Quasi-likelihood, Fuzzy clustering, Regularization by entropy, Cluster-level heterogeneity, Coupon redemption",https://link.springer.com//article/10.1007/s11634-010-0069-0,1163
Inequalities between multi-rater kappas,"The paper presents inequalities between four descriptive statistics that have been used to measure the nominal agreement between two or more raters. Each of the four statistics is a function of the pairwise information. Light’s kappa and Hubert’s kappa are multi-rater versions of Cohen’s kappa. Fleiss’ kappa is a multi-rater extension of Scott’s pi, whereas Randolph’s kappa generalizes Bennett et al. S to multiple raters. While a consistent ordering between the numerical values of these agreement measures has frequently been observed in practice, there is thus far no theoretical proof of a general ordering inequality among these measures. It is proved that Fleiss’ kappa is a lower bound of Hubert’s kappa and Randolph’s kappa, and that Randolph’s kappa is an upper bound of Hubert’s kappa and Light’s kappa if all pairwise agreement tables are weakly marginal symmetric or if all raters assign a certain minimum proportion of the objects to a specified category.","Nominal agreement, Cohen’s kappa, Scott’s pi, Light’s kappa, Hubert’s kappa, Fleiss’ kappa, Randolph’s kappa, Cauchy–Schwarz inequality, Arithmetic-harmonic means inequality",https://link.springer.com//article/10.1007/s11634-010-0073-4,1163
Detecting atypical observations in financial data: the forward search for elliptical copulas,"In the last few years, copulas have been widely applied in many field of studies. Concentrating our attention on financial applications, we pursue the goal to detect multivariate atypical observations by extending to elliptical copulas the forward search originally introduced in linear and nonlinear regression by Atkinson and Riani (Robust diagnostic regression analysis. Springer, New York, 2000). Considering that, in the forward search, observations are ranked according to their closeness to the fitted data, we need to define a measure through which to initialize, progress and monitor the search. We achieve this goal building up the forward search for elliptical copulas relying on the squared Mahalanobis distance. Stressing the need to find theoretical boundaries for the inference on outliers, we introduce a procedure for computing envelopes as in Riani and Atkinson (Adv Data Anal Classif 1:123–141, 2007). Once defined our framework, we apply the forward search to a simulated environment where contaminations are exogenously introduced then, we carry out the analysis on n equity log-return real time series.","Copulas, Forward search, Squared Mahalanobis distance",https://link.springer.com//article/10.1007/s11634-010-0072-5,1163
Outlier detection and robust covariance estimation using mathematical programming,"The outlier detection problem and the robust covariance estimation problem are often interchangeable. Without outliers, the classical method of maximum likelihood estimation (MLE) can be used to estimate parameters of a known distribution from observational data. When outliers are present, they dominate the log likelihood function causing the MLE estimators to be pulled toward them. Many robust statistical methods have been developed to detect outliers and to produce estimators that are robust against deviation from model assumptions. However, the existing methods suffer either from computational complexity when problem size increases or from giving up desirable properties, such as affine equivariance. An alternative approach is to design a special mathematical programming model to find the optimal weights for all the observations, such that at the optimal solution, outliers are given smaller weights and can be detected. This method produces a covariance estimator that has the following properties: First, it is affine equivariant. Second, it is computationally efficient even for large problem sizes. Third, it easy to incorporate prior beliefs into the estimator by using semi-definite programming. The accuracy of this method is tested for different contamination models, including recently proposed ones. The method is not only faster than the Fast-MCD method for high dimensional data but also has reasonable accuracy for the tested cases.","Covariance matrix estimation, Robust statistics, Outlier detection, Optimization, Semi-definite programming, Newton–Raphson method",https://link.springer.com//article/10.1007/s11634-010-0070-7,1163
A review of robust clustering methods,"Deviations from theoretical assumptions together with the presence of certain amount of outlying observations are common in many practical statistical applications. This is also the case when applying Cluster Analysis methods, where those troubles could lead to unsatisfactory clustering results. Robust Clustering methods are aimed at avoiding these unsatisfactory results. Moreover, there exist certain connections between robust procedures and Cluster Analysis that make Robust Clustering an appealing unifying framework. A review of different robust clustering approaches in the literature is presented. Special attention is paid to methods based on trimming which try to discard most outlying data when carrying out the clustering process.","Clustering, Robustness, Model-based clustering, Trimming",https://link.springer.com//article/10.1007/s11634-010-0064-5,1163
A simulation study to compare robust clustering methods based on mixtures,"The following mixture model-based clustering methods are compared in a simulation study with one-dimensional data, fixed number of clusters and a focus on outliers and uniform “noise”: an ML-estimator (MLE) for Gaussian mixtures, an MLE for a mixture of Gaussians and a uniform distribution (interpreted as “noise component” to catch outliers), an MLE for a mixture of Gaussian distributions where a uniform distribution over the range of the data is fixed (Fraley and Raftery in Comput J 41:578–588, 1998), a pseudo-MLE for a Gaussian mixture with improper fixed constant over the real line to catch “noise” (RIMLE; Hennig in Ann Stat 32(4): 1313–1340, 2004), and MLEs for mixtures of t-distributions with and without estimation of the degrees of freedom (McLachlan and Peel in Stat Comput 10(4):339–348, 2000). The RIMLE (using a method to choose the fixed constant first proposed in Coretto, The noise component in model-based clustering. Ph.D thesis, Department of Statistical Science, University College London, 2008) is the best method in some, and acceptable in all, simulation setups, and can therefore be recommended.","Model-based clustering, Gaussian mixture, Mixture of t-distributions, Noise component",https://link.springer.com//article/10.1007/s11634-010-0065-4,1163
The k-step spatial sign covariance matrix,"The Sign Covariance Matrix is an orthogonal equivariant estimator of multivariate scale. It is often used as an easy-to-compute and highly robust estimator. In this paper we propose a k-step version of the Sign Covariance Matrix, which improves its efficiency while keeping the maximal breakdown point. If k tends to infinity, Tyler’s M-estimator is obtained. It turns out that even for very low values of k, one gets almost the same efficiency as Tyler’s M-estimator.","Breakdown point, Multivariate analysis, Principal components, Robust estimation, Spatial signs",https://link.springer.com//article/10.1007/s11634-010-0062-7,1163
Robust kernel principal component analysis and classification,"Kernel principal component analysis (KPCA) extends linear PCA from a real vector space to any high dimensional kernel feature space. The sensitivity of linear PCA to outliers is well-known and various robust alternatives have been proposed in the literature. For KPCA such robust versions received considerably less attention. In this article we present kernel versions of three robust PCA algorithms: spherical PCA, projection pursuit and ROBPCA. These robust KPCA algorithms are analyzed in a classification context applying discriminant analysis on the KPCA scores. The performances of the different robust KPCA algorithms are studied in a simulation study comparing misclassification percentages, both on clean and contaminated data. An outlier map is constructed to visualize outliers in such classification problems. A real life example from protein classification illustrates the usefulness of robust KPCA and its corresponding outlier map.","Principal component analysis, Kernel methods, Classification, Robustness",https://link.springer.com//article/10.1007/s11634-010-0068-1,1163
Optimal robust estimates using the Hellinger distance,"Optimal robust M-estimates of a multidimensional parameter are described using Hampel’s infinitesimal approach. The optimal estimates are derived by minimizing a measure of efficiency under the model, subject to a bounded measure of infinitesimal robustness. To this purpose we define measures of efficiency and infinitesimal sensitivity based on the Hellinger distance. We show that these two measures coincide with similar ones defined by Yohai using the Kullback–Leibler divergence, and therefore the corresponding optimal estimates coincide too. We also give an example where we fit a negative binomial distribution to a real dataset of “days of stay in hospital” using the optimal robust estimates.","Hampel’s infinitesimal approach, gross error sensitivity, negative binomial distribution",https://link.springer.com//article/10.1007/s11634-010-0061-8,1163
Inference for robust canonical variate analysis,"We consider the problem of optimally separating two multivariate populations. Robust linear discriminant rules can be obtained by replacing the empirical means and covariance in the classical discriminant rules by S or MM-estimates of location and scatter. We propose to use a fast and robust bootstrap method to obtain inference for such a robust discriminant analysis. This is useful since classical bootstrap methods may be unstable as well as extremely time-consuming when robust estimates such as S or MM-estimates are involved. In particular, fast and robust bootstrap can be used to investigate which variables contribute significantly to the canonical variate, and thus the discrimination of the classes. Through bootstrap, we can also examine the stability of the canonical variate. We illustrate the method on some real data examples.","Bootstrap, Canonical variate, Linear discriminant analysis, Robustness",https://link.springer.com//article/10.1007/s11634-010-0063-6,1163
A review on consistency and robustness properties of support vector machines for heavy-tailed distributions,"Support vector machines (SVMs) belong to the class of modern statistical machine learning techniques and can be described as M-estimators with a Hilbert norm regularization term for functions. SVMs are consistent and robust for classification and regression purposes if based on a Lipschitz continuous loss and a bounded continuous kernel with a dense reproducing kernel Hilbert space. For regression, one of the conditions used is that the output variable Y has a finite first absolute moment. This assumption, however, excludes heavy-tailed distributions. Recently, the applicability of SVMs was enlarged to these distributions by considering shifted loss functions. In this review paper, we briefly describe the approach of SVMs based on shifted loss functions and list some properties of such SVMs. Then, we prove that SVMs based on a bounded continuous kernel and on a convex and Lipschitz continuous, but not necessarily differentiable, shifted loss function have a bounded Bouligand influence function for all distributions, even for heavy-tailed distributions including extreme value distributions and Cauchy distributions. SVMs are thus robust in this sense. Our result covers the important loss functions \({\epsilon}\) -insensitive for regression and pinball for quantile regression, which were not covered by earlier results on the influence function. We demonstrate the usefulness of SVMs even for heavy-tailed distributions by applying SVMs to a simulated data set with Cauchy errors and to a data set of large fire insurance claims of Copenhagen Re.","Regularized empirical risk minimization, Support vector machines, Consistency, Robustness, Bouligand influence function, Heavy tails",https://link.springer.com//article/10.1007/s11634-010-0067-2,1163
Methods for merging Gaussian mixture components,"The problem of merging Gaussian mixture components is discussed in situations where a Gaussian mixture is fitted but the mixture components are not separated enough from each other to interpret them as “clusters”. The problem of merging Gaussian mixtures is not statistically identifiable, therefore merging algorithms have to be based on subjective cluster concepts. Cluster concepts based on unimodality and misclassification probabilities (“patterns”) are distinguished. Several different hierarchical merging methods are proposed for different cluster concepts, based on the ridgeline analysis of modality of Gaussian mixtures, the dip test, the Bhattacharyya dissimilarity, a direct estimator of misclassification and the strength of predicting pairwise cluster memberships. The methods are compared by a simulation study and application to two real datasets. A new visualisation method of the separation of Gaussian mixture components, the ordered posterior plot, is also introduced.","Model-based cluster analysis, Multilayer mixture, Unimodality, Prediction strength, Ridgeline, Dip test",https://link.springer.com//article/10.1007/s11634-010-0058-3,1163
Regularized fuzzy clusterwise ridge regression,"Fuzzy clusterwise regression has been a useful method for investigating cluster-level heterogeneity of observations based on linear regression. This method integrates fuzzy clustering and ordinary least-squares regression, thereby enabling to estimate regression coefficients for each cluster and fuzzy cluster memberships of observations simultaneously. In practice, however, fuzzy clusterwise regression may suffer from multicollinearity as it builds on ordinary least-squares regression. To deal with this problem in fuzzy clusterwise regression, a new method, called regularized fuzzy clusterwise ridge regression, is proposed that combines ridge regression with regularized fuzzy clustering in a unified framework. In the proposed method, ridge regression is adopted to estimate clusterwise regression coefficients while handling potential multicollinearity among predictor variables. In addition, regularized fuzzy clustering based on maximizing entropy is utilized to systematically determine an optimal degree of fuzziness in memberships. A simulation study is conducted to evaluate parameter recovery of the proposed method as compared to the extant non-regularized counterpart. The usefulness of the proposed method is illustrated by an application concerning the relationship among the characteristics of used cars.","Clusterwise regression, Regularized fuzzy clustering, Ridge regression",https://link.springer.com//article/10.1007/s11634-009-0056-5,1163
Comparing partitions of two sets of units based on the same variables,We propose a procedure based on a latent variable model for the comparison of two partitions of different units described by the same set of variables. The null hypothesis here is that the two partitions come from the same underlying mixture model. We define a method of “projecting” partitions using a supervised classification method: once one partition is taken as a reference; the individuals of the second data set are allocated to the clusters of the reference partition; it gives two partitions of the same units of the second data set: the original and the projected one and we evaluate their difference by usual measures of association. The empirical distributions of the association measures are derived by simulation.,"Rand index, Redundancy index, Discriminant analysis, Latent classes, Partitions",https://link.springer.com//article/10.1007/s11634-009-0057-4,1163
Tag SNP selection based on clustering according to dominant sets found using replicator dynamics,"Tag SNP selection is an important problem in genetic association studies. A class of algorithms to perform this task, among them a popular tool called Tagger, can be described as searching for a minimal vertex cover of a graph. In this article this approach is contrasted with a recently introduced clustering algorithm based on the graph theoretical concept of dominant sets. To compare the performance of both procedures comprehensive simulation studies have been performed using SNP data from the ten ENCODE regions included in the HapMap project. Quantitative traits have been simulated from additive models with a single causative SNP. Simulation results suggest that clustering performs always at least as good as Tagger, while in more than a third of the considered instances substantial improvement can be observed. Additionally an extension of the clustering algorithm is described which can be used for larger genomic data sets.","Clustering, Dominant set, Replicator dynamics, Tag SNP selection",https://link.springer.com//article/10.1007/s11634-010-0059-2,1163
Tests of ignoring and eliminating in nonsymmetric correspondence analysis,"Nonsymmetric correspondence analysis (NSCA) aims to examine predictive relationships between rows and columns of a contingency table. The predictor categories of such tables are often accompanied by some auxiliary information. Constrained NSCA (CNSCA) incorporates such information as linear constraints on the predictor categories. However, imposing constraints also means that part of the predictive relationship is left unaccounted for by the constraints. A method of NSCA is proposed for analyzing the residual part along with the part accounted for by the constraints. The CATANOVA test may be invoked to test the significance of each part. The two tests parallel the distinction between tests of ignoring and eliminating, and help gain some insight into what is known as Simpson’s paradox in the analysis of contingency tables. Two examples are given to illustrate the distinction.","Contingency table analysis, The Goodman–Kruskal τ index, CATANOVA, Simpson’s paradox",https://link.springer.com//article/10.1007/s11634-009-0054-7,1163
Metrics of Lp-type and distributional equivalence principle,"Given a frequency table \({F=\{f_{jk},(j,k)\in\,J\times K\}}\) crossing two categorical variables J and K, we consider a family of metrics of Lp-type on J defined by \({d_J^p (j,j^{\prime}) = \Sigma_k g(f_{.k})|f_{jk}/f_{j.} - f_{j^{\prime}k}/f_{j^{\prime}.}|^p}\), where g is a positive function, and a symmetrical one on K. We investigate under which conditions on g, the famous principle of distributional equivalence is fulfilled by these metrics for every rational or every real F.","Distributional equivalence, χ2-Metric, Metrics of Lp-type, Correspondence analysis",https://link.springer.com//article/10.1007/s11634-009-0049-4,1163
On critical sets of a finite Moore family,"Cluster collections obtained within the framework of most cluster structures studied in data analysis and classification are essentially Moore families. In this paper, we propose a simple intuitive necessary and sufficient condition for some subset of objects to be a critical set of a finite Moore family. This condition is based on a new characterization of quasi-closed sets. Moreover, we provide a necessary condition for a subset containing more than k objects (k ≥ 2) to be a critical set of a k-weakly hierarchical Moore family. Finally, as a consequence of this result, we identify critical sets of some k-weakly hierarchical Moore families and thereby generalize a result earlier obtained by Domenach and Leclerc in the particular case of weak hierarchies.","Closure operator, Closure system, Implication, Quasi-closed set, Weak hierarchy",https://link.springer.com//article/10.1007/s11634-009-0053-8,1163
The value of the last digit: statistical fraud detection with digit analysis,"Digit distributions are a popular tool for the detection of tax payers’ noncompliance and other fraud. In the early stage of digital analysis, Nigrini and Mittermaier (A J Pract Theory 16(2):52–67, 1997) made use of Benford’s Law (Benford in Am Philos Soc 78:551–572, 1938) as a natural reference distribution. A justification of that hypothesis is only known for multiplicative sequences (Schatte in J Inf Process Cyber EIK 24:443–455, 1988). In applications, most of the number generating processes are of an additive nature and no single choice of ‘an universal first-digit law’ seems to be plausible (Scott and Fasli in Benford’s law: an empirical investigation and a novel explanation. CSM Technical Report 349, Department of Computer Science, University of Essex, http://cswww.essex.ac.uk/technical-reports/2001/CSM-349.pdf, 2001). In that situation, some practioneers (e.g. financial authorities) take recourse to a last digit analysis based on the hypothesis of a Laplace distribution. We prove that last digits are approximately uniform for distributions with an absolutely continuous distribution function. From a practical perspective, that result, of course, is only moderately interesting. For that reason, we derive a result for ‘certain’ sums of lattice-variables as well. That justification is provided in terms of stationary distributions.","Fraud detection, Last digits, Digit analysis, Benford’s law",https://link.springer.com//article/10.1007/s11634-009-0048-5,1163
New robust dynamic plots for regression mixture detection,"The forward search is a powerful general method for detecting multiple masked outliers and for determining their effect on inferences about models fitted to data. From the monitoring of a series of statistics based on subsets of data of increasing size we obtain multiple views of any hidden structure. One of the problems of the forward search has always been the lack of an automatic link among the great variety of plots which are monitored. Usually it happens that a lot of interesting features emerge unexpectedly during the progression of the forward search only when a specific combination of forward plots is inspected at the same time. Thus, the analyst should be able to interact with the plots and redefine or refine the links among them. In the absence of dynamic linking and interaction tools, the analyst risks to miss relevant hidden information. In this paper we fill this gap and provide the user with a set of new robust graphical tools whose power will be demonstrated on several regression problems. Through the analysis of real and simulated data we give a series of examples where dynamic interaction with different “robust plots” is used to highlight the presence of groups of outliers and regression mixtures and appraise the effect that these hidden groups exert on the fitted model.","Forward search, Robustness, Exploratory data analysis, Data visualization, Statistical graphics, Brushing and linking",https://link.springer.com//article/10.1007/s11634-009-0050-y,1163
Temporally adaptive estimation of logistic classifiers on data streams,"Modern technology has allowed real-time data collection in a variety of domains, ranging from environmental monitoring to healthcare. Consequently, there is a growing need for algorithms capable of performing inferential tasks in an online manner, continuously revising their estimates to reflect the current status of the underlying process. In particular, we are interested in constructing online and temporally adaptive classifiers capable of handling the possibly drifting decision boundaries arising in streaming environments. We first make a quadratic approximation to the log-likelihood that yields a recursive algorithm for fitting logistic regression online. We then suggest a novel way of equipping this framework with self-tuning forgetting factors. The resulting scheme is capable of tracking changes in the underlying probability distribution, adapting the decision boundary appropriately and hence maintaining high classification accuracy in dynamic or unstable environments. We demonstrate the scheme’s effectiveness in both real and simulated streaming environments.","Logistic regression, Adaptive forgetting, Data streams, Classification",https://link.springer.com//article/10.1007/s11634-009-0051-x,1163
Comparison of three hypothesis testing approaches for the selection of the appropriate number of clusters of variables,"Within the scope of cluster analysis of variables, the selection of the appropriate number of clusters is of paramount interest. The strategy of determination of the appropriate number of clusters adopted herein is based on a hypothesis testing approach. It consists in testing whether the variation of a partition quality criterion between two consecutive partitions is far removed from the expected variation under the null-hypothesis stipulating a lack of structure. Three hypothesis testing strategies are detailed and compared in the scope of clustering of variables: Gap, Weighted Gap and a statistic associated with CLV methodology. Finally, an illustration is presented based on data from a preference study.","Clustering of variables, Number of clusters, Hypothesis testing approach, Gap statistic, CLV",https://link.springer.com//article/10.1007/s11634-009-0047-6,1163
Aggregation invariance in general clustering approaches,"General clustering deals with weighted objects and fuzzy memberships. We investigate the group- or object-aggregation-invariance properties possessed by the relevant functionals (effective number of groups or objects, centroids, dispersion, mutual object-group information, etc.). The classical squared Euclidean case can be generalized to non-Euclidean distances, as well as to non-linear transformations of the memberships, yielding the c-means clustering algorithm as well as two presumably new procedures, the convex and pairwise convex clustering. Cluster stability and aggregation-invariance of the optimal memberships associated to the various clustering schemes are examined as well.","Aggregation invariance, c-Means clustering, EM algorithm, Fuzzy clustering, Model-based clustering, Mutual information",https://link.springer.com//article/10.1007/s11634-009-0052-9,1163
Parsimonious cluster systems,"We introduce in this paper a new clustering structure, parsimonious cluster systems, which generalizes phylogenetic trees. We characterize it as the set of hypertrees stable under restriction and prove that this set is in bijection with a known dissimilarity model: chordal quasi-ultrametrics. We then present one possible way to graphically represent elements of this model.","Overlapping clustering, Parsimony, Phylogenetic trees, Dissimilarities",https://link.springer.com//article/10.1007/s11634-009-0046-7,1163
Comparison of alignment free string distances for complete genome phylogeny,"In this paper, we compare the accuracy of four string distances on complete genomes to reconstruct phylogenies using simulated and real biological data. These distances are based on common words shared by raw genomic sequences and do not require preliminary processing steps such as gene identification or sequence alignment. Moreover, they are computable in linear time. The first distance is based on Maximum Significant Matches (MSM). The second is computed from the frequencies of all the words of length k (KW). The third distance is based on the Average length of maximum Common Substrings at any position (ACS). The last one is based on the Ziv–Lempel compression algorithm (ZL). We describe a simulation process of evolution to generate a set of sequences having evolved according to a random tree topology T. This process allows both base substitution and fragment insertion/deletion, including horizontal transfers. The distances between the generated sequences are computed using the four formulas and the corresponding trees T′ are reconstructed using Neighbor-Joining. T and T′ are compared according to topological criteria. These comparisons show that the MSM distance outperforms the others whatever the parameters used to generate sequences. Finally, we test the MSM and KW distances on real biological data (i.e. prokaryotic complete genomes) and we compare the NJ trees to a Maximum Likelihood 16S + 23S RNA tree. We show that the MSM distance provides accurate results to study intra-phylum relationships, much better than those given by KW.","Phylogeny, String distances, Complete bacterial genomes",https://link.springer.com//article/10.1007/s11634-009-0041-z,1163
Variable selection in model-based clustering using multilocus genotype data,"We propose a variable selection procedure in model-based clustering using multilocus genotype data. Indeed, it may happen that some loci are not relevant for clustering into statistically different populations. Inferring the number K of clusters and the relevant clustering subset S of loci is seen as a model selection problem. The competing models are compared using penalized maximum likelihood criteria. Under weak assumptions on the penalty function, we prove the consistency of the resulting estimator \({(\widehat{K}_n, \widehat{S}_n)}\). An associated algorithm named Mixture Model for Genotype Data (MixMoGenD) has been implemented using c++ programming language and is available on http://www.math.u-psud.fr/~toussile. To avoid an exhaustive search of the optimum model, we propose a modified Backward-Stepwise algorithm, which enables a better search of the optimum model among all possible cardinalities of S. We present numerical experiments on simulated and real datasets that highlight the interest of our loci selection procedure.","Model-based clustering, Penalized maximum likelihood criteria, Population genetics, Variable selection",https://link.springer.com//article/10.1007/s11634-009-0043-x,1163
Trimming algorithms for clustering contaminated grouped data and their robustness,"We establish an affine equivariant, constrained heteroscedastic model and criterion with trimming for clustering contaminated, grouped data. We show existence of the maximum likelihood estimator, propose a method for determining an appropriate constraint, and design a strategy for finding reasonable clusterings. We finally compute breakdown points of the estimated parameters thereby showing asymptotic robustness of the method.","Statistical clustering, Robust clustering, Trimming algorithm, Breakdown points, Heteroscedasticity, HDBT ratio",https://link.springer.com//article/10.1007/s11634-009-0044-9,1163
"On Robinsonian dissimilarities, the consecutive ones property and latent variable models","A dissimilarity measure on a set of objects is Robinsonian if its matrix can be symmetrically permuted so that its elements do not decrease when moving away from the main diagonal along any row or column. The Robinson property of a dissimilarity reflects an order of the objects. If a dissimilarity is not observed directly, it must be obtained from the data. Given that an ordinal structure is assumed to underlie the data, the dissimilarity function of choice may or may not recover the order correctly. For four dissimilarity measures for binary data it is investigated what ordinal data structure of 0s and 1s is correctly recovered. We derive sufficient conditions for the dissimilarity functions to be Robinsonian. The sufficient conditions differ with the dissimilarity measures. The paper concludes with some limitations of the study.","Dissimilarity measures, Binary data, Ordinal comparison, Pyramids, Ordered clustering systems, Weakly pseudo-hierarchies",https://link.springer.com//article/10.1007/s11634-009-0042-y,1163
Bayesian unsupervised classification framework based on stochastic partitions of data and a parallel search strategy,"Advantages of statistical model-based unsupervised classification over heuristic alternatives have been widely demonstrated in the scientific literature. However, the existing model-based approaches are often both conceptually and numerically instable for large and complex data sets. Here we consider a Bayesian model-based method for unsupervised classification of discrete valued vectors, that has certain advantages over standard solutions based on latent class models. Our theoretical formulation defines a posterior probability measure on the space of classification solutions corresponding to stochastic partitions of observed data. To efficiently explore the classification space we use a parallel search strategy based on non-reversible stochastic processes. A decision-theoretic approach is utilized to formalize the inferential process in the context of unsupervised classification. Both real and simulated data sets are used for the illustration of the discussed methods.","Bayesian classification, Markov chain Monte Carlo, Statistical learning, Stochastic optimization",https://link.springer.com//article/10.1007/s11634-009-0036-9,1163
Where are the large and difficult datasets?,"A great many comparative performance assessments of classification rules have been undertaken, ranging from small ones involving just one or two methods, to large ones involving many tens of methods. We are undertaking a meta-analytic study of these studies, attempting to distil some overall conclusions. This paper describes just one of our observations. The dataset analysed in this paper contains 5,203 error rates taken from 45 articles and describing 146 datasets. One curious general relationship which was persistent in our data, despite the fact that we were looking at results mixed between distributions rather than conditional on distributions, was that error rate decreased with increasing dataset size. We believe this to be an artefact of the way datasets are collected by the research community.","Error rate, Meta-analysis, Comparative studies, Repositories",https://link.springer.com//article/10.1007/s11634-009-0037-8,1163
Optimum simultaneous discretization with data grid models in supervised classification: a Bayesian model selection approach,"In the domain of data preparation for supervised classification, filter methods for variable ranking are time efficient. However, their intrinsic univariate limitation prevents them from detecting redundancies or constructive interactions between variables. This paper introduces a new method to automatically, rapidly and reliably extract the classificatory information of a pair of input variables. It is based on a simultaneous partitioning of the domains of each input variable, into intervals in the numerical case and into groups of categories in the categorical case. The resulting input data grid allows to quantify the joint information between the two input variables and the output variable. The best joint partitioning is searched by maximizing a Bayesian model selection criterion. Intensive experiments demonstrate the benefits of the approach, especially the significant improvement of accuracy for classification tasks.","Data preparation, Discretization, Feature selection, Model selection, Supervised classification",https://link.springer.com//article/10.1007/s11634-009-0038-7,1163
An update algorithm for restricted random walk clustering for dynamic data sets,"In this article, we present a randomized dynamic cluster algorithm for large data sets. It is based on the restricted random walk cluster algorithm by Schöll and Schöll-Paschinger that has given good results in past studies. We discuss different approaches for the clustering of dynamic data sets. In contrast to most of these methods, dynamic restricted random walk clustering is also efficient for a small percentage of changes in the data set and has the additional advantage that the updates asymptotically produce the same clusters as a reclustering with the static variant; there is thus no need for any reclustering ever. In addition, the method has a relatively low computational complexity which enables it to cluster large data sets.","Clustering, Dynamic clustering, Random walks",https://link.springer.com//article/10.1007/s11634-009-0039-6,1163
Multi-objective optimization for clustering 3-way gene expression data,"In this paper, we use the Fuzzy C-means method for clustering 3-way gene expression data via optimization of multiple objectives. A reformulation of the total clustering criterion is used to obtain an expression which has fewer variables compared to the classical FCM criterion. This transformation allows the use of a direct global optimizer in constrast to the alternating search commonly used. Gene expression data from microarray technology is generally of high dimension. The problem of empty space is known for this kind of data. We propose in this paper a transformation allowing more contrast in distances between all pairs of data samples. This, hence, increases the likelihood of detecting group structure, if any, in high dimensional datasets.","Multi-objective optimization, Clustering, Fuzzy C-means, Microarray, Gene expression data",https://link.springer.com//article/10.1007/s11634-008-0032-5,1163
A constrained-optimization based half-quadratic algorithm for robustly fitting sets of linearly parametrized curves,"We consider the problem of multiple fitting of linearly parametrized curves, that arises in many computer vision problems such as road scene analysis. Data extracted from images usually contain non-Gaussian noise and outliers, which makes classical estimation methods ineffective. In this paper, we first introduce a family of robust probability density functions which appears to be well-suited to many real-world problems. Also, such noise models are suitable for defining continuation heuristics to escape shallow local minima and their robustness is devised in terms of breakdown point. Second, the usual Iterative Reweighted Least Squares (IRLS) robust estimator is extended to the problem of robustly estimating sets of linearly parametrized curves. The resulting, non-convex optimization problem is tackled within a Lagrangian approach, leading to the so-called Simultaneous Robust Multiple Fitting (SRMF) algorithm, whose global convergence to a local minimum is proved using results from constrained optimization theory.","Non-convex problem, Constrained optimisation, Primal and dual problem, Robust estimators, Image analysis",https://link.springer.com//article/10.1007/s11634-008-0031-6,1163
About the non-convex optimization problem induced by non-positive semidefinite kernel learning,"During the last years, kernel based methods proved to be very successful for many real-world learning problems. One of the main reasons for this success is the efficiency on large data sets which is a result of the fact that kernel methods like support vector machines (SVM) are based on a convex optimization problem. Solving a new learning problem can now often be reduced to the choice of an appropriate kernel function and kernel parameters. However, it can be shown that even the most powerful kernel methods can still fail on quite simple data sets in cases where the inherent feature space induced by the used kernel function is not sufficient. In these cases, an explicit feature space transformation or detection of latent variables proved to be more successful. Since such an explicit feature construction is often not feasible for large data sets, the ultimate goal for efficient kernel learning would be the adaptive creation of new and appropriate kernel functions. It can, however, not be guaranteed that such a kernel function still leads to a convex optimization problem for Support Vector Machines. Therefore, we have to enhance the optimization core of the learning method itself before we can use it with arbitrary, i.e., non-positive semidefinite, kernel functions. This article motivates the usage of appropriate feature spaces and discusses the possible consequences leading to non-convex optimization problems. We will show that these new non-convex optimization SVM are at least as accurate as their quadratic programming counterparts on eight real-world benchmark data sets in terms of the generalization performance. They always outperform traditional approaches in terms of the original optimization problem. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite or indefinite kernel functions.","Support vector machine, Kernel methods, Non-convex optimization",https://link.springer.com//article/10.1007/s11634-008-0033-4,1163
A DC programming approach for feature selection in support vector machines learning,"Feature selection consists of choosing a subset of available features that capture the relevant properties of the data. In supervised pattern classification, a good choice of features is fundamental for building compact and accurate classifiers. In this paper, we develop an efficient feature selection method using the zero-norm l0 in the context of support vector machines (SVMs). Discontinuity at the origin for l0 makes the solution of the corresponding optimization problem difficult to solve. To overcome this drawback, we use a robust DC (difference of convex functions) programming approach which is a general framework for non-convex continuous optimisation. We consider an appropriate continuous approximation to l0 such that the resulting problem can be formulated as a DC program. Our DC algorithm (DCA) has a finite convergence and requires solving one linear program at each iteration. Computational experiments on standard datasets including challenging feature-selection problems of the NIPS 2003 feature selection challenge and gene selection for cancer classification show that the proposed method is promising: while it suppresses up to more than 99% of the features, it can provide a good classification. Moreover, the comparative results illustrate the superiority of the proposed approach over standard methods such as classical SVMs and feature selection concave.","Feature selection, SVM, Nonconvex optimisation, DC programming, DCA",https://link.springer.com//article/10.1007/s11634-008-0030-7,1163
Transfer distance between partitions,"The comparison of partitions is a central topic in clustering, as well as when comparing partitioning algorithms or when classifying nominal variables. In this paper, we deal with the transfer distance between partitions, defined as the minimum number of transfers of one element from its class to another (possibly empty) necessary to turn one partition into the other one. After reviewing some theoretical results about this distance, we analyse its behaviour by an experimental study in order to make its interpretation easier.","Partition, Distance, Clustering, Transfer graph, Centrality",https://link.springer.com//article/10.1007/s11634-008-0029-0,1163
Generalized marginal homogeneity model and its relation to marginal equimoments for square contingency tables with ordered categories,"For square contingency tables with ordered categories, Tomizawa (Calcutta Stat Assoc Bull 43:123–125, 1993a; Sankhyā Ser B 60:293–300, 1998) gave theorems that the marginal homogeneity (MH) model is equivalent to certain two or three models holding simultaneously. This paper proposes a generalized MH model, which describes a structure of the odds that an observation will fall in row category i or below and column category i + 1 or above, instead of in column category i or below and row category i + 1 or above. In addition, this paper gives the theorems that the MH model is equivalent to the generalized MH model and some models holding simultaneously whose each indicates: (1) the equality of m-order moment of row and column variables, (2) the equality of skewness of them and (3) the equality of kurtosis of them. When the MH model fits the data poorly, these may be useful for seeing the reason for the poor fit; for instance, the poor fit of the MH model is caused by the poor fit of the equality of row and column means rather than the generalized MH model. Examples are given.","Generalized marginal homogeneity, Kurtosis, Marginal homogeneity, Moment, Ordered category, Skewness, Square contingency table",https://link.springer.com//article/10.1007/s11634-008-0028-1,1163
Fitting semiparametric clustering models to dissimilarity data,"The cluster analysis problem of partitioning a set of objects from dissimilarity data is here handled with the statistical model-based approach of fitting the “closest” classification matrix to the observed dissimilarities. A classification matrix represents a clustering structure expressed in terms of dissimilarities. In cluster analysis there is a lack of methodologies widely used to directly partition a set of objects from dissimilarity data. In real applications, a hierarchical clustering algorithm is applied on dissimilarities and subsequently a partition is chosen by visual inspection of the dendrogram. Alternatively, a “tandem analysis” is used by first applying a Multidimensional Scaling (MDS) algorithm and then by using a partitioning algorithm such as k-means applied on the dimensions specified by the MDS. However, neither the hierarchical clustering algorithms nor the tandem analysis is specifically defined to solve the statistical problem of fitting the closest partition to the observed dissimilarities. This lack of appropriate methodologies motivates this paper, in particular, the introduction and the study of three new object partitioning models for dissimilarity data, their estimation via least-squares and the introduction of three new fast algorithms.","Cluster analysis, Partitions, Semiparametric clustering models, LS-estimation",https://link.springer.com//article/10.1007/s11634-008-0025-4,1163
"On multi-way metricity, minimality and diagonal planes","Validity of the triangle inequality and minimality, both axioms for two-way dissimilarities, ensures that a two-way dissimilarity is nonnegative and symmetric. Three-way generalizations of the triangle inequality and minimality from the literature are reviewed and it is investigated what forms of symmetry and nonnegativity are implied by the three-way axioms. A special form of three-way symmetry that can be deduced is equality of the diagonal planes of the three-dimensional cube. Furthermore, it is studied what diagonal plane equalities hold for the four-dimensional tesseract.","Diagonal plane equality, Tetrahedron inequality, Multi-way symmetry, Three-way block, Tesseract, Multi-way dissimilarity",https://link.springer.com//article/10.1007/s11634-008-0026-3,1163
Cluster analysis of census data using the symbolic data approach,"The aim of this paper is to investigate the economic specialization of the Italian local labor systems (sets of contiguous municipalities with a high degree of self-containment of daily commuter travel) by using the Symbolic Data approach, on the basis of data derived from the Census of Industrial and Service Activities. Specifically, the economic structure of a local labor system (LLS) is described by an interval-type variable, a special symbolic data type that allows for the fact that all municipalities within the same LLS do not have the same economic structure.","Symbolic data analysis, Local labor systems, Interval variables",https://link.springer.com//article/10.1007/s11634-008-0024-5,1163
Multiple taxicab correspondence analysis,"We compare the statistical analysis of multidimensional contingency tables by multiple correspondence analysis (MCA) and multiple taxicab correspondence analysis (MTCA). We will show in this paper: First, MTCA and MCA can produce different results. Second, taxicab correspondence analysis of a Burt table is equivalent to centroid correspondence analysis of the indicator matrix. Third, along the first principal axis, the projected response patterns in MTCA will be clustered and the number of cluster points is less than or equal to 1+ the number of variables. Fourth, visual maps produced by MTCA seem to be clearer and more readable in the presence of rarely occurring categories of the variables than the graphical displays produced by MCA. Two well known data sets are analyzed.","Indicator matrix, Burt table, Correspondence analysis, Taxicab decomposition, Centroid decomposition",https://link.springer.com//article/10.1007/s11634-008-0023-6,1163
Interpolation of spatial and spatio-temporal Gaussian fields using Gaussian Markov random fields,"This paper considers interpolation on a lattice of covariance-based Gaussian Random Field models (Geostatistics models) using Gaussian Markov Random Fields (GMRFs) (conditional autoregression models). Two methods for estimating the GMRF parameters are considered. One generalises maximum likelihood for complete data, and the other ensures a better correspondence between fitted and theoretical correlations for higher lags. The methods can be used both for spatial and spatio-temporal data. Some different cross-validation methods for model choice are compared. The predictive ability of the GMRF is demonstrated by a simulation study, and an example using a real image is considered.","Gaussian Markov random fields, Geostatistics, Interpolation, Inverse correlations, Kriging, Spatio-temporal processes",https://link.springer.com//article/10.1007/s11634-008-0019-2,1163
Two local dissimilarity measures for weighted graphs with application to protein interaction networks,"We extend the Czekanowski-Dice dissimilarity measure, classically used to cluster the vertices of unweighted graphs, to weighted ones. The first proposed formula corresponds to edges weighted by a probability of existence. The second one is adapted to edges weighted by intensity or strength. We show on simulated graphs that the class identification process is improved by computing weighted compared to unweighted edges. Finally, an application to a drosophila protein network illustrates the fact that using these new formulas improves the ’biological accuracy’ of partitioning.","Graph distance, Graph partitioning, Heuristic optimisation, Biological networks",https://link.springer.com//article/10.1007/s11634-008-0018-3,1163
SVM-Maj: a majorization approach to linear support vector machines with different hinge errors,"Support vector machines (SVM) are becoming increasingly popular for the prediction of a binary dependent variable. SVMs perform very well with respect to competing techniques. Often, the solution of an SVM is obtained by switching to the dual. In this paper, we stick to the primal support vector machine problem, study its effective aspects, and propose varieties of convex loss functions such as the standard for SVM with the absolute hinge error as well as the quadratic hinge and the Huber hinge errors. We present an iterative majorization algorithm that minimizes each of the adaptations. In addition, we show that many of the features of an SVM are also obtained by an optimal scaling approach to regression. We illustrate this with an example from the literature and do a comparison of different methods on several empirical data sets.","Support vector machines, Iterative majorization, Absolute hinge error, Quadratic hinge error, Huber hinge error, Optimal scaling",https://link.springer.com//article/10.1007/s11634-008-0020-9,1163
Plastic card fraud detection using peer group analysis,"Peer group analysis is an unsupervised method for monitoring behaviour over time. In the context of plastic card fraud detection, this technique can be used to find anomalous transactions. These are transactions that deviate strongly from their peer group and are flagged as potentially fraudulent. Time alignment, the quality of the peer groups and the timeliness of assigning fraud flags to transactions are described. We demonstrate the ability to detect fraud using peer groups with real credit card transaction data and define a novel method for evaluating performance.","Fraud detection, Outlier detection, Peer groups",https://link.springer.com//article/10.1007/s11634-008-0021-8,1163
Generalized constrained co-inertia analysis,"In this paper a method for generalized constrained co-inertia analysis is proposed. This approach is based on a suitable decomposition of each set of variables which incorporates external information on both rows and columns of data matrices. These external information are incorporated in the analysis in order to improve the interpretability of the phenomenon under investigation. Once both matrices are decomposed according to the external information, we propose to apply the co-inertia analysis to any pair of obtainable submatrices in order to study relationships between them. A variety of existing and new extensions of COA are then realized. According to the nature of the data, this approach subsumes also several generalized constrained methods proposed in literature. An example is given to illustrate the proposed method.","External information, Constrained principal component analysis, Co-inertia analysis, Generalized singular value decomposition",https://link.springer.com//article/10.1007/s11634-008-0017-4,1163
Discrimination with jointly equicorrelated multi-level multivariate data,"In this article we study a linear as well as a quadratic discriminant function for multi-level multivariate repeated measurement data under the assumption of multivariate normality. We assume that the m-variate observations have jointly equicorrelated covariance structure in addition to a Kronecker product structure on the mean vector. The new discriminant functions are very effective in discriminating individuals when the number of observations is very small. The proposed classification rules are demonstrated on a real data set. The error rates of the proposed classification rules are found to be much less than the error rates of the traditional classification rules, when in fact the traditional classification rules fail most of the time owing to the small sample sizes.","Classification rule, Jointly equicorrelated covariance structure, Multivariate repeated measures data, Maximum likelihood estimates",https://link.springer.com//article/10.1007/s11634-007-0013-0,1163
A global algorithm to estimate the expectations of the components of an observed univariate mixture,"This paper deals with the unsupervised classification of univariate observations. Given a set of observations originating from a K-component mixture, we focus on the estimation of the component expectations. We propose an algorithm based on the minimization of the “K-product” (KP) criterion we introduced in a previous work. We show that the global minimum of this criterion can be reached by first solving a linear system then calculating the roots of some polynomial of order K. The KP global minimum provides a first raw estimate of the component expectations, then a nearest-neighbour classification enables to refine this estimation. Our method’s relevance is finally illustrated through simulations of various mixtures. When the mixture components do not strongly overlap, the KP algorithm provides better estimates than the Expectation-Maximization algorithm.","Univariate mixture, Component expectations estimation, Unsupervised classification",https://link.springer.com//article/10.1007/s11634-007-0014-z,1163
Lambda pruning: an approximation of the string subsequence kernel for practical SVM classification and redundancy clustering,"The support vector machine (SVM) is a powerful learning algorithm, e.g., for classification and clustering tasks, that works even for complex data structures such as strings, trees, lists and general graphs. It is based on the usage of a kernel function for measuring scalar products between data units. For analyzing string data Lodhi et al. (J Mach Learn Res 2:419–444, 2002) have introduced a String Subsequence kernel (SSK). In this paper we propose an approximation to SSK based on dropping higher orders terms (i.e., subsequences which are spread out more than a certain threshold) that reduces the computational burden of SSK. As we are also concerned with practical application of complex kernels with high computational complexity and memory consumption, we provide an empirical model to predict runtime and memory of the approximation as well as the original SSK, based on easily measurable properties of input data. We provide extensive results on the properties of the proposed approximation, SSK-LP, with respect to prediction accuracy, runtime and memory consumption. Using some real-life datasets of text mining tasks, we show that models based on SSK and SSK-LP perform similarly for a set of real-life learning tasks, and that the empirical runtime model is also useful in roughly determining total learning time for a SVM using either kernel.","Machine learning, Kernel methods, String kernels, Runtime estimation, Memory consumption estimation",https://link.springer.com//article/10.1007/s11634-007-0012-1,1163
A recursive partitioning tool for interval prediction,"The traditional approach to regression trees involves partitioning the space of predictor variables into subsets that optimise a function of the response variable(s), and then predicting future response values by a single-valued summary statistic in each subset. Our belief is that a prediction interval is of greater practical use than a predictive value, and that the criterion for the partitioning should be based on such intervals rather than on single values. We define four potential criteria in the case of a single response variable, discuss computational aspects of producing the partition, evaluate the criteria on both real and simulated data, and draw some tentative conclusions about their relative efficacies. The methodology is extended to the case of multiple response variables, and its viability is demonstrated by application to some further real data. The possibility of fitting distributions to within-subsets data is discussed, and some potential extensions are briefly outlined.","Binary splitting, CART, Clustering, Criterion optimisation, Multivariate response variable, Prediction intervals, Regression trees",https://link.springer.com//article/10.1007/s11634-007-0015-y,1163
Classification in music research,"Since a few years, classification in music research is a very broad and quickly growing field. Most important for adequate classification is the knowledge of adequate observable or deduced features on the basis of which meaningful groups or classes can be distinguished. Unsupervised classification additionally needs an adequate similarity or distance measure grouping is to be based upon. Evaluation of supervised learning is typically based on the error rates of the classification rules. In this paper we first discuss typical problems and possible influential features derived from signal analysis, mental mechanisms or concepts, and compositional structure. Then, we present typical solutions of such tasks related to music research, namely for organization of music collections, transcription of music signals, cognitive psychology of music, and compositional structure analysis.","Classification in musicology, Automatic transcription, Music psychology, Organization of music collections, Compositional structure analysis",https://link.springer.com//article/10.1007/s11634-007-0016-x,1163
Fuzzy clustering based on nonconvex optimisation approaches using difference of convex (DC) functions algorithms,"We present a fast and robust nonconvex optimization approach for Fuzzy C-Means (FCM) clustering model. Our approach is based on DC (Difference of Convex functions) programming and DCA (DC Algorithms) that have been successfully applied in various fields of applied sciences, including Machine Learning. The FCM model is reformulated in the form of three equivalent DC programs for which different DCA schemes are investigated. For accelerating the DCA, an alternative FCM-DCA procedure is developed. Experimental results on several real world problems that include microarray data illustrate the effectiveness of the proposed algorithms and their superiority over the standard FCM algorithm, with respect to both running-time and accuracy of solutions.","Fuzzy clustering, Nonconvex optimization, DC programming, DCA",https://link.springer.com//article/10.1007/s11634-007-0011-2,1163
Variable selection in discriminant analysis based on the location model for mixed variables,"Non-parametric smoothing of the location model is a potential basis for discriminating between groups of objects using mixtures of continuous and categorical variables simultaneously. However, it may lead to unreliable estimates of parameters when too many variables are involved. This paper proposes a method for performing variable selection on the basis of distance between groups as measured by smoothed Kullback–Leibler divergence. Searching strategies using forward, backward and stepwise selections are outlined, and corresponding stopping rules derived from asymptotic distributional results are proposed. Results from a Monte Carlo study demonstrate the feasibility of the method. Examples on real data show that the method is generally competitive with, and sometimes is better than, other existing classification methods.","Brier score, Cross-validation, Discriminant analysis, Error rate, Kullback-Leibler divergence, Location model, Non-parametric smoothing procedures, Variable selection",https://link.springer.com//article/10.1007/s11634-007-0009-9,1163
Fast calibrations of the forward search for testing multiple outliers in regression,"The paper considers the problem of testing for multiple outliers in a regression model and provides fast approximations to the null distribution of the minimum deletion residual used as a test statistic. Since direct simulation of each combination of number of observations and number of parameters is too time consuming, methods using simple normal samples are described for approximating the pointwise distribution of the test statistic. One approximation is based on adjustments to the results of simple simulations. The other uses properties of order statistics from folded t distributions to move outside the significance levels available by simulation. Analyses of data with beta errors and of transformed data on survival times demonstrate the usefulness in graphical methods of the inclusion of our bounds.","Beta-distributed errors, Deletion residual, Multiple outliers, Order statistics, Robust methods, Truncated t distribution",https://link.springer.com//article/10.1007/s11634-007-0007-y,1163
Convex ordering criteria for Lévy processes,"Modelling financial and insurance time series with Lévy processes or with exponential Lévy processes is a relevant actual practice and an active area of research. It allows qualitatively and quantitatively good adaptation to the empirical statistical properties of asset returns. Due to model incompleteness it is a problem of considerable interest to determine the dependence of option prices in these models on the choice of pricing measures and to establish nontrivial price bounds. In this paper we review and extend ordering results of stochastic and convex type for this class of models. We also extend the ordering results to processes with independent increments (PII) and present several examples and applications as to α-stable processes, NIG-processes, GH-distributions, and others. Criteria are given for the Lévy measures which imply corresponding comparison results for European type options in (exponential) Lévy models.","Convex ordering, Lévy measure, Lévy process",https://link.springer.com//article/10.1007/s11634-007-0008-x,1163
Adaptive dissimilarity index for measuring time series proximity,"The most widely used measures of time series proximity are the Euclidean distance and dynamic time warping. The latter can be derived from the distance introduced by Maurice Fréchet in 1906 to account for the proximity between curves. The major limitation of these proximity measures is that they are based on the closeness of the values regardless of the similarity w.r.t. the growth behavior of the time series. To alleviate this drawback we propose a new dissimilarity index, based on an automatic adaptive tuning function, to include both proximity measures w.r.t. values and w.r.t. behavior. A comparative numerical analysis between the proposed index and the classical distance measures is performed on the basis of two datasets: a synthetic dataset and a dataset from a public health study.","Classification, Time Series, Fréchet distance, Dynamic time warping",https://link.springer.com//article/10.1007/s11634-006-0004-6,1163
"Generalized mixture models, semi-supervised learning, and unknown class inference","In this paper, we discuss generalized mixture models and related semi-supervised learning methods, and show how they can be used to provide explicit methods for unknown class inference. After a brief description of standard mixture modeling and current model-based semi-supervised learning methods, we provide the generalization and discuss its computational implementation using three-stage expectation–maximization algorithm.","Mixture Model, Label Data, Unlabeled Sample, Automatic Target Recognition, Unknown Class",https://link.springer.com//article/10.1007/s11634-006-0001-9,1163
Mixture-model-based signal denoising,This paper proposes a new signal denoising methodology for dealing with asymmetrical noises. The adopted strategy is based on a regression model where the noise is supposed to be additive and distributed following a mixture of Gaussian densities. The parameters estimation is performed using a Generalized EM (GEM) algorithm. Experimental studies on simulated and real signals in the context of a diagnosis application in the railway domain reveal that the proposed approach performs better than the least-squares and wavelets methods.,"Denoising, Asymmetrical noise, Regression, Gaussian mixture model, EM Algorithm, GEM algorithm",https://link.springer.com//article/10.1007/s11634-006-0002-8,1163
Galois closed entity sets and k-balls of quasi-ultrametric multi-way dissimilarities,"Quasi-ultrametric multi-way dissimilarities and their respective sets of k-balls extend the fundamental bijection in classification between ultrametric pairwise dissimilarities and indexed hierarchies. We show that nonempty Galois closed subsets of a finite entity set coincide with k-balls of some quasi-ultrametric multi-way dissimilarity. This result relates the order theoretic Galois connection based clustering approach to the dissimilarity based one. Moreover, it provides an effective way to specify easy-to-interpret cluster systems, from complex data sets, as well as to derive informative attribute implications.","Closure operator, Cluster analysis, Galois connection, Multi-way dissimilarity, Quasi-ultrametric",https://link.springer.com//article/10.1007/s11634-006-0005-5,1163
Least squares estimation of linear regression models for convex compact random sets,"Simple and multiple linear regression models are considered between variables whose “values” are convex compact random sets in \({\mathbb{R}^p}\) , (that is, hypercubes, spheres, and so on). We analyze such models within a set-arithmetic approach. Contrary to what happens for random variables, the least squares optimal solutions for the basic affine transformation model do not produce suitable estimates for the linear regression model. First, we derive least squares estimators for the simple linear regression model and examine them from a theoretical perspective. Moreover, the multiple linear regression model is dealt with and a stepwise algorithm is developed in order to find the estimates in this case. The particular problem of the linear regression with interval-valued data is also considered and illustrated by means of a real-life example.","Linear regression model, Convex compact random sets, Support function, Set arithmetic approach, Point estimation, Least squares method, Interval-valued data, Set-valued data",https://link.springer.com//article/10.1007/s11634-006-0003-7,1163
